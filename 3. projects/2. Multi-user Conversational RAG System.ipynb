{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab4814d",
   "metadata": {},
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9852c4",
   "metadata": {},
   "source": [
    "## Multi-user Conversational RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4065e",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7179f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain-openai\n",
    "!pip install -qq langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e253a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed007cc7",
   "metadata": {},
   "source": [
    "## Enter API Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbde1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getpass import getpass\n",
    "\n",
    "# OPENAI_KEY = getpass('Enter your OpenAI Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022c609",
   "metadata": {},
   "source": [
    "## Setup Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9045858c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc1675",
   "metadata": {},
   "source": [
    "### Load Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09de960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50.2M/50.2M [00:13<00:00, 3.83MB/s]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Download a file from a URL\n",
    "def http_get(url, path) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a URL to a given path on disc\n",
    "    \"\"\"\n",
    "    if os.path.dirname(path) != \"\":\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    req = requests.get(url, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Exception when trying to download {}. Response {}\".format(url, req.status_code), file=sys.stderr)\n",
    "        req.raise_for_status()\n",
    "        return\n",
    "\n",
    "    download_filepath = path + \"_part\"\n",
    "    with open(download_filepath, \"wb\") as file_binary:\n",
    "        content_length = req.headers.get(\"Content-Length\")\n",
    "        total = int(content_length) if content_length is not None else None\n",
    "        progress = tqdm(unit=\"B\", total=total, unit_scale=True)\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                progress.update(len(chunk))\n",
    "                file_binary.write(chunk)\n",
    "\n",
    "    os.rename(download_filepath, path)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a60dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 169597\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59632a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169597"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda1ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".\n"
     ]
    }
   ],
   "source": [
    "print(passages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899342b",
   "metadata": {},
   "source": [
    "### Load Open AI LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb42d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dd869",
   "metadata": {},
   "source": [
    "### Generate LLM Embeddings and store them in Chroma Vector DB\n",
    "\n",
    "**Chroma Vector DB** is a versatile, open-source vector database designed for managing and querying vector embeddings. It is easy to set up and integrates well with various AI tools and algorithms. Chroma is particularly useful for applications that require rapid and precise retrieval of content represented as embeddings—efficient data formats for text, images, and soon, audio and video.\n",
    "\n",
    "**Key Features:**\n",
    "- **Integration with AI Tools:** Chroma supports embedding functions from leading providers like OpenAI, Google, and Hugging Face, allowing for flexible and powerful data handling.\n",
    "- **Ease of Use:** The database provides default embedding functions, or users can integrate external APIs to generate embeddings.\n",
    "- **Efficient Querying:** Users can create collections to store embeddings, documents, and metadata. These can be queried to retrieve the most similar items, making information retrieval quick and effective.\n",
    "- **Flexible API:** Chroma offers a straightforward API that supports both standard operations and custom embedding functions.\n",
    "\n",
    "For more detailed information, visit the official Chroma documentation [here](https://docs.trychroma.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa21487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".',\n",
       " 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.',\n",
       " \"A crater is a round dent on a planet. They are usually shaped like a circle or an oval. They are usually made by something like a meteor hitting the surface of a planet. Underground activity such as volcanoes or explosions can also cause them but it's not as likely.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36c9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd1145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore we'll be using\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# The splitting and chunking strategy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80cfc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=doc) for doc in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23d2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000,\n",
    "                                          chunk_overlap=200)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0da3382e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".'),\n",
       " Document(metadata={}, page_content='Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.'),\n",
       " Document(metadata={}, page_content=\"A crater is a round dent on a planet. They are usually shaped like a circle or an oval. They are usually made by something like a meteor hitting the surface of a planet. Underground activity such as volcanoes or explosions can also cause them but it's not as likely.\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974d0c1",
   "metadata": {},
   "source": [
    "## Create Vector DB and Retriever\n",
    "\n",
    "If you have already created `wiki_db`in the previous hands-on session then just load the DB and DO NOT run the following code to create the database again, ignore this when running on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e82b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector DB of docs and embeddings - takes 1 min on Colab\n",
    "chroma_db = Chroma.from_documents(documents=chunked_docs, collection_name='wiki_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./wiki_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909b440",
   "metadata": {},
   "source": [
    "## Load Vector DB from disk\n",
    "\n",
    "Run the following code if your vector DB already exists on disk from the previous hands-on session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./wiki_db\",\n",
    "                   collection_name='wiki_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d36ee",
   "metadata": {},
   "source": [
    "### Build a QA RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If the answer is not present in the context, just say that you don't know.\n",
    "            Keep the answer to the point.\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Answer:\n",
    "         \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef68959",
   "metadata": {},
   "source": [
    "## RAG Chain - Using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3547343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (similarity_retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf50e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the fastest animal?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20551f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who was the winner of the champions league in 2020?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980288a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the capital of India?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b567b25",
   "metadata": {},
   "source": [
    "# Conversational RAG System with LangChain\n",
    "\n",
    "In many Q&A applications, the ability to engage in back-and-forth conversations with users is crucial. This necessitates the application having a form of \"memory\" to recall past interactions and apply this context to current queries.\n",
    "\n",
    "This guide focuses on integrating historical messages into the application's logic. Additional details on managing chat history can be found [here](https://python.langchain.com/docs/expression_language/how_to/message_history/).\n",
    "\n",
    "![](https://i.imgur.com/8hLJMPl.gif)\n",
    "\n",
    "### Building on the Q&A RAG System - to a Conversational Q&A RAG System\n",
    "\n",
    "We will enhance our Q&A RAG System, which utilizes the Wikipedia dataset, by implementing the following updates:\n",
    "\n",
    "- **Prompt Adjustment:** Our prompt will be modified to include historical messages as inputs, allowing the system to maintain context over the course of a conversation.\n",
    "\n",
    "- **Contextualizing Questions:** We will introduce a sub-chain mechanism to reformulate the latest user query by considering the chat history. This is crucial for understanding questions that refer back to previous messages. For example, a query like \"Can you elaborate on the second point?\" relies on the context provided by preceding interactions, which affects the system's ability to retrieve relevant information effectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44d6c8",
   "metadata": {},
   "source": [
    "## Contextualizing the Question\n",
    "\n",
    "To maintain a seamless flow in conversations, especially in a Q&A setting, it's essential to incorporate historical interactions. Here’s how we achieve this:\n",
    "\n",
    "### Defining a Sub-Chain for Historical Context\n",
    "\n",
    "1. **Sub-Chain Creation:** We'll define a sub-chain that uses both historical messages and the latest user query. This sub-chain reformulates the question if it refers to any past interactions, ensuring the system, especially the vector database understands the context to return the most relevant documents to this newly reworded question.\n",
    "\n",
    "2. **Using `MessagesPlaceholder`:** Our prompt construction involves a `MessagesPlaceholder` variable named `chat_history`. This setup allows us to input a list of messages using the `chat_history` key. The system integrates these messages, positioning them after its own responses and before the latest user question.\n",
    "\n",
    "3. **Helper Function Usage:** We employ the `create_history_aware_retriever` function available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html). This function is crucial for handling instances where the chat history might be empty and orchestrates the sequence of operations: `prompt | llm | StrOutputParser() | retriever`.\n",
    "\n",
    "4. **Chain Construction:** The `create_history_aware_retriever` constructs a chain that processes inputs under the keys `input` and `chat_history`, ensuring the output schema aligns with that of a retriever.\n",
    "\n",
    "By implementing these steps, our system can effectively utilize historical context to better understand and respond to user queries, thereby enhancing the conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rephrase_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatgpt, similarity_retriever, rephrase_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632fc06",
   "metadata": {},
   "source": [
    "This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4b119",
   "metadata": {},
   "source": [
    "## Building the QA RAG Chain with Chat History\n",
    "\n",
    "Now we're ready to construct our comprehensive QA RAG chain, which leverages historical context for more accurate and relevant responses.\n",
    "\n",
    "### Components of the QA RAG Chain\n",
    "\n",
    "1. **Creating Document Chains:**\n",
    "   - We use the `create_stuff_documents_chain` function, which is detailed [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html). This function is used to create a `question_answer_chain`, accepting inputs such as `context`, `chat_history`, and `input`. It efficiently combines the retrieved context with the conversation history and the current query to generate an informed answer.\n",
    "\n",
    "2. **Building the Final QA RAG Chain:**\n",
    "   - The entire QA RAG chain is assembled using the `create_retrieval_chain` function, available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html). This chain integrates the `history_aware_retriever` with the `question_answer_chain`. It retains intermediate outputs like the retrieved context for added convenience during the query handling process.\n",
    "   - The `create_retrieval_chain` function accepts keys such as `input` and `chat_history` and includes `input`, `chat_history`, `context`, and `answer` in its outputs.\n",
    "\n",
    "By implementing these steps, the system not only contextualizes but also provides accurate answers by synthesizing information from both the current and historical interactions. This method enhances the conversational AI’s ability to understand and respond to user queries dynamically, making the interactions more engaging and relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "                      Use the following pieces of retrieved context to answer the question.\n",
    "                      If the answer is not present in the context, just say that you don't know.\n",
    "                      Keep the answer to the point.\n",
    "\n",
    "                      Context:\n",
    "                      {context}\n",
    "\n",
    "                  \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"Question:\n",
    "                     {input}\n",
    "\n",
    "                     Answer:\n",
    "                  \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatgpt, qa_prompt)\n",
    "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9904418",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is the capital of India?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4609945",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me more about this city\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the fastest animal?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f97c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about its different species\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8c5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
