{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45a1c189-b953-448e-ad8d-9a8cd8fa713f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b5c497-4249-4aa4-83e6-87d878a1b43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "p3eREw2pcdqm"
   },
   "source": [
    "# Basic RAG System with LangChain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5694068f-5e59-4f96-a689-b6cd4121eda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FPj5mhnc4lp_"
   },
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f63de5ea-6061-4006-9da3-10f134585c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37743,
     "status": "ok",
     "timestamp": 1734096008297,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "3J_Z9mKEa8fM",
    "outputId": "85bca36f-72f5-4f55-a886-818279d48974"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.10\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "466fa8a3-d7c8-4c61-b19b-49945c89f720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36088,
     "status": "ok",
     "timestamp": 1734096044382,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "W9SJBRTmdiF5",
    "outputId": "af64f3b2-aca6-4cbb-a71d-defef87934cb"
   },
   "outputs": [],
   "source": [
    "!pip install langchain-chroma==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "394bba82-6494-426f-937e-51bff4394d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PtBa7rlWJWH3"
   },
   "source": [
    "## Enter API Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf7536cf-032d-455f-9fd8-f5911fd5b731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14636,
     "status": "ok",
     "timestamp": 1734096059015,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Av1UpSgXZUsI",
    "outputId": "09ff839e-5834-449b-e4b6-93c6d4d3370e"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter your OpenAI Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07760c27-47bc-4df4-b1a7-8772950e1978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "T5rOqCyianbP"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50a87c-3627-4009-81a0-8ac273d549fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1734096061827,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "1PIStD04Zp9p"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad69d42-ed57-4f61-abba-4c70e5c1529f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wmUCqWtf5gpk"
   },
   "source": [
    "### Load Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f9a742-21c6-47fe-b97b-a0269fbbeb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18126,
     "status": "ok",
     "timestamp": 1734096083630,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "QQjM-gKUDT1Q",
    "outputId": "c79f98fd-5eb4-420c-fc0e-e57340576ceb"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Download a file from a URL\n",
    "def http_get(url, path) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a URL to a given path on disc\n",
    "    \"\"\"\n",
    "    if os.path.dirname(path) != \"\":\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    req = requests.get(url, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Exception when trying to download {}. Response {}\".format(url, req.status_code), file=sys.stderr)\n",
    "        req.raise_for_status()\n",
    "        return\n",
    "\n",
    "    download_filepath = path + \"_part\"\n",
    "    with open(download_filepath, \"wb\") as file_binary:\n",
    "        content_length = req.headers.get(\"Content-Length\")\n",
    "        total = int(content_length) if content_length is not None else None\n",
    "        progress = tqdm(unit=\"B\", total=total, unit_scale=True)\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                progress.update(len(chunk))\n",
    "                file_binary.write(chunk)\n",
    "\n",
    "    os.rename(download_filepath, path)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f1608a-f3eb-4285-aabb-9e463f87bbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1734096086039,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "9_NrBVzteHaZ",
    "outputId": "246e833a-f08d-4c84-e315-b840641e9b0c"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "057e5ad8-18f2-4a90-9a7b-3f677a07b864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1734096088189,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Id4P7kZ-DT1R"
   },
   "outputs": [],
   "source": [
    "# We subset our data so we only use a subset of wikipedia to run things faster\n",
    "passages = [passage for passage in passages for x in ['fish', 'india', 'cheetah']\n",
    "              if x in passage.lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e838115-8963-4b92-904d-18c2c7ed6df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734096088189,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "e13_oFtlDT1S",
    "outputId": "0085cb36-e222-474d-b461-754c4d8f7fdc"
   },
   "outputs": [],
   "source": [
    "len(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd0dfdfc-ad48-4744-8aeb-6220b3dcd19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734096088189,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "sm77itlPEDx0",
    "outputId": "751257f6-ec5d-479c-bcb4-66ffb04eccf8"
   },
   "outputs": [],
   "source": [
    "passages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe267ec3-767a-46b1-8bee-d2980d20567c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KZUxQwCl5jY-"
   },
   "source": [
    "### Load Open AI LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ff52b3-a246-41b4-a4fb-1a9a1df96d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 2815,
     "status": "ok",
     "timestamp": 1734096091001,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "0NKUgjxLL-Ux"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76de167e-805e-4b8c-b0b2-8c4ab6c8a0a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K1nD_kYA5ncq"
   },
   "source": [
    "### Generate LLM Embeddings and store them in Chroma Vector DB\n",
    "\n",
    "**Chroma Vector DB** is a versatile, open-source vector database designed for managing and querying vector embeddings. It is easy to set up and integrates well with various AI tools and algorithms. Chroma is particularly useful for applications that require rapid and precise retrieval of content represented as embeddings—efficient data formats for text, images, and soon, audio and video.\n",
    "\n",
    "**Key Features:**\n",
    "- **Integration with AI Tools:** Chroma supports embedding functions from leading providers like OpenAI, Google, and Hugging Face, allowing for flexible and powerful data handling.\n",
    "- **Ease of Use:** The database provides default embedding functions, or users can integrate external APIs to generate embeddings.\n",
    "- **Efficient Querying:** Users can create collections to store embeddings, documents, and metadata. These can be queried to retrieve the most similar items, making information retrieval quick and effective.\n",
    "- **Flexible API:** Chroma offers a straightforward API that supports both standard operations and custom embedding functions.\n",
    "\n",
    "For more detailed information, visit the official Chroma documentation [here](https://docs.trychroma.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf5f5c2-8f8e-4a1a-9b94-fd2d97f40388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734096091002,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "bbEe-0csNRmB",
    "outputId": "a92ad34d-0810-4cd0-d89c-fbd27e067084"
   },
   "outputs": [],
   "source": [
    "passages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535f283a-0bbf-4215-89dd-b483867dd8fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734096091002,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "HyFSs6xZfzok"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f081ba6-0a10-4bc1-bbc1-d5b02fcf4715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1734096092961,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "KPRVzeE6NwE7"
   },
   "outputs": [],
   "source": [
    "# The vectorstore we'll be using\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# The splitting and chunking strategy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9364f9c9-bac8-416a-bddd-8c17232de838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1734096094010,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Xt-9DpXxXscN"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=doc) for doc in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b8e3f1-573a-48dc-a1b9-63618c95e1d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734096095229,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "N3jwYefVL78q",
    "outputId": "e2795b29-f193-4393-d8a4-73a23896b971"
   },
   "outputs": [],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fca7e59-ad7a-42b5-bcfc-cf07ec44ea43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1734096097743,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "P_9jsBAJWMk5"
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=3000,\n",
    "                                          chunk_overlap=200)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6081c23e-c5b3-45b7-ae59-293e233f331d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1734096099622,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "5Tu4rqnWYLFI",
    "outputId": "72d9fce6-b8f2-4c14-d165-e60609cd4fe0"
   },
   "outputs": [],
   "source": [
    "chunked_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be94050-b09d-487c-b4cb-14c50cd0580e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6RXepcnd1E7b"
   },
   "source": [
    "## Create Vector DB and Retriever\n",
    "\n",
    "If you have already created `wiki_db`in the previous hands-on session then just load the DB and DO NOT run the following code to create the database again, ignore this when running on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e9bbbb-fc67-426f-b149-5d1bf2486b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 9018,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "_st2XDc3N2Kf"
   },
   "outputs": [],
   "source": [
    "# create vector DB of docs and embeddings - takes 1 min on Colab\n",
    "chroma_db = Chroma.from_documents(documents=chunked_docs, collection_name='wiki_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./wiki_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ccfd53-655c-4d1b-8d80-f215a43efeb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9ju_zBIj1Zsb"
   },
   "source": [
    "## Load Vector DB from disk\n",
    "\n",
    "Run the following code if your vector DB already exists on disk from the previous hands-on session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91adace2-4eea-4736-b54d-a53b6c7903d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "pNvj0dDH1WDg"
   },
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./wiki_db\",\n",
    "                   collection_name='wiki_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53cccdf4-7f98-4aeb-a1fb-f9561b39db78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "NFC3uPqYop0a",
    "outputId": "d3652782-a58e-4891-b6b0-6b93334aedef"
   },
   "outputs": [],
   "source": [
    "chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450d9a11-1360-4bc2-bb25-adc59c4e7834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "pTmvHpCRlXOO"
   },
   "outputs": [],
   "source": [
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98bbd884-0745-42b4-bbac-70e81a23a969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Bfn4Z4Em55bb"
   },
   "source": [
    "### Build a QA RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d000de3-11a2-4818-aeb0-faca11379081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "8Pu5U9HNkl-q"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If the answer is not present in the context, just say that you don't know.\n",
    "            Keep the answer to the point.\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Answer:\n",
    "         \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd3a1d44-fbca-4328-9842-5bb23af67813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "1MpA4HCDw_fP",
    "outputId": "abb77c8f-f2dc-400d-8dec-983e73a975aa"
   },
   "outputs": [],
   "source": [
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2234a13e-1fa1-4bba-9c08-76270e434c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ng4cKPYfmRyg"
   },
   "source": [
    "## RAG Chain - Using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb22f0a2-69c5-44e0-8faf-f67082bc2422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1734096110883,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "18p2bJahmLX_"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (similarity_retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe8e86a-7e3c-4dc3-9638-68330beeb7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1734096112795,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "njFlSt9Smznk",
    "outputId": "3dda1470-b5bf-4bd9-a458-3b574ba8062b"
   },
   "outputs": [],
   "source": [
    "query = \"What is the fastest animal?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92bf47da-b3a5-40b0-b452-e9a43b275062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1734096113356,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "PS5f9QKVn935",
    "outputId": "c32e3515-68d0-437e-eb37-45e846590942"
   },
   "outputs": [],
   "source": [
    "query = \"Who was the winner of the champions league in 2020?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1a3fe07-6bdc-4534-adf0-4ca6bc4e416b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1734096114696,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "-OfsGdmhnn6u",
    "outputId": "25b9cbc9-ec88-49a8-aed2-7c7b22267d63"
   },
   "outputs": [],
   "source": [
    "query = \"What is the capital of India?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04465df-8607-4b8b-8bef-3265f3ea373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1734096116705,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "VwnV1fvWPILC",
    "outputId": "44397c13-ac30-49dc-cdfd-3351f8bd8bb6"
   },
   "outputs": [],
   "source": [
    "query = \"Tell me more about it in detail\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7fa8705-6a0a-40be-bf3e-a8cacf949f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QVemECCPfYg0"
   },
   "source": [
    "# Conversational RAG System with LangChain\n",
    "\n",
    "In many Q&A applications, the ability to engage in back-and-forth conversations with users is crucial. This necessitates the application having a form of \"memory\" to recall past interactions and apply this context to current queries.\n",
    "\n",
    "This guide focuses on integrating historical messages into the application's logic. Additional details on managing chat history can be found [here](https://python.langchain.com/docs/expression_language/how_to/message_history/).\n",
    "\n",
    "![](https://i.imgur.com/8hLJMPl.gif)\n",
    "\n",
    "### Building on the Q&A RAG System - to a Conversational Q&A RAG System\n",
    "\n",
    "We will enhance our Q&A RAG System, which utilizes the Wikipedia dataset, by implementing the following updates:\n",
    "\n",
    "- **Prompt Adjustment:** Our prompt will be modified to include historical messages as inputs, allowing the system to maintain context over the course of a conversation.\n",
    "\n",
    "- **Contextualizing Questions:** We will introduce a sub-chain mechanism to reformulate the latest user query by considering the chat history. This is crucial for understanding questions that refer back to previous messages. For example, a query like \"Can you elaborate on the second point?\" relies on the context provided by preceding interactions, which affects the system's ability to retrieve relevant information effectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd90ce6-da40-42f7-a64d-3a09762e9622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H0sMxGqEojYM"
   },
   "source": [
    "## Contextualizing the Question\n",
    "\n",
    "To maintain a seamless flow in conversations, especially in a Q&A setting, it's essential to incorporate historical interactions. Here’s how we achieve this:\n",
    "\n",
    "### Defining a Sub-Chain for Historical Context\n",
    "\n",
    "1. **Sub-Chain Creation:** We'll define a sub-chain that uses both historical messages and the latest user query. This sub-chain reformulates the question if it refers to any past interactions, ensuring the system, especially the vector database understands the context to return the most relevant documents to this newly reworded question.\n",
    "\n",
    "2. **Using `MessagesPlaceholder`:** Our prompt construction involves a `MessagesPlaceholder` variable named `chat_history`. This setup allows us to input a list of messages using the `chat_history` key. The system integrates these messages, positioning them after its own responses and before the latest user question.\n",
    "\n",
    "3. **Helper Function Usage:** We employ the `create_history_aware_retriever` function available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html). This function is crucial for handling instances where the chat history might be empty and orchestrates the sequence of operations: `prompt | llm | StrOutputParser() | retriever`.\n",
    "\n",
    "4. **Chain Construction:** The `create_history_aware_retriever` constructs a chain that processes inputs under the keys `input` and `chat_history`, ensuring the output schema aligns with that of a retriever.\n",
    "\n",
    "By implementing these steps, our system can effectively utilize historical context to better understand and respond to user queries, thereby enhancing the conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "776d78e5-06a6-44eb-a856-73630805239a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1734096122530,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "1Drh0P1d2s0m"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rephrase_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatgpt, similarity_retriever, rephrase_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ce4776-00a7-4d8c-9fac-56d28f77ee5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QN2SIBOs29Si"
   },
   "source": [
    "This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ebb0e2-a28d-45eb-a0b0-10b2b0ecc583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "67lm91Yl5eJg"
   },
   "source": [
    "## Building the QA RAG Chain with Chat History\n",
    "\n",
    "Now we're ready to construct our comprehensive QA RAG chain, which leverages historical context for more accurate and relevant responses.\n",
    "\n",
    "### Components of the QA RAG Chain\n",
    "\n",
    "1. **Creating Document Chains:**\n",
    "   - We use the `create_stuff_documents_chain` function, which is detailed [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html). This function is used to create a `question_answer_chain`, accepting inputs such as `context`, `chat_history`, and `input`. It efficiently combines the retrieved context with the conversation history and the current query to generate an informed answer.\n",
    "\n",
    "2. **Building the Final QA RAG Chain:**\n",
    "   - The entire QA RAG chain is assembled using the `create_retrieval_chain` function, available [here](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html). This chain integrates the `history_aware_retriever` with the `question_answer_chain`. It retains intermediate outputs like the retrieved context for added convenience during the query handling process.\n",
    "   - The `create_retrieval_chain` function accepts keys such as `input` and `chat_history` and includes `input`, `chat_history`, `context`, and `answer` in its outputs.\n",
    "\n",
    "By implementing these steps, the system not only contextualizes but also provides accurate answers by synthesizing information from both the current and historical interactions. This method enhances the conversational AI’s ability to understand and respond to user queries dynamically, making the interactions more engaging and relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0206a85d-6fdc-429f-ab35-f4be5580b795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1734096126115,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "f8v6WgeG5KQa"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "                      Use the following pieces of retrieved context to answer the question.\n",
    "                      If the answer is not present in the context, just say that you don't know.\n",
    "                      Keep the answer to the point.\n",
    "\n",
    "                      Context:\n",
    "                      {context}\n",
    "\n",
    "                  \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"Question:\n",
    "                     {input}\n",
    "\n",
    "                     Answer:\n",
    "                  \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatgpt, qa_prompt)\n",
    "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f70e18-e3f4-4629-8b0f-1fd5267763d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1734096129390,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "mPguXvkG7b6c",
    "outputId": "0b427bf4-d026-4c0e-e571-b4466709b2f6"
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is the capital of India?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a07f984-31e0-4a4a-83e0-77b09a9b4993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1734096131226,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "nZxkLV4bJkOt",
    "outputId": "6fbff559-d9f6-474c-bce8-982e3bb04b20"
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28cb2530-e235-45bf-8b62-55da08c5d819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1734096132501,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "v_fPkZ9P7rF4",
    "outputId": "af481e63-a258-4ff2-e753-540278bacb0e"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f5a5502-b018-4579-b357-23f68ce0b6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2438,
     "status": "ok",
     "timestamp": 1734096137273,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "nZRM06Y375LF",
    "outputId": "9e0a4507-35e6-4e23-e249-02af8a7683b6"
   },
   "outputs": [],
   "source": [
    "question = \"Tell me more about this city\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9dce01e-2a71-4e60-95a2-b657ab29a3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozckE4Hr8yOj",
    "outputId": "5645c86a-c42d-45ef-886f-73deda314333"
   },
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2e87f3-8313-46d8-8d92-9c52a42972ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQqNj73E8zVL",
    "outputId": "0b39dd55-ffb7-456c-b319-1dacd3f3c77c"
   },
   "outputs": [],
   "source": [
    "question = \"What is the fastest animal?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f2b03f3-911e-4e12-a85b-2e54a56a2a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op5Lb-9uJ5aI",
    "outputId": "3d9e040f-634d-47f0-bd57-c5237d4f2805"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b490ed85-1e2b-4738-8568-e970ee1ac6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ra8o20RH88WQ"
   },
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74a3890-353c-4f15-bc50-082f2913a9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNkuQx7DezF5",
    "outputId": "0192b7f0-60aa-4a23-a16b-2e071801926e"
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "820f62e7-7833-4cf1-ad29-54aac9598249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84jh_iP5bEn5",
    "outputId": "94f7f5d5-4a88-4984-f080-52ce545791f9"
   },
   "outputs": [],
   "source": [
    "chat_history[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4055c2aa-9d7e-4737-8e09-5bf93aa4410c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7s3Ii71fKa9U",
    "outputId": "9c84e1be-da32-4c5f-ad29-174cecdc3680"
   },
   "outputs": [],
   "source": [
    "question = \"Tell me about its different species\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5b42f6-88c1-4ac6-9ecf-e4a221de92bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yOFkt3K52SLt"
   },
   "source": [
    "# Multi-User Conversational RAG System with LangChain\n",
    "\n",
    "In many Q&A applications, the ability to engage in back-and-forth conversations with users is crucial. This necessitates the application having a form of \"memory\" to recall past interactions and apply this context to current queries.\n",
    "\n",
    "However in most real-world conversational systems, multiple users or user sessions will be accessing the system simultaneously.\n",
    "\n",
    "![](https://i.imgur.com/X4WivLu.gif)\n",
    "\n",
    "Here we will show how you can use `SQLChatMessageHistory` such that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time. Instead of in-memory we can store it in a SQL database which can be used to store a lot of conversations.\n",
    "\n",
    "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in a SQL database. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n",
    "\n",
    "We also use a `memory_buffer_window` function to only use the top-K last historical conversations before sending it to the LLM, basically our own implementation of `ConversationBufferWindowMemory`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b1432d0-3e7b-48b2-ba44-0927f727c9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1734096143199,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "9OnEXZ113TEp",
    "outputId": "5d3b26cc-64e0-4078-fc2b-8c69190d7219"
   },
   "outputs": [],
   "source": [
    "# removes the memory database file - usually not needed\n",
    "# you can run this only when you want to remove all conversation histories\n",
    "!rm memory.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cc8f877-0e77-4c6b-b1d0-34a29a200ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1432,
     "status": "ok",
     "timestamp": 1734096145427,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "sleeltMX3WfG"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "######### REPHRASER ############\n",
    "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rephrase_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatgpt, similarity_retriever, rephrase_prompt\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "######### MULTI_USER RAG RESPONSE GENERATOR ############\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "                      Analyze the user question carefully and use the following pieces of\n",
    "                      retrieved context to answer the question.\n",
    "                      If the answer is not present in the context, just say that you don't know.\n",
    "                      Keep the answer to the point.\n",
    "\n",
    "                      Context:\n",
    "                      {context}\n",
    "                  \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"\"\"Question:\n",
    "                     {input}\n",
    "\n",
    "                     Answer:\n",
    "                  \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# used to retrieve conversation history from database\n",
    "# based on a specific user or session ID\n",
    "def get_session_history_db(session_id):\n",
    "    history = SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
    "    return history\n",
    "\n",
    "# subset historical conversations based on last K conversation messages\n",
    "# here by default we use the last 10 conversations (ai-human) as memory to the input prompt\n",
    "def memory_buffer_window(messages, lastk_conversations=10):\n",
    "    return messages[-(lastk_conversations*2):] # each conversation has 2 messages - (human prompt, AI response)\n",
    "\n",
    "# custom RAG chain which looks at last K conversational messages\n",
    "question_answer_chain = (\n",
    "    RunnablePassthrough.assign(chat_history=lambda x: memory_buffer_window(x[\"chat_history\"]))\n",
    "      |\n",
    "    qa_prompt\n",
    "      |\n",
    "    chatgpt\n",
    "      |\n",
    "    StrOutputParser()\n",
    ")\n",
    "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "############ CONVERSATIONAL RAG CHAIN ####################\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    qa_rag_chain,\n",
    "    get_session_history_db,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05380169-f027-4e98-8194-d437497d5e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1734096149112,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "B8Tx0vy3-bGp"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def conv_rag_chatbot(usersession_id, prompt):\n",
    "    response = conversational_rag_chain.invoke(\n",
    "                                {\"input\": prompt},\n",
    "                                config={\n",
    "                                    \"configurable\": {\"session_id\": usersession_id}\n",
    "                                }\n",
    "    )\n",
    "    print('Answer:')\n",
    "    display(Markdown(response['answer']))\n",
    "    print('Sources:')\n",
    "    for document in response['context']:\n",
    "        print(document)\n",
    "        print()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5879cc5f-45a4-4bef-befb-e7de03720ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 1570,
     "status": "ok",
     "timestamp": 1734096152808,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "QaPLZp3T-7hb",
    "outputId": "9d6071df-3db5-4adf-bb2d-98afc4fea45f"
   },
   "outputs": [],
   "source": [
    "us_id = 'bond007'\n",
    "r = conv_rag_chatbot(us_id, 'What is the capital of India?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e54cd2-63ee-486b-b603-04789ae317c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 2733,
     "status": "ok",
     "timestamp": 1734096158149,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "XNwp9jKH74M2",
    "outputId": "d07706fd-1bac-4ee8-9f99-05e877c3536c"
   },
   "outputs": [],
   "source": [
    "r = conv_rag_chatbot(us_id, 'Tell me more about it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c403474-d868-487c-90bf-3edf98484e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1734096162256,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "kZz9rmHs7b0h",
    "outputId": "c5aa7971-aaa8-465b-fc9c-3185d9c5f16d"
   },
   "outputs": [],
   "source": [
    "us_id = 'jim003'\n",
    "r = conv_rag_chatbot(us_id, 'What is the fastest animal on land?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc9fb74-a2a3-41dc-b2c6-7977b9d4bee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 3160,
     "status": "ok",
     "timestamp": 1734096172682,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "icEp-_b984-r",
    "outputId": "9375a6f3-de5a-4549-ab77-bd5dac951720"
   },
   "outputs": [],
   "source": [
    "us_id = 'bond007'\n",
    "r = conv_rag_chatbot(us_id, 'Tell me about wildlife in India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c28b95-2260-4b31-b53d-337af8bc46a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "OroRf99D7rZF",
    "outputId": "67ae5820-fcef-4461-b70b-5ff2bd00626c"
   },
   "outputs": [],
   "source": [
    "us_id = 'jim003'\n",
    "r = conv_rag_chatbot(us_id, 'tell me more about its different species')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "M8_Simple_RAG,_Conversational_RAG_and_Multi_User_Conversational_RAG_Systems",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
