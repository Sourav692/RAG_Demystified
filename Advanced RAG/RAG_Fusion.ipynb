{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cb47f4a-5c29-4411-bede-7034006f56dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yu46Og5LfP6w"
   },
   "source": [
    "# Step-by-step implementation\n",
    "The following are the code implementation of RAG-Fusion. The first five steps are similar to the Multi-Query technique:\n",
    "  1. Import necessary libraries\n",
    "  2. Set up the LangSmith and OpenAI API keys\n",
    "  3. Load and split documents\n",
    "  4. Index documents\n",
    "  5. RAG-Fusion: Query generation\n",
    "  6. Retrieval with reciprocal rank fusion (RRF)\n",
    "  7. Run the RAG model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde8bb8b-7819-40a8-8c61-9ef036b9ee90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BxQPwCQPgrQk"
   },
   "source": [
    "## 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9220b7e5-4a17-4f5a-9465-179e807b3256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9pydZW2_LeT9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c84a605-86f5-4305-bb64-98ccc9bd1070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NVW0nPhjgvkX"
   },
   "source": [
    "## 2. Set up the LangSmith and OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "980a7d3b-acc9-4223-a145-d9fcc7dbe4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7B5xMVqyxf8P"
   },
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '' # Add your LangSmith LangChain API key\n",
    "os.environ['LANGCHAIN_PROJECT']='RAG-Fusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5086560e-59f6-438b-9d57-23dbf426acda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nPIvQwmPLeVy"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] = \"\"  # Add your OpenAI API key\n",
    "if OPENAI_API_KEY == \"\":\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "588e711b-ade8-46e2-bd54-3be6bc1a0c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wasrOysDg17L"
   },
   "source": [
    "## 3. Load and split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ab3747-39c7-41c8-9f52-de61e3cab4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dPNKIdvxMC8Z"
   },
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"blog.langchain.dev_announcing-langsmith_.txt\"),\n",
    "    TextLoader(\"blog.langchain.dev_automating-web-research_.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a568d41c-36ce-4bbc-b12c-450f9026454b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8i6jzT-DOsKU"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5f1053-cc03-49ca-8276-de46ad861bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OwfYWlhNMYWw"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=400, chunk_overlap=60)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a6803c0-9390-4f58-be1b-63100df27bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UK8_gUpWiNfN"
   },
   "source": [
    "## 4. Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0df56b8-1522-43ed-bec8-6137b3f4d531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "blx_2BbMMlFg"
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73966646-8b36-4332-80d7-e235b8ec7b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sz8NByH3iUI8"
   },
   "source": [
    "## 5. RAG-Fusion: Query generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dcbfc7c-36e7-40a2-b3b4-e2b9e956a192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uh44g7OrM_iB"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant tasked with generating seach queries for a vector search engine.\n",
    "The user has a question: \"{question}\"\n",
    "Your goal/task is to create five variations of this {question} that capture different aspects of the user's intent. These variations will help the search engine retrieve relevant documents even if they don't use the exact keywords as the original question.\n",
    "Provide these alternative questions, each on a new line.**\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "rag_fusion_prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    rag_fusion_prompt_template\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13e4d2e6-9eb9-42ad-8312-01b7becf4221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UOwmnmg7iZQC"
   },
   "source": [
    "## 6. Retrieval with reciprocal rank fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b8a81f-027b-4e3d-94f9-9d87465bb04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nhAqRjgANDlF"
   },
   "outputs": [],
   "source": [
    "def reciprocal_rank_function(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a unique string identifier\n",
    "            doc_str = str(doc)  # Simple string conversion\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (doc, score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aedd28ea-16bf-44f0-87a1-515e775f258a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9KaS4Uu_NS8v"
   },
   "outputs": [],
   "source": [
    "question = \"What is Langsmith, and why do we need it?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | reciprocal_rank_function\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9298237f-8d83-42a9-94c1-aa7d159d8a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ELe8Z5d7icRB"
   },
   "source": [
    "## 7. Run the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cbb0e3d-fa36-47ff-82c9-ee61ebaff5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uqVKNpJfOUk1"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "RAG_Fusion",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
