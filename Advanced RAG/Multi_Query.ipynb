{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53dfd349-6cf8-4a9b-a987-0a437f087793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yu46Og5LfP6w"
   },
   "source": [
    "# Step-by-step implementation\n",
    "The following are the steps to implement multi-query:\n",
    "  1. Import necessary modules\n",
    "  2. Set up the LangSmith and OpenAI API keys\n",
    "  3. Prepare data and split text\n",
    "  4. Index documents\n",
    "  5. Generate multi-perspective query with LLM\n",
    "  6. Retrieve documents using multi-query\n",
    "  7. Run the RAG model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8b8965-629b-4b36-998b-cbf2c1f9cf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BxQPwCQPgrQk"
   },
   "source": [
    "## 1. Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1677f8-1a6f-4f88-938c-8c359c80209e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9pydZW2_LeT9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aebb1142-5fd2-4af5-879f-39f51c5b35e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NVW0nPhjgvkX"
   },
   "source": [
    "## 2. Set up the LangSmith and OpenAI API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2215fd4-b6c9-4c60-865a-a7547b2184a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7B5xMVqyxf8P"
   },
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '' # Add your LangSmith LangChain API key\n",
    "os.environ['LANGCHAIN_PROJECT']='Multi-Query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b6eef9-9000-4096-9d93-0f6f34c10c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nPIvQwmPLeVy"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"] = \"\"  # Add your OpenAI API key\n",
    "if OPENAI_API_KEY == \"\":\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80150780-125c-47bd-91cc-0707c2cc3fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wasrOysDg17L"
   },
   "source": [
    "## 3. Prepare data and split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5305fa28-ba97-4b4a-b5dd-6fc671aabcd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dPNKIdvxMC8Z"
   },
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"blog.langchain.dev_announcing-langsmith_.txt\"),\n",
    "    TextLoader(\"blog.langchain.dev_automating-web-research_.txt\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f4f4b2b-954b-4406-a6d9-06707838b81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8i6jzT-DOsKU"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41954b49-3279-4c2d-956c-8a7757664881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OwfYWlhNMYWw"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=400, chunk_overlap=60)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe4abda-7419-47a7-acbe-fa37a3833006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UK8_gUpWiNfN"
   },
   "source": [
    "## 4. Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f7d111-1024-456f-a252-926b84f10a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "blx_2BbMMlFg"
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb6917d-19d1-4b94-932b-1d79fc08dfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sz8NByH3iUI8"
   },
   "source": [
    "## 5. Generate multi-perspective query with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8e7f65-f67a-4a71-818e-5e604fe08b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uh44g7OrM_iB"
   },
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant tasked with generating informative queries for a vector search engine.\n",
    "The user has a question: \"{question}\"\n",
    "Your goal/task is to create three variations of this question that capture different aspects of the user's intent. These variations will help the search engine retrieve relevant documents even if they don't use the exact keywords as the original question.\n",
    "Provide these alternative questions, each on a new line.**\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5aa1ab3-bb02-4c57-a5ce-304fc5468426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UOwmnmg7iZQC"
   },
   "source": [
    "## 6. Retrieve documents using multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d245286-5b3c-4874-9fd6-bf3707725c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nhAqRjgANDlF"
   },
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[list]):\n",
    "  \"\"\" Unique union of retrieved docs \"\"\"\n",
    "  # Flatten list of lists\n",
    "  flattened_docs = [doc for sublist in documents for doc in sublist]\n",
    "\n",
    "  # Option 1: Check library documentation for hashable attribute (e.g., 'id')\n",
    "  if hasattr(flattened_docs[0], 'id'):  # Replace 'id' with the appropriate attribute\n",
    "      unique_docs = list(set(doc.id for doc in flattened_docs))\n",
    "\n",
    "  # Option 2: Convert to string (if suitable)\n",
    "  else:\n",
    "      unique_docs = list(set(str(doc) for doc in flattened_docs))\n",
    "\n",
    "  return unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9daaa22-506b-4155-a223-f9129082f4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9KaS4Uu_NS8v"
   },
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "question = \"What is LangSmith, and why do we need it?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ef5a15d-2cfa-4bf4-8cea-385ae07d565a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ELe8Z5d7icRB"
   },
   "source": [
    "## 7. Run the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a5f888-f69f-48b8-9575-02f9e5e24f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uqVKNpJfOUk1"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Multi_Query",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
