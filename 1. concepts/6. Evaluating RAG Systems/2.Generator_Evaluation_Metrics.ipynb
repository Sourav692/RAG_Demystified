{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40593191-0b62-4413-86cd-8899710603f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0e8dc7-036f-4de3-88a3-55b5ed757180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZlekplN_WC5A"
   },
   "source": [
    "# Generator Evaluation Metrics\n",
    "\n",
    "![](https://i.imgur.com/GaMHy7w.png)\n",
    "\n",
    "The generation step, which comes after retrieval, generally includes:\n",
    "\n",
    "- Building a prompt that combines the initial input with the context retrieved in the previous step.\n",
    "- Feeding this prompt to your LLM, which produces the final generated response.\n",
    "\n",
    "\n",
    "Key Metrics to Evaluate here include:\n",
    "\n",
    "- Answer Relevancy\n",
    "- Faithfulness\n",
    "- Hallucination Check\n",
    "- Custom LLM as a Judge (G-Eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33bf149b-34b7-46b2-b476-224600d3191b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bCsfBjF5l_sz"
   },
   "source": [
    "## LLM-based Answer Relevancy - DeepEval\n",
    "\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the `actual_output` of your LLM application is compared to the provided `input`.\n",
    "\n",
    "`deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query\n",
    "- `actual_output` : Actual LLM Response\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/GbNSCFC.png)\n",
    "\n",
    "\n",
    "\n",
    "## Semantic Similarity based Answer Relevancy - RAGAS\n",
    "\n",
    "DeepEval has bindings to Ragas which enables us to use the RAGASAnswerRelevancyMetric which focuses on assessing how pertinent the generated answer is to the given query using cosine similarity. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy.\n",
    "\n",
    "![](https://i.imgur.com/vq1ytZ3.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65587888-f81f-4dfc-a45c-c63e3c838e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1734095873218,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2nWBbl_3l_s0",
    "outputId": "e32d6c8f-6484-4ba6-b9ee-2be7da70d804"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2773deca-bf0d-43c9-81b7-8e95f7d153ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "81EpPklYl_s1"
   },
   "source": [
    "### Example - DeepEval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9305c8-5399-48ec-9228-694c12ed98e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3675,
     "status": "ok",
     "timestamp": 1734095878632,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2klPvY0Al_s2",
    "outputId": "78f646f0-bf75-4b67-b458-33346528fd59"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    ")\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c738912-7a6f-487f-9c59-f38738f3e7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734095878632,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "JR5A3ZhPp6Dz",
    "outputId": "552510ec-1894-4a35-bcca-e830f48b6fe9"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a8585d-09cc-45c3-9a49-ce13fcf3a26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9vZNbqkwx-Sj"
   },
   "source": [
    "### Example - RAGAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb797f46-1f5c-487c-990e-c985edfc4c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620,
     "referenced_widgets": [
      "8890484b6c3e4018907b96b026510712",
      "0a1982c87e18425a97c43a61c18f928b",
      "d7e48662099b409baeb15177af124bb5",
      "d3d8515a3b194fb3b1fdc820607a00bf",
      "ce0f9b8f431246da93ebbe3c5cb8b026",
      "28c6583c344341599988c3dad3070cc8",
      "9d4be022d51847e8a1ccd0a9eb885de2",
      "7614ca4667f948ccba8f980a1ae4722d",
      "ab7499442d7648168ca9415bda596e05",
      "ad467cc9f9884950ada71fbb95cfccbd",
      "c5bd0a78f28749e39a63421f0da20446"
     ]
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1734095893121,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "VyssN8E4pv-U",
    "outputId": "449d537e-ac37-47a7-cbbc-84b673781abc"
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    ")\n",
    "\n",
    "metric = RAGASAnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc93360-ab76-4316-86e9-acf19e722268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1734095897061,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "WAubWCbzl_s3",
    "outputId": "2e878c34-3b16-4214-d5e8-d65dd47b5184"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50dbb016-4ab7-4720-bb0b-8371b1722117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tlZajG6R1EUc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e627de7f-feed-4e6a-914f-5bcfe6f3c5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JmdnS3rw1EhX"
   },
   "source": [
    "## Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`.\n",
    "\n",
    "`deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the FaithfulnessMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query (not used in the computation)\n",
    "- `actual_output` : Actual LLM Response\n",
    "- `retrieval_context` : Top-N retrieved document chunks (nodes) from Vector DB\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/OCSFPTb.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e7c4d9-707f-4311-8da1-ff24cc34f9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPm0hd3m1EhY",
    "outputId": "1fc21ab4-c554-4b6b-a732-a1e45c1503ce"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e09101f-84cf-4fdf-a690-dc14d7cfe6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tzoXGIqh1EhZ"
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5115b8db-b9f4-4929-a02a-0884ab1c6098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-p0SPQqW166X",
    "outputId": "60e3f71c-95c1-48d6-a1a8-26a6f17501b4"
   },
   "outputs": [],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56089ffb-74c7-4c60-a088-70056c1b7848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3hw-CRSG1EhZ",
    "outputId": "fd8d4b8a-df2f-462d-bbe7-f7e584519602"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    retrieval_context=retrieved_context\n",
    ")\n",
    "\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf6944a-0c99-4824-bf9c-981524ab938b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kDPBfo71EhZ",
    "outputId": "699f306e-344e-4ac3-ea64-b845d6e9bc82"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a93d4c-56f6-481b-a298-1573314e0603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3OEaZBlf5Ijm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "808dc093-90bd-4cb3-a4bd-c14e763d1891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LrIyB4rA5I36"
   },
   "source": [
    "## Hallucination Check\n",
    "\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided (human ground truth) `context`.\n",
    "\n",
    "`deepeval`'s hallucination metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the HallucinationMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query (not used in the computation)\n",
    "- `actual_output` : Actual LLM Response\n",
    "- `context` : Human Ground Truth Context Document Chunks (Nodes)\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/qyVBKU2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd24b98d-07a8-4859-953c-7774580edc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGeeQFHy5I37",
    "outputId": "85afd6d4-bfda-4e1f-bbd8-07266bce8ebc"
   },
   "outputs": [],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61af9c3-d654-4157-b228-6d4918a7916d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "L3uQFN3I5I37"
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b20bd1-bf36-497d-b3bd-e4ae2b81d30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGxjvNZM5I38",
    "outputId": "199dcfe7-6521-42d3-e711-416f67f3b26d"
   },
   "outputs": [],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f577dd7-d22d-4194-af44-e08802745116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgjaKdKR6O4_",
    "outputId": "addeede2-fd07-421e-83d1-4475e38b9c69"
   },
   "outputs": [],
   "source": [
    "human_ground_truth_context = [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
    "                              \"Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.\"]\n",
    "human_ground_truth_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869fb096-d869-4f22-a53b-60094d3d2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "XXl6z6Cq5I38",
    "outputId": "85693856-dd93-46bb-87d5-bcc5abf0933a"
   },
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    context=human_ground_truth_context\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd69775-3b7b-4390-8953-e7174b96189a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNV-zh4f5I38",
    "outputId": "e12a69ed-48e4-49b7-fe0b-69572ab4b52d"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5fe577-75a2-4544-998a-268faeaf2129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "W6VU09PE6jOj"
   },
   "outputs": [],
   "source": [
    "ai_response = 'AI refers to machines mimicking human intelligence to produce cyborgs and electric sheep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e6aace-05db-4339-98b7-a5a7c9917798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "8jBVsZui6k_S",
    "outputId": "a232645d-c59c-4264-da67-58283c77595e"
   },
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=ai_response,\n",
    "    context=human_ground_truth_context\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921ea2dd-fbeb-4d41-adfb-22887ce1a0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sthkQ2_F6wwW",
    "outputId": "bf71ad6e-ee28-485a-8bea-614cb9b279eb"
   },
   "outputs": [],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f51651c-9209-4a52-9bba-b6c0889caeb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "h0-E1OjJF3ak"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "M7_Deep_Dive_into_RAG_Evaluation_Metrics",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a1982c87e18425a97c43a61c18f928b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c6583c344341599988c3dad3070cc8",
      "placeholder": "​",
      "style": "IPY_MODEL_9d4be022d51847e8a1ccd0a9eb885de2",
      "value": "Evaluating: 100%"
     }
    },
    "28c6583c344341599988c3dad3070cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7614ca4667f948ccba8f980a1ae4722d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8890484b6c3e4018907b96b026510712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a1982c87e18425a97c43a61c18f928b",
       "IPY_MODEL_d7e48662099b409baeb15177af124bb5",
       "IPY_MODEL_d3d8515a3b194fb3b1fdc820607a00bf"
      ],
      "layout": "IPY_MODEL_ce0f9b8f431246da93ebbe3c5cb8b026"
     }
    },
    "9d4be022d51847e8a1ccd0a9eb885de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab7499442d7648168ca9415bda596e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad467cc9f9884950ada71fbb95cfccbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5bd0a78f28749e39a63421f0da20446": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce0f9b8f431246da93ebbe3c5cb8b026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3d8515a3b194fb3b1fdc820607a00bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad467cc9f9884950ada71fbb95cfccbd",
      "placeholder": "​",
      "style": "IPY_MODEL_c5bd0a78f28749e39a63421f0da20446",
      "value": " 1/1 [00:03&lt;00:00,  3.12s/it]"
     }
    },
    "d7e48662099b409baeb15177af124bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7614ca4667f948ccba8f980a1ae4722d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab7499442d7648168ca9415bda596e05",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
