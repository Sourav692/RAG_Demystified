{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c2121a",
   "metadata": {},
   "source": [
    "## Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c9114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b2364",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n",
    "\n",
    "### Examples of Document Loaders:\n",
    "\n",
    "- **Text File Loader:** Loads data from a simple `.txt` file.\n",
    "- **Web Page Loader:** Retrieves the text content from any web page.\n",
    "- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n",
    "\n",
    "### Functionality:\n",
    "\n",
    "- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n",
    "- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
    "\n",
    "For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708857f2",
   "metadata": {},
   "source": [
    "### Directory Loaders\n",
    "\n",
    "LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb801068",
   "metadata": {},
   "source": [
    "We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n",
    "\n",
    "```\n",
    "loaders = {\n",
    "  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n",
    "}\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `file_format_extension` can be anything like `.docx`, `.pdf`etc.\n",
    "- `LoaderClass` is a specific data loader like `PyMuPDFLoader`\n",
    "- `LoaderKeywordArguments` are any specific keyword arguments which needs to be passed into that loader at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813032f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b6301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map file extensions to their respective loaders\n",
    "# Each key is a file extension (e.g., '.pdf', '.docx')\n",
    "# Each value is a tuple: (LoaderClass, LoaderKeywordArguments)\n",
    "#   - LoaderClass: The class used to load that file type\n",
    "#   - LoaderKeywordArguments: Dictionary of keyword arguments for the loader\n",
    "\n",
    "loaders = {\n",
    "    # For PDF files, use PyMuPDFLoader with no additional arguments\n",
    "    '.pdf': (PyMuPDFLoader, {}),\n",
    "\n",
    "    # For DOCX files, use UnstructuredWordDocumentLoader with specific options:\n",
    "    #   - 'strategy': 'fast' (use fast parsing)\n",
    "    #   - 'chunking_strategy': 'by_title' (split document by title)\n",
    "    #   - 'max_characters': 3000 (maximum characters per document chunk)\n",
    "    #   - 'new_after_n_chars': 2500 (preferred chunk size before starting a new chunk)\n",
    "    #   - 'mode': 'elements' (parse document as elements)\n",
    "    '.docx': (\n",
    "        UnstructuredWordDocumentLoader,\n",
    "        {\n",
    "            'strategy': 'fast',\n",
    "            'chunking_strategy': 'by_title',\n",
    "            'max_characters': 3000,      # max limit of a document chunk\n",
    "            'new_after_n_chars': 2500,   # preferred document chunk size\n",
    "            'mode': 'elements'\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362ea19",
   "metadata": {},
   "source": [
    "`DirectoryLoader` accepts a `loader_cls` argument, which defaults to `UnstructuredLoader` but we can pass our own loaders which we defined above in the `loader_cls`argument and any keyword args for the loader can be passed in the `loader_kwargs` argument.\n",
    "\n",
    "We can also show a progress bar by setting `show_progress=True`\n",
    "\n",
    "We can use the `glob` parameter to control which files to load based on file patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca476429",
   "metadata": {},
   "source": [
    "Here we create two separate loaders to load files which are word documents and PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62859fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  6.52it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Define a function to create a DirectoryLoader for a specific file type\n",
    "def create_directory_loader(file_type, directory_path):\n",
    "    return DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=f\"**/*{file_type}\",\n",
    "        loader_cls=loaders[file_type][0],\n",
    "        loader_kwargs=loaders[file_type][1],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "# Create DirectoryLoader instances for each file type\n",
    "pdf_loader = create_directory_loader('.pdf', '../../docs')\n",
    "docx_loader = create_directory_loader('.docx', '../../docs')\n",
    "\n",
    "# Load the files\n",
    "pdf_documents = pdf_loader.load()\n",
    "docx_documents = docx_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06662fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47db4c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 0}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 1}, page_content='2\\nZ. Shen et al.\\n37], layout detection [38, 22], table detection [26], and scene text detection [4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [1] or PyTorch [24], and the high-level parameters can\\nbe obfuscated by implementation details [8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,\\nand these pipelines are not documented in any central location (and often not\\ndocumented at all). This makes it diﬃcult for research teams to learn about how\\nfull pipelines are implemented and leads them to invest signiﬁcant resources in\\nreinventing the DIA wheel.\\nLayoutParser provides a uniﬁed toolkit to support DL-based document image\\nanalysis and processing. To address the aforementioned challenges, LayoutParser\\nis built with the following components:\\n1. An oﬀ-the-shelf toolkit for applying DL models for layout detection, character\\nrecognition, and other DIA tasks (Section 3)\\n2. A rich repository of pre-trained neural network models (Model Zoo) that\\nunderlies the oﬀ-the-shelf usage\\n3. Comprehensive tools for eﬃcient document image data annotation and model\\ntuning to support diﬀerent levels of customization\\n4. A DL model hub and community platform for the easy sharing, distribu-\\ntion, and discussion of DIA models and pipelines, to promote reusability,\\nreproducibility, and extensibility (Section 4)\\nThe library implements simple and intuitive Python APIs without sacriﬁcing\\ngeneralizability and versatility, and can be easily installed via pip. Its convenient\\nfunctions for handling document image data can be seamlessly integrated with\\nexisting DIA pipelines. With detailed documentations and carefully curated\\ntutorials, we hope this tool will beneﬁt a variety of end-users, and will lead to\\nadvances in applications in both industry and academic research.\\nLayoutParser is well aligned with recent eﬀorts for improving DL model\\nreusability in other disciplines like natural language processing [8, 34] and com-\\nputer vision [35], but with a focus on unique challenges in DIA. We show\\nLayoutParser can be applied in sophisticated and large-scale digitization projects'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 2}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n3\\nthat require precision, eﬃciency, and robustness, as well as simple and light-\\nweight document processing tasks focusing on eﬃcacy and ﬂexibility (Section 5).\\nLayoutParser is being actively maintained, and support for more deep learning\\nmodels and novel methods in text-based layout analysis methods [37, 34] is\\nplanned.\\nThe rest of the paper is organized as follows. Section 2 provides an overview\\nof related work. The core LayoutParser library, DL Model Zoo, and customized\\nmodel training are described in Section 3, and the DL model hub and commu-\\nnity platform are detailed in Section 4. Section 5 shows two examples of how\\nLayoutParser can be used in practical DIA projects, and Section 6 concludes.\\n2\\nRelated Work\\nRecently, various DL models and datasets have been developed for layout analysis\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\ntation tasks on historical documents. Object detection-based methods like Faster\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\nbeen used in table detection [27]. However, these models are usually implemented\\nindividually and there is no uniﬁed framework to load and use such models.\\nThere has been a surge of interest in creating open-source tools for document\\nimage processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10 are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\\npaddleOCR12 usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [35],\\n6 The number shown is obtained by specifying the search type as ‘code’.\\n7 https://ocr-d.de/en/about\\n8 https://github.com/BobLd/DocumentLayoutAnalysis\\n9 https://github.com/leonlulu/DeepLayout\\n10 https://github.com/hpanwar08/detectron2\\n11 https://github.com/JaidedAI/EasyOCR\\n12 https://github.com/PaddlePaddle/PaddleOCR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 3}, page_content='4\\nZ. Shen et al.\\nEfficient Data Annotation\\nCustomized Model Training\\nModel Customization\\nDIA Model Hub\\nDIA Pipeline Sharing\\nCommunity Platform\\nLayout Detection Models\\nDocument Images \\nThe Core LayoutParser Library\\nOCR Module\\nStorage & Visualization\\nLayout Data Structure\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\\npapers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3\\nThe Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 4}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n5\\nTable 1: Current layout detection models in the LayoutParser model zoo\\nDataset\\nBase Model1 Large Model\\nNotes\\nPubLayNet [38]\\nF / M\\nM\\nLayouts of modern scientiﬁc documents\\nPRImA [3]\\nM\\n-\\nLayouts of scanned modern magazines and scientiﬁc reports\\nNewspaper [17]\\nF\\n-\\nLayouts of scanned US newspapers from the 20th century\\nTableBank [18]\\nF\\nF\\nTable region on modern scientiﬁc and business document\\nHJDataset [31]\\nF / M\\n-\\nLayouts of history Japanese documents\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\\nbackbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\\nR-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\\nzoo in coming months.\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When\\nnecessary, users can employ existing or customized OCR models via the uniﬁed\\nAPI provided in the OCR module. 4) LayoutParser comes with a set of utility\\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\\nis also highly customizable, via its integration with functions for layout data\\nannotation and model training. We now provide detailed descriptions for each\\ncomponent.\\n3.1\\nLayout Detection Models\\nIn LayoutParser, a layout model takes a document image as an input and\\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\\nfrom traditional methods, it relies on deep convolutional neural networks rather\\nthan manually curated rules to identify content regions. It is formulated as an\\nobject detection problem and state-of-the-art models like Faster R-CNN [28] and\\nMask R-CNN [12] are used. This yields prediction results of high accuracy and\\nmakes it possible to build a concise, generalized interface for layout detection.\\nLayoutParser, built upon Detectron2 [35], provides a minimal API that can\\nperform layout detection with only four lines of code in Python:\\n1 import\\nlayoutparser as lp\\n2 image = cv2.imread(\"image_file\") # load\\nimages\\n3 model = lp. Detectron2LayoutModel (\\n4\\n\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\\n5 layout = model.detect(image)\\nLayoutParser provides a wealth of pre-trained model weights using various\\ndatasets covering diﬀerent languages, time periods, and document types. Due to\\ndomain shift [7], the prediction performance can notably drop when models are ap-\\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\\nto select models trained on a dataset similar to the test samples. A semantic syntax\\nis used for initializing the model weights in LayoutParser, using both the dataset\\nname and model name lp://<dataset-name>/<model-architecture-name>.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 5}, page_content='6\\nZ. Shen et al.\\nFig. 2: The relationship between the three types of layout data structures.\\nCoordinate supports three kinds of variation; TextBlock consists of the co-\\nordinate information and extra features like block text, types, and reading orders;\\na Layout object is a list of all possible layout elements, including other Layout\\nobjects. They all support the same set of transformation and operation APIs for\\nmaximum ﬂexibility.\\nShown in Table 1, LayoutParser currently hosts 9 pre-trained models trained\\non 5 diﬀerent datasets. Description of the training dataset is provided alongside\\nwith the trained models such that users can quickly identify the most suitable\\nmodels for their tasks. Additionally, when such a model is not readily available,\\nLayoutParser also supports training customized layout models and community\\nsharing of the models (detailed in Section 3.5).\\n3.2\\nLayout Data Structures\\nA critical feature of LayoutParser is the implementation of a series of data\\nstructures and operations that can be used to eﬃciently process and manipulate\\nthe layout elements. In document image analysis pipelines, various post-processing\\non the layout analysis model outputs is usually required to obtain the ﬁnal\\noutputs. Traditionally, this requires exporting DL model outputs and then loading\\nthe results into other pipelines. All model outputs from LayoutParser will be\\nstored in carefully engineered data types optimized for further processing, which\\nmakes it possible to build an end-to-end document digitization pipeline within\\nLayoutParser. There are three key components in the data structure, namely\\nthe Coordinate system, the TextBlock, and the Layout. They provide diﬀerent\\nlevels of abstraction for the layout data, and a set of APIs are supported for\\ntransformations or operations on these classes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 6}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n7\\nCoordinates are the cornerstones for storing layout information. Currently,\\nthree types of Coordinate data structures are provided in LayoutParser, shown\\nin Figure 2. Interval and Rectangle are the most common data types and\\nsupport specifying 1D or 2D regions within a document. They are parameterized\\nwith 2 and 4 parameters. A Quadrilateral class is also implemented to support\\na more generalized representation of rectangular regions when the document\\nis skewed or distorted, where the 4 corner points can be speciﬁed and a total\\nof 8 degrees of freedom are supported. A wide collection of transformations\\nlike shift, pad, and scale, and operations like intersect, union, and is_in,\\nare supported for these classes. Notably, it is common to separate a segment\\nof the image and analyze it individually. LayoutParser provides full support\\nfor this scenario via image cropping operations crop_image and coordinate\\ntransformations like relative_to and condition_on that transform coordinates\\nto and from their relative representations. We refer readers to Table 2 for a more\\ndetailed description of these operations13.\\nBased on Coordinates, we implement the TextBlock class that stores both\\nthe positional and extra features of individual layout elements. It also supports\\nspecifying the reading orders via setting the parent ﬁeld to the index of the parent\\nobject. A Layout class is built that takes in a list of TextBlocks and supports\\nprocessing the elements in batch. Layout can also be nested to support hierarchical\\nlayout structures. They support the same operations and transformations as the\\nCoordinate classes, minimizing both learning and deployment eﬀort.\\n3.3\\nOCR\\nLayoutParser provides a uniﬁed interface for existing OCR tools. Though there\\nare many OCR tools available, they are usually conﬁgured diﬀerently with distinct\\nAPIs or protocols for using them. It can be ineﬃcient to add new OCR tools into\\nan existing pipeline, and diﬃcult to make direct comparisons among the available\\ntools to ﬁnd the best option for a particular project. To this end, LayoutParser\\nbuilds a series of wrappers among existing OCR engines, and provides nearly\\nthe same syntax for using them. It supports a plug-and-play style of using OCR\\nengines, making it eﬀortless to switch, evaluate, and compare diﬀerent OCR\\nmodules:\\n1 ocr_agent = lp.TesseractAgent ()\\n2 # Can be easily\\nswitched to other OCR\\nsoftware\\n3 tokens = ocr_agent.detect(image)\\nThe OCR outputs will also be stored in the aforementioned layout data\\nstructures and can be seamlessly incorporated into the digitization pipeline.\\nCurrently LayoutParser supports the Tesseract and Google Cloud Vision OCR\\nengines.\\nLayoutParser also comes with a DL-based CNN-RNN OCR model [6] trained\\nwith the Connectionist Temporal Classiﬁcation (CTC) loss [10]. It can be used\\nlike the other OCR modules, and can be easily trained on customized datasets.\\n13 This is also available in the LayoutParser documentation pages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 7}, page_content='8\\nZ. Shen et al.\\nTable 2: All operations supported by the layout elements. The same APIs are\\nsupported across diﬀerent layout element classes including Coordinate types,\\nTextBlock and Layout.\\nOperation Name\\nDescription\\nblock.pad(top, bottom, right, left)\\nEnlarge the current block according to the input\\nblock.scale(fx, fy)\\nScale the current block given the ratio\\nin x and y direction\\nblock.shift(dx, dy)\\nMove the current block with the shift\\ndistances in x and y direction\\nblock1.is in(block2)\\nWhether block1 is inside of block2\\nblock1.intersect(block2)\\nReturn the intersection region of block1 and block2.\\nCoordinate type to be determined based on the inputs.\\nblock1.union(block2)\\nReturn the union region of block1 and block2.\\nCoordinate type to be determined based on the inputs.\\nblock1.relative to(block2)\\nConvert the absolute coordinates of block1 to\\nrelative coordinates to block2\\nblock1.condition on(block2)\\nCalculate the absolute coordinates of block1 given\\nthe canvas block2’s absolute coordinates\\nblock.crop image(image)\\nObtain the image segments in the block region\\n3.4\\nStorage and visualization\\nThe end goal of DIA is to transform the image-based document data into a\\nstructured database. LayoutParser supports exporting layout data into diﬀerent\\nformats like JSON, csv, and will add the support for the METS/ALTO XML\\nformat 14 . It can also load datasets from layout analysis-speciﬁc formats like\\nCOCO [38] and the Page Format [25] for training layout models (Section 3.5).\\nVisualization of the layout detection results is critical for both presentation\\nand debugging. LayoutParser is built with an integrated API for displaying the\\nlayout information along with the original document image. Shown in Figure 3, it\\nenables presenting layout data with rich meta information and features in diﬀerent\\nmodes. More detailed information can be found in the online LayoutParser\\ndocumentation page.\\n3.5\\nCustomized Model Training\\nBesides the oﬀ-the-shelf library, LayoutParser is also highly customizable with\\nsupports for highly unique and challenging document analysis tasks. Target\\ndocument images can be vastly diﬀerent from the existing datasets for train-\\ning layout models, which leads to low layout detection accuracy. Training data\\n14 https://altoxml.github.io'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 8}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n9\\nFig. 3: Layout detection and OCR results visualization generated by the\\nLayoutParser APIs. Mode I directly overlays the layout region bounding boxes\\nand categories over the original image. Mode II recreates the original document\\nvia drawing the OCR’d texts at their corresponding positions on the image\\ncanvas. In this ﬁgure, tokens in textual regions are ﬁltered using the API and\\nthen displayed.\\ncan also be highly sensitive and not sharable publicly. To overcome these chal-\\nlenges, LayoutParser is built with rich features for eﬃcient data annotation and\\ncustomized model training.\\nLayoutParser incorporates a toolkit optimized for annotating document lay-\\nouts using object-level active learning [32]. With the help from a layout detection\\nmodel trained along with labeling, only the most important layout objects within\\neach image, rather than the whole image, are required for labeling. The rest of\\nthe regions are automatically annotated with high conﬁdence predictions from\\nthe layout detection model. This allows a layout dataset to be created more\\neﬃciently with only around 60% of the labeling budget.\\nAfter the training dataset is curated, LayoutParser supports diﬀerent modes\\nfor training the layout models. Fine-tuning can be used for training models on a\\nsmall newly-labeled dataset by initializing the model with existing pre-trained\\nweights. Training from scratch can be helpful when the source dataset and\\ntarget are signiﬁcantly diﬀerent and a large training set is available. However, as\\nsuggested in Studer et al.’s work[33], loading pre-trained weights on large-scale\\ndatasets like ImageNet [5], even from totally diﬀerent domains, can still boost\\nmodel performance. Through the integrated API provided by LayoutParser,\\nusers can easily compare model performances on the benchmark datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 9}, page_content='10\\nZ. Shen et al.\\nFig. 4: Illustration of (a) the original historical Japanese document with layout\\ndetection results and (b) a recreated version of the document image that achieves\\nmuch better character recognition recall. The reorganization algorithm rearranges\\nthe tokens based on the their detected bounding boxes given a maximum allowed\\nheight.\\n4\\nLayoutParser Community Platform\\nAnother focus of LayoutParser is promoting the reusability of layout detection\\nmodels and full digitization pipelines. Similar to many existing deep learning\\nlibraries, LayoutParser comes with a community model hub for distributing\\nlayout models. End-users can upload their self-trained models to the model hub,\\nand these models can be loaded into a similar interface as the currently available\\nLayoutParser pre-trained models. For example, the model trained on the News\\nNavigator dataset [17] has been incorporated in the model hub.\\nBeyond DL models, LayoutParser also promotes the sharing of entire doc-\\nument digitization pipelines. For example, sometimes the pipeline requires the\\ncombination of multiple DL models to achieve better accuracy. Currently, pipelines\\nare mainly described in academic papers and implementations are often not pub-\\nlicly available. To this end, the LayoutParser community platform also enables\\nthe sharing of layout pipelines to promote the discussion and reuse of techniques.\\nFor each shared pipeline, it has a dedicated project page, with links to the source\\ncode, documentation, and an outline of the approaches. A discussion panel is\\nprovided for exchanging ideas. Combined with the core LayoutParser library,\\nusers can easily build reusable components based on the shared pipelines and\\napply them to solve their unique problems.\\n5\\nUse Cases\\nThe core objective of LayoutParser is to make it easier to create both large-scale\\nand light-weight document digitization pipelines. Large-scale document processing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 10}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n11\\nfocuses on precision, eﬃciency, and robustness. The target documents may have\\ncomplicated structures, and may require training multiple layout detection models\\nto achieve the optimal accuracy. Light-weight pipelines are built for relatively\\nsimple documents, with an emphasis on development ease, speed and ﬂexibility.\\nIdeally one only needs to use existing resources, and model training should be\\navoided. Through two exemplar projects, we show how practitioners in both\\nacademia and industry can easily build such pipelines using LayoutParser and\\nextract high-quality structured document data for their downstream tasks. The\\nsource code for these projects will be publicly available in the LayoutParser\\ncommunity hub.\\n5.1\\nA Comprehensive Historical Document Digitization Pipeline\\nThe digitization of historical documents can unlock valuable data that can shed\\nlight on many important social, economic, and historical questions. Yet due to\\nscan noises, page wearing, and the prevalence of complicated layout structures, ob-\\ntaining a structured representation of historical document scans is often extremely\\ncomplicated.\\nFig. 5: Illustration of how LayoutParser\\nhelps with the historical document digi-\\ntization pipeline.\\nIn this example, LayoutParser was\\nused to develop a comprehensive\\npipeline, shown in Figure 5, to gener-\\nate high-quality structured data from\\nhistorical Japanese ﬁrm ﬁnancial ta-\\nbles with complicated layouts. The\\npipeline applies two layout models to\\nidentify diﬀerent levels of document\\nstructures and two customized OCR\\nengines for optimized character recog-\\nnition accuracy.\\nAs shown in Figure 4 (a), the\\ndocument contains columns of text\\nwritten vertically 15, a common style\\nin Japanese. Due to scanning noise\\nand archaic printing technology, the\\ncolumns can be skewed or have vari-\\nable widths, and hence cannot be eas-\\nily identiﬁed via rule-based methods.\\nWithin each column, words are sepa-\\nrated by white spaces of variable size,\\nand the vertical positions of objects\\ncan be an indicator of their layout\\ntype.\\n15 A document page consists of eight rows like this. For simplicity we skip the row\\nsegmentation discussion and refer readers to the source code when available.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 11}, page_content='12\\nZ. Shen et al.\\nTo decipher the complicated layout\\nstructure, two object detection models have been trained to recognize individual\\ncolumns and tokens, respectively. A small training set (400 images with approxi-\\nmately 100 annotations each) is curated via the active learning based annotation\\ntool [32] in LayoutParser. The models learn to identify both the categories and\\nregions for each token or column via their distinct visual features. The layout\\ndata structure enables easy grouping of the tokens within each column, and\\nrearranging columns to achieve the correct reading orders based on the horizontal\\nposition. Errors are identiﬁed and rectiﬁed via checking the consistency of the\\nmodel predictions. Therefore, though trained on a small dataset, the pipeline\\nachieves a high level of layout detection accuracy: it achieves a 96.97 AP [19]\\nscore across 5 categories for the column detection model, and a 89.23 AP across\\n4 categories for the token detection model.\\nA combination of character recognition methods is developed to tackle the\\nunique challenges in this document. In our experiments, we found that irregular\\nspacing between the tokens led to a low character recognition recall rate, whereas\\nexisting OCR models tend to perform better on densely-arranged texts. To\\novercome this challenge, we create a document reorganization algorithm that\\nrearranges the text based on the token bounding boxes detected in the layout\\nanalysis step. Figure 4 (b) illustrates the generated image of dense text, which is\\nsent to the OCR APIs as a whole to reduce the transaction costs. The ﬂexible\\ncoordinate system in LayoutParser is used to transform the OCR results relative\\nto their original positions on the page.\\nAdditionally, it is common for historical documents to use unique fonts\\nwith diﬀerent glyphs, which signiﬁcantly degrades the accuracy of OCR models\\ntrained on modern texts. In this document, a special ﬂat font is used for printing\\nnumbers and could not be detected by oﬀ-the-shelf OCR engines. Using the highly\\nﬂexible functionalities from LayoutParser, a pipeline approach is constructed\\nthat achieves a high recognition accuracy with minimal eﬀort. As the characters\\nhave unique visual structures and are usually clustered together, we train the\\nlayout model to identify number regions with a dedicated category. Subsequently,\\nLayoutParser crops images within these regions, and identiﬁes characters within\\nthem using a self-trained OCR model based on a CNN-RNN [6]. The model\\ndetects a total of 15 possible categories, and achieves a 0.98 Jaccard score16 and\\na 0.17 average Levinstein distances17 for token prediction on the test set.\\nOverall, it is possible to create an intricate and highly accurate digitization\\npipeline for large-scale digitization using LayoutParser. The pipeline avoids\\nspecifying the complicated rules used in traditional methods, is straightforward\\nto develop, and is robust to outliers. The DL models also generate ﬁne-grained\\nresults that enable creative approaches like page reorganization for OCR.\\n16 This measures the overlap between the detected and ground-truth characters, and\\nthe maximum is 1.\\n17 This measures the number of edits from the ground-truth text to the predicted text,\\nand lower is better.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 12}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n13\\nFig. 6: This lightweight table detector can identify tables (outlined in red) and\\ncells (shaded in blue) in diﬀerent locations on a page. In very few cases (d), it\\nmight generate minor error predictions, e.g, failing to capture the top text line of\\na table.\\n5.2\\nA light-weight Visual Table Extractor\\nDetecting tables and parsing their structures (table extraction) are of central im-\\nportance for many document digitization tasks. Many previous works [26, 30, 27]\\nand tools 18 have been developed to identify and parse table structures. Yet they\\nmight require training complicated models from scratch, or are only applicable\\nfor born-digital PDF documents. In this section, we show how LayoutParser can\\nhelp build a light-weight accurate visual table extractor for legal docket tables\\nusing the existing resources with minimal eﬀort.\\nThe extractor uses a pre-trained layout detection model for identifying the\\ntable regions and some simple rules for pairing the rows and the columns in the\\nPDF image. Mask R-CNN [12] trained on the PubLayNet dataset [38] from the\\nLayoutParser Model Zoo can be used for detecting table regions. By ﬁltering\\nout model predictions of low conﬁdence and removing overlapping predictions,\\nLayoutParser can identify the tabular regions on each page, which signiﬁcantly\\nsimpliﬁes the subsequent steps. By applying the line detection functions within\\nthe tabular segments, provided in the utility module from LayoutParser, the\\npipeline can identify the three distinct columns in the tables. A row clustering\\nmethod is then applied via analyzing the y coordinates of token bounding boxes in\\nthe left-most column, which are obtained from the OCR engines. A non-maximal\\nsuppression algorithm is used to remove duplicated rows with extremely small\\ngaps. Shown in Figure 6, the built pipeline can detect tables at diﬀerent positions\\non a page accurately. Continued tables from diﬀerent pages are concatenated,\\nand a structured table representation has been easily created.\\n18 https://github.com/atlanhq/camelot, https://github.com/tabulapdf/tabula'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 13}, page_content='14\\nZ. Shen et al.\\n6\\nConclusion\\nLayoutParser provides a comprehensive toolkit for deep learning-based document\\nimage analysis. The oﬀ-the-shelf library is easy to install, and can be used to\\nbuild ﬂexible and accurate pipelines for processing documents with complicated\\nstructures. It also supports high-level customization and enables easy labeling and\\ntraining of DL models on unique document image datasets. The LayoutParser\\ncommunity platform facilitates sharing DL models and DIA pipelines, inviting\\ndiscussion and promoting code reproducibility and reusability. The LayoutParser\\nteam is committed to keeping the library updated continuously and bringing\\nthe most recent advances in DL-based DIA, such as multi-modal document\\nmodeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.\\nAcknowledgements We thank the anonymous reviewers for their comments\\nand suggestions. This project is supported in part by NSF Grant OIA-2033558\\nand funding from the Harvard Data Science Initiative and Harvard Catalyst.\\nZejiang Shen thanks Doug Downey for suggestions.\\nReferences\\n[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\\nJ., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\\nVi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\\nhttps://www.tensorflow.org/, software available from tensorﬂow.org\\n[2] Alberti, M., Pondenkandath, V., W¨ursch, M., Ingold, R., Liwicki, M.: Deepdiva: a\\nhighly-functional python framework for reproducible experiments. In: 2018 16th\\nInternational Conference on Frontiers in Handwriting Recognition (ICFHR). pp.\\n423–428. IEEE (2018)\\n[3] Antonacopoulos, A., Bridson, D., Papadopoulos, C., Pletschacher, S.: A realistic\\ndataset for performance evaluation of document layout analysis. In: 2009 10th\\nInternational Conference on Document Analysis and Recognition. pp. 296–300.\\nIEEE (2009)\\n[4] Baek, Y., Lee, B., Han, D., Yun, S., Lee, H.: Character region awareness for text\\ndetection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. pp. 9365–9374 (2019)\\n[5] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale\\nHierarchical Image Database. In: CVPR09 (2009)\\n[6] Deng, Y., Kanervisto, A., Ling, J., Rush, A.M.: Image-to-markup generation with\\ncoarse-to-ﬁne attention. In: International Conference on Machine Learning. pp.\\n980–989. PMLR (2017)\\n[7] Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.\\nIn: International conference on machine learning. pp. 1180–1189. PMLR (2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 14}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n15\\n[8] Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N., Peters,\\nM., Schmitz, M., Zettlemoyer, L.: Allennlp: A deep semantic natural language\\nprocessing platform. arXiv preprint arXiv:1803.07640 (2018)\\n[9]  Lukasz Garncarek, Powalski, R., Stanis lawek, T., Topolski, B., Halama, P.,\\nGrali´nski, F.: Lambert: Layout-aware (language) modeling using bert for in-\\nformation extraction (2020)\\n[10] Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal\\nclassiﬁcation: labelling unsegmented sequence data with recurrent neural networks.\\nIn: Proceedings of the 23rd international conference on Machine learning. pp.\\n369–376 (2006)\\n[11] Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets for\\ndocument image classiﬁcation and retrieval. In: 2015 13th International Conference\\non Document Analysis and Recognition (ICDAR). pp. 991–995. IEEE (2015)\\n[12] He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the\\nIEEE international conference on computer vision. pp. 2961–2969 (2017)\\n[13] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\\npp. 770–778 (2016)\\n[14] Kay, A.: Tesseract: An open-source optical character recognition engine. Linux J.\\n2007(159), 2 (Jul 2007)\\n[15] Lamiroy, B., Lopresti, D.: An open architecture for end-to-end document analysis\\nbenchmarking. In: 2011 International Conference on Document Analysis and\\nRecognition. pp. 42–47. IEEE (2011)\\n[16] Lee, B.C., Weld, D.S.: Newspaper navigator: Open faceted search for 1.5\\nmillion images. In: Adjunct Publication of the 33rd Annual ACM Sym-\\nposium\\non\\nUser\\nInterface\\nSoftware\\nand\\nTechnology.\\np.\\n120–122.\\nUIST\\n’20 Adjunct, Association for Computing Machinery, New York, NY, USA\\n(2020). https://doi.org/10.1145/3379350.3416143, https://doi-org.offcampus.\\nlib.washington.edu/10.1145/3379350.3416143\\n[17] Lee, B.C.G., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N.,\\nThomas, D., Zwaard, K., Weld, D.S.: The Newspaper Navigator Dataset: Extracting\\nHeadlines and Visual Content from 16 Million Historic Newspaper Pages in\\nChronicling America, p. 3055–3062. Association for Computing Machinery, New\\nYork, NY, USA (2020), https://doi.org/10.1145/3340531.3412767\\n[18] Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: Table benchmark\\nfor image-based table detection and recognition. arXiv preprint arXiv:1903.01949\\n(2019)\\n[19] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,\\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\\non computer vision. pp. 740–755. Springer (2014)\\n[20] Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\\npattern recognition. pp. 3431–3440 (2015)\\n[21] Neudecker, C., Schlarb, S., Dogan, Z.M., Missier, P., Suﬁ, S., Williams, A., Wolsten-\\ncroft, K.: An experimental workﬂow development platform for historical document\\ndigitisation and analysis. In: Proceedings of the 2011 workshop on historical\\ndocument imaging and processing. pp. 161–168 (2011)\\n[22] Oliveira, S.A., Seguin, B., Kaplan, F.: dhsegment: A generic deep-learning approach\\nfor document segmentation. In: 2018 16th International Conference on Frontiers\\nin Handwriting Recognition (ICFHR). pp. 7–12. IEEE (2018)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-22T01:27:10+00:00', 'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-22T01:27:10+00:00', 'trapped': '', 'modDate': 'D:20210622012710Z', 'creationDate': 'D:20210622012710Z', 'page': 15}, page_content='16\\nZ. Shen et al.\\n[23] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\\nDesmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)\\n[24] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\\nT., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style,\\nhigh-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019)\\n[25] Pletschacher, S., Antonacopoulos, A.: The page (page analysis and ground-truth\\nelements) format framework. In: 2010 20th International Conference on Pattern\\nRecognition. pp. 257–260. IEEE (2010)\\n[26] Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet:\\nAn approach for end to end table detection and structure recognition from image-\\nbased documents. In: Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops. pp. 572–573 (2020)\\n[27] Qasim, S.R., Mahmood, H., Shafait, F.: Rethinking table recognition using graph\\nneural networks. In: 2019 International Conference on Document Analysis and\\nRecognition (ICDAR). pp. 142–147. IEEE (2019)\\n[28] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. In: Advances in neural information\\nprocessing systems. pp. 91–99 (2015)\\n[29] Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph\\nneural network model. IEEE transactions on neural networks 20(1), 61–80 (2008)\\n[30] Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning\\nfor detection and structure recognition of tables in document images. In: 2017 14th\\nIAPR international conference on document analysis and recognition (ICDAR).\\nvol. 1, pp. 1162–1167. IEEE (2017)\\n[31] Shen, Z., Zhang, K., Dell, M.: A large dataset of historical japanese documents\\nwith complex layouts. In: Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops. pp. 548–549 (2020)\\n[32] Shen, Z., Zhao, J., Dell, M., Yu, Y., Li, W.: Olala: Object-level active learning\\nbased layout annotation. arXiv preprint arXiv:2010.01762 (2020)\\n[33] Studer, L., Alberti, M., Pondenkandath, V., Goktepe, P., Kolonko, T., Fischer,\\nA., Liwicki, M., Ingold, R.: A comprehensive study of imagenet pre-training for\\nhistorical document image analysis. In: 2019 International Conference on Document\\nAnalysis and Recognition (ICDAR). pp. 720–725. IEEE (2019)\\n[34] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\\nRault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s transformers: State-of-\\nthe-art natural language processing. arXiv preprint arXiv:1910.03771 (2019)\\n[35] Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://\\ngithub.com/facebookresearch/detectron2 (2019)\\n[36] Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C.,\\nChe, W., et al.: Layoutlmv2: Multi-modal pre-training for visually-rich document\\nunderstanding. arXiv preprint arXiv:2012.14740 (2020)\\n[37] Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of\\ntext and layout for document image understanding (2019)\\n[38] Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for doc-\\nument\\nlayout\\nanalysis.\\nIn:\\n2019\\nInternational\\nConference\\non\\nDocument\\nAnalysis\\nand\\nRecognition\\n(ICDAR).\\npp.\\n1015–1022.\\nIEEE\\n(Sep\\n2019).\\nhttps://doi.org/10.1109/ICDAR.2019.00166'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 0}, page_content='An Introduction to Convolutional Neural Networks\\nKeiron O’Shea1 and Ryan Nash2\\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\\nkeo7@aber.ac.uk\\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\\n4YW\\nnashrd@live.lancs.ac.uk\\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\\nbiologically inspired computational models are able to far exceed the per-\\nformance of previous forms of artiﬁcial intelligence in common machine\\nlearning tasks. One of the most impressive forms of ANN architecture is\\nthat of the Convolutional Neural Network (CNN). CNNs are primarily\\nused to solve difﬁcult image-driven pattern recognition tasks and with\\ntheir precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently\\npublished papers and newly formed techniques in developing these bril-\\nliantly fantastic image recognition models. This introduction assumes you\\nare familiar with the fundamentals of ANNs and machine learning.\\nKeywords: Pattern recognition, artiﬁcial neural networks, machine learn-\\ning, image analysis\\n1\\nIntroduction\\nArtiﬁcial Neural Networks (ANNs) are computational processing systems of\\nwhich are heavily inspired by way biological nervous systems (such as the hu-\\nman brain) operate. ANNs are mainly comprised of a high number of intercon-\\nnected computational nodes (referred to as neurons), of which work entwine in\\na distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would\\nload the input, usually in the form of a multidimensional vector to the input\\nlayer of which will distribute it to the hidden layers. The hidden layers will then\\nmake decisions from the previous layer and weigh up how a stochastic change\\nwithin itself detriments or improves the ﬁnal output, and this is referred to as\\nthe process of learning. Having multiple hidden layers stacked upon each-other\\nis commonly called deep learning.\\narXiv:1511.08458v2  [cs.NE]  2 Dec 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 1}, page_content='2\\nKeiron O’Shea et al.\\nInput 1\\nInput 2\\nInput 3\\nInput 4\\nInput Layer\\nHidden Layer\\nOutput Layer\\nOutput\\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\\nof a input layer, a hidden layer and an output layer. This structure is the basis\\nof a number of common ANN architectures, included but not limited to Feed-\\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\\nRecurrent Neural Networks (RNNs).\\nThe two key learning paradigms in image processing tasks are supervised and\\nunsupervised learning. Supervised learning is learning through pre-labelled\\ninputs, which act as targets. For each training example there will be a set of\\ninput values (vectors) and one or more associated designated output values.\\nThe goal of this form of training is to reduce the models overall classiﬁcation\\nerror, through correct calculation of the output value of training example by\\ntraining.\\nUnsupervised learning differs in that the training set does not include any la-\\nbels. Success is usually determined by whether the network is able to reduce or\\nincrease an associated cost function. However, it is important to note that most\\nimage-focused pattern-recognition tasks usually depend on classiﬁcation using\\nsupervised learning.\\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs\\nin that they are comprised of neurons that self-optimise through learning. Each\\nneuron will still receive an input and perform a operation (such as a scalar\\nproduct followed by a non-linear function) - the basis of countless ANNs. From\\nthe input raw image vectors to the ﬁnal output of the class score, the entire of\\nthe network will still express a single perceptive score function (the weight).\\nThe last layer will contain loss functions associated with the classes, and all of\\nthe regular tips and tricks developed for traditional ANNs still apply.\\nThe only notable difference between CNNs and traditional ANNs is that CNNs\\nare primarily used in the ﬁeld of pattern recognition within images. This allows\\nus to encode image-speciﬁc features into the architecture, making the network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 2}, page_content='Introduction to Convolutional Neural Networks\\n3\\nmore suited for image-focused tasks - whilst further reducing the parameters\\nrequired to set up the model.\\nOne of the largest limitations of traditional forms of ANN is that they tend to\\nstruggle with the computational complexity required to compute image data.\\nCommon machine learning benchmarking datasets such as the MNIST database\\nof handwritten digits are suitable for most forms of ANN, due to its relatively\\nsmall image dimensionality of just 28 × 28. With this dataset a single neuron in\\nthe ﬁrst hidden layer will contain 784 weights (28×28×1 where 1 bare in mind\\nthat MNIST is normalised to just black and white values), which is manageable\\nfor most forms of ANN.\\nIf you consider a more substantial coloured image input of 64 × 64, the number\\nof weights on just a single neuron of the ﬁrst layer increases substantially to\\n12, 288. Also take into account that to deal with this scale of input, the network\\nwill also need to be a lot larger than one used to classify colour-normalised\\nMNIST digits, then you will understand the drawbacks of using such models.\\n1.1\\nOverﬁtting\\nBut why does it matter? Surely we could just increase the number of hidden lay-\\ners in our network, and perhaps increase the number of neurons within them?\\nThe simple answer to this question is no. This is down to two reasons, one be-\\ning the simple problem of not having unlimited computational power and time\\nto train these huge ANNs.\\nThe second reason is stopping or reducing the effects of overﬁtting. Overﬁtting\\nis basically when a network is unable to learn effectively due to a number of\\nreasons. It is an important concept of most, if not all machine learning algo-\\nrithms and it is important that every precaution is taken as to reduce its effects.\\nIf our models were to exhibit signs of overﬁtting then we may see a reduced\\nability to pinpoint generalised features for not only our training dataset, but\\nalso our test and prediction sets.\\nThis is the main reason behind reducing the complexity of our ANNs. The less\\nparameters required to train, the less likely the network will overﬁt - and of\\ncourse, improve the predictive performance of the model.\\n2\\nCNN architecture\\nAs noted earlier, CNNs primarily focus on the basis that the input will be com-\\nprised of images. This focuses the architecture to be set up in way to best suit\\nthe need for dealing with the speciﬁc type of data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 3}, page_content='4\\nKeiron O’Shea et al.\\nOne of the key differences is that the neurons that the layers within the CNN\\nare comprised of neurons organised into three dimensions, the spatial dimen-\\nsionality of the input (height and the width) and the depth. The depth does not\\nrefer to the total number of layers within the ANN, but the third dimension of a\\nactivation volume. Unlike standard ANNS, the neurons within any given layer\\nwill only connect to a small region of the layer preceding it.\\nIn practice this would mean that for the example given earlier, the input ’vol-\\nume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), lead-\\ning to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where\\nn represents the possible number of classes) as we would have condensed the\\nfull input dimensionality into a smaller volume of class scores ﬁled across the\\ndepth dimension.\\n2.1\\nOverall architecture\\nCNNs are comprised of three types of layers. These are convolutional layers,\\npooling layers and fully-connected layers. When these layers are stacked, a\\nCNN architecture has been formed. A simpliﬁed CNN architecture for MNIST\\nclassiﬁcation is illustrated in Figure 2.\\ninput\\n0\\n9\\nconvolution\\n w/ReLu\\npooling\\noutput\\nfully-connected\\nw/ ReLu\\nfully-connected\\n...\\nFig. 2: An simple CNN architecture, comprised of just ﬁve layers\\nThe basic functionality of the example CNN above can be broken down into\\nfour key areas.\\n1. As found in other forms of ANN, the input layer will hold the pixel values\\nof the image.\\n2. The convolutional layer will determine the output of neurons of which are\\nconnected to local regions of the input through the calculation of the scalar\\nproduct between their weights and the region connected to the input vol-\\nume. The rectiﬁed linear unit (commonly shortened to ReLu) aims to apply'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 4}, page_content='Introduction to Convolutional Neural Networks\\n5\\nan ’elementwise’ activation function such as sigmoid to the output of the\\nactivation produced by the previous layer.\\n3. The pooling layer will then simply perform downsampling along the spa-\\ntial dimensionality of the given input, further reducing the number of pa-\\nrameters within that activation.\\n4. The fully-connected layers will then perform the same duties found in\\nstandard ANNs and attempt to produce class scores from the activations,\\nto be used for classiﬁcation. It is also suggested that ReLu may be used\\nbetween these layers, as to improve performance.\\nThrough this simple method of transformation, CNNs are able to transform\\nthe original input layer by layer using convolutional and downsampling tech-\\nniques to produce class scores for classiﬁcation and regression purposes.\\nFig. 3: Activations taken from the ﬁrst convolutional layer of a simplistic deep\\nCNN, after training on the MNIST database of handwritten digits. If you look\\ncarefully, you can see that the network has successfully picked up on character-\\nistics unique to speciﬁc numeric digits.\\nHowever, it is important to note that simply understanding the overall archi-\\ntecture of a CNN architecture will not sufﬁce. The creation and optimisation\\nof these models can take quite some time, and can be quite confusing. We will\\nnow explore in detail the individual layers, detailing their hyperparameters\\nand connectivities.\\n2.2\\nConvolutional layer\\nAs the name implies, the convolutional layer plays a vital role in how CNNs\\noperate. The layers parameters focus around the use of learnable kernels.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 5}, page_content='6\\nKeiron O’Shea et al.\\nThese kernels are usually small in spatial dimensionality, but spreads along the\\nentirety of the depth of the input. When the data hits a convolutional layer,\\nthe layer convolves each ﬁlter across the spatial dimensionality of the input to\\nproduce a 2D activation map. These activation maps can be visualised, as seen\\nin Figure 3.\\nAs we glide through the input, the scalar product is calculated for each value in\\nthat kernel. (Figure 4) From this the network will learn kernels that ’ﬁre’ when\\nthey see a speciﬁc feature at a given spatial position of the input. These are\\ncommonly known as activations.\\n0\\n0\\n0\\n1\\n0\\n2\\n0\\n1\\n1\\n4\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n-4\\n-8\\nPooled Vector\\nKernel\\nDestination Pixel\\n0\\n0\\n0\\n1\\n0\\n2\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n2\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\nInput Vector\\nFig. 4: A visual representation of a convolutional layer. The centre element of the\\nkernel is placed over the input vector, of which is then calculated and replaced\\nwith a weighted sum of itself and any nearby pixels.\\nEvery kernel will have a corresponding activation map, of which will be stacked\\nalong the depth dimension to form the full output volume from the convolu-\\ntional layer.\\nAs we alluded to earlier, training ANNs on inputs such as images results in\\nmodels of which are too big to train effectively. This comes down to the fully-\\nconnected manner of standard ANN neurons, so to mitigate against this every\\nneuron in a convolutional layer is only connected to small region of the input\\nvolume. The dimensionality of this region is commonly referred to as the re-\\nceptive ﬁeld size of the neuron. The magnitude of the connectivity through the\\ndepth is nearly always equal to the depth of the input.\\nFor example, if the input to the network is an image of size 64 × 64 × 3 (a RGB-\\ncoloured image with a dimensionality of 64 × 64) and we set the receptive ﬁeld\\nsize as 6 × 6, we would have a total of 108 weights on each neuron within the\\nconvolutional layer. (6 × 6 × 3 where 3 is the magnitude of connectivity across\\nthe depth of the volume) To put this into perspective, a standard neuron seen\\nin other forms of ANN would contain 12, 288 weights each.\\nConvolutional layers are also able to signiﬁcantly reduce the complexity of the\\nmodel through the optimisation of its output. These are optimised through\\nthree hyperparameters, the depth, the stride and setting zero-padding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 6}, page_content='Introduction to Convolutional Neural Networks\\n7\\nThe depth of the output volume produced by the convolutional layers can be\\nmanually set through the number of neurons within the layer to a the same\\nregion of the input. This can be seen with other forms of ANNs, where the\\nall of the neurons in the hidden layer are directly connected to every single\\nneuron beforehand. Reducing this hyperparameter can signiﬁcantly minimise\\nthe total number of neurons of the network, but it can also signiﬁcantly reduce\\nthe pattern recognition capabilities of the model.\\nWe are also able to deﬁne the stride in which we set the depth around the spatial\\ndimensionality of the input in order to place the receptive ﬁeld. For example if\\nwe were to set a stride as 1, then we would have a heavily overlapped receptive\\nﬁeld producing extremely large activations. Alternatively, setting the stride to a\\ngreater number will reduce the amount of overlapping and produce an output\\nof lower spatial dimensions.\\nZero-padding is the simple process of padding the border of the input, and\\nis an effective method to give further control as to the dimensionality of the\\noutput volumes.\\nIt is important to understand that through using these techniques, we will alter\\nthe spatial dimensionality of the convolutional layers output. To calculate this,\\nyou can make use of the following formula:\\n(V −R) + 2Z\\nS + 1\\nWhere V represents the input volume size (height×width×depth), R represents\\nthe receptive ﬁeld size, Z is the amount of zero padding set and S referring to\\nthe stride. If the calculated result from this equation is not equal to a whole\\ninteger then the stride has been incorrectly set, as the neurons will be unable to\\nﬁt neatly across the given input.\\nDespite our best efforts so far we will still ﬁnd that our models are still enor-\\nmous if we use an image input of any real dimensionality. However, methods\\nhave been developed as to greatly curtail the overall number of parameters\\nwithin the convolutional layer.\\nParameter sharing works on the assumption that if one region feature is useful\\nto compute at a set spatial region, then it is likely to be useful in another region.\\nIf we constrain each individual activation map within the output volume to the\\nsame weights and bias, then we will see a massive reduction in the number of\\nparameters being produced by the convolutional layer.\\nAs a result of this as the backpropagation stage occurs, each neuron in the out-\\nput will represent the overall gradient of which can be totalled across the depth\\n- thus only updating a single set of weights, as opposed to every single one.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 7}, page_content='8\\nKeiron O’Shea et al.\\n2.3\\nPooling layer\\nPooling layers aim to gradually reduce the dimensionality of the representa-\\ntion, and thus further reduce the number of parameters and the computational\\ncomplexity of the model.\\nThe pooling layer operates over each activation map in the input, and scales\\nits dimensionality using the “MAX” function. In most CNNs, these come in the\\nform of max-pooling layers with kernels of a dimensionality of 2 × 2 applied\\nwith a stride of 2 along the spatial dimensions of the input. This scales the\\nactivation map down to 25% of the original size - whilst maintaining the depth\\nvolume to its standard size.\\nDue to the destructive nature of the pooling layer, there are only two generally\\nobserved methods of max-pooling. Usually, the stride and ﬁlters of the pooling\\nlayers are both set to 2 × 2, which will allow the layer to extend through the\\nentirety of the spatial dimensionality of the input. Furthermore overlapping\\npooling may be utilised, where the stride is set to 2 with a kernel size set to\\n3. Due to the destructive nature of pooling, having a kernel size above 3 will\\nusually greatly decrease the performance of the model.\\nIt is also important to understand that beyond max-pooling, CNN architectures\\nmay contain general-pooling. General pooling layers are comprised of pooling\\nneurons that are able to perform a multitude of common operations including\\nL1/L2-normalisation, and average pooling. However, this tutorial will primar-\\nily focus on the use of max-pooling.\\n2.4\\nFully-connected layer\\nThe fully-connected layer contains neurons of which are directly connected to\\nthe neurons in the two adjacent layers, without being connected to any layers\\nwithin them. This is analogous to way that neurons are arranged in traditional\\nforms of ANN. (Figure 1)\\n3\\nRecipes\\nDespite the relatively small number of layers required to form a CNN, there\\nis no set way of formulating a CNN architecture. That being said, it would be\\nidiotic to simply throw a few of layers together and expect it to work. Through\\nreading of related literature it is obvious that much like other forms of ANNs,\\nCNNs tend to follow a common architecture. This common architecture is illus-\\ntrated in Figure 2, where convolutional layers are stacked, followed by pooling\\nlayers in a repeated manner before feeding forward to fully-connected layers.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 8}, page_content='Introduction to Convolutional Neural Networks\\n9\\nAnother common CNN architecture is to stack two convolutional layers before\\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\\nstacking multiple convolutional layers allows for more complex features of the\\ninput vector to be selected.\\ninput\\nconvolution w/ ReLu\\npooling\\nconvolution\\nw/ ReLu\\npooling\\nfully-connected\\nw/ ReLu\\nfully-connected\\nconvolution w/ ReLu\\npooling\\n0\\n9\\noutput \\n...\\nFig. 5: A common form of CNN architecture in which convolutional layers are\\nstacked between ReLus continuously before being passed through the pooling\\nlayer, before going between one or many fully connected ReLus.\\nIt is also advised to split large convolutional layers up into many smaller sized\\nconvolutional layers. This is to reduce the amount of computational complexity\\nwithin a given convolutional layer. For example, if you were to stack three con-\\nvolutional layers on top of each other with a receptive ﬁeld of 3×3. Each neuron\\nof the ﬁrst convolutional layer will have a 3×3 view of the input vector. A neu-\\nron on the second convolutional layer will then have a 5 × 5 view of the input\\nvector. A neuron on the third convolutional layer will then have a 7 × 7 view of\\nthe input vector. As these stacks feature non-linearities which in turn allows us\\nto express stronger features of the input with fewer parameters. However, it is\\nimportant to understand that this does come with a distinct memory allocation\\nproblem - especially when making use of the backpropagation algorithm.\\nThe input layer should be recursively divisible by two. Common numbers in-\\nclude 32 × 32, 64 × 64, 96 × 96, 128 × 128 and 224 × 224.\\nWhilst using small ﬁlters, set stride to one and make use of zero-padding as to\\nensure that the convolutional layers do not reconﬁgure any of the dimension-\\nality of the input. The amount of zero-padding to be used should be calculated\\nby taking one away from the receptive ﬁeld size and dividing by two.activation\\nCNNs are extremely powerful machine learning algorithms, however they can\\nbe horrendously resource-heavy. An example of this problem could be in ﬁlter-\\ning a large image (anything over 128 × 128 could be considered large), so if the\\ninput is 227 × 227 (as seen with ImageNet) and we’re ﬁltering with 64 kernels\\neach with a zero padding of then the result will be three activation vectors of\\nsize 227 × 227 × 64 - which calculates to roughly 10 million activations - or an\\nenormous 70 megabytes of memory per image. In this case you have two op-\\ntions. Firstly, you can reduce the spatial dimensionality of the input images by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 9}, page_content='10\\nKeiron O’Shea et al.\\nresizing the raw images to something a little less heavy. Alternatively, you can\\ngo against everything we stated earlier in this document and opt for larger ﬁlter\\nsizes with a larger stride (2, as opposed to 1).\\nIn addition to the few rules-of-thumb outlined above, it is also important to ac-\\nknowledge a few ’tricks’ about generalised ANN training techniques. The au-\\nthors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training\\nRestricted Boltzmann Machines”.\\n4\\nConclusion\\nConvolutional Neural Networks differ to other forms of Artiﬁcal Neural Net-\\nwork in that instead of focusing on the entirety of the problem domain, knowl-\\nedge about the speciﬁc type of input is exploited. This in turn allows for a much\\nsimpler network architecture to be set up.\\nThis paper has outlined the basic concepts of Convolutional Neural Networks,\\nexplaining the layers required to build one and detailing how best to structure\\nthe network in most image analysis tasks.\\nResearch in the ﬁeld of image analysis using neural networks has somewhat\\nslowed in recent times. This is partly due to the incorrect belief surrounding the\\nlevel of complexity and knowledge required to begin modelling these superbly\\npowerful machine learning algorithms. The authors hope that this paper has\\nin some way reduced this confusion, and made the ﬁeld more accessible to\\nbeginners.\\nAcknowledgements\\nThe authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for\\nuseful discussion and suggestions.\\nReferences\\n1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-\\nage classiﬁcation. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE\\nConference on. pp. 3642–3649. IEEE (2012)\\n2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in\\nbreast cancer histology images with deep neural networks. In: Medical Image Com-\\nputing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer\\n(2013)\\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,\\nhigh performance convolutional neural networks for image classiﬁcation. In: IJCAI\\nProceedings-International Joint Conference on Artiﬁcial Intelligence. vol. 22, p. 1237\\n(2011)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 10}, page_content='Introduction to Convolutional Neural Networks\\n11\\n4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural\\nnetwork committees for handwritten character classiﬁcation. In: Document Analysis\\nand Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE\\n(2011)\\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-\\nworksa review. Pattern recognition 35(10), 2279–2301 (2002)\\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware\\naccelerated convolutional neural networks for synthetic vision systems. In: Circuits\\nand Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.\\n257–260. IEEE (2010)\\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum\\n9(1), 926 (2010)\\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-\\nproving neural networks by preventing co-adaptation of feature detectors. arXiv\\npreprint arXiv:1207.0580 (2012)\\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action\\nrecognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),\\n221–231 (2013)\\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-\\nscale video classiﬁcation with convolutional neural networks. In: Computer Vision\\nand Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE\\n(2014)\\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In: Advances in neural information processing systems.\\npp. 1097–1105 (2012)\\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,\\nL.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-\\ntation 1(4), 541–551 (1989)\\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-\\nument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.\\nNeural Networks, IEEE Transactions on 9(4), 685–696 (1998)\\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-\\nworks applied to visual document analysis. In: null. p. 958. IEEE (2003)\\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of\\nToronto (2013)\\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-\\nvolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.\\nIEEE. pp. 224–229. IEEE (2005)\\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:\\nAdvances in Neural Information Processing Systems. pp. 2553–2561 (2013)\\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks\\n(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-\\nceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional\\nneural networks. arXiv preprint arXiv:1301.3557 (2013)\\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:\\nComputer Vision–ECCV 2014, pp. 818–833. Springer (2014)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 0}, page_content='Published as a conference paper at ICLR 2021\\nAN IMAGE IS WORTH 16X16 WORDS:\\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\\n∗equal technical contribution, †equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\nABSTRACT\\nWhile the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1\\nINTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiﬁcation in supervised fashion.\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 1}, page_content='Published as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufﬁcient amounts of data.\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefﬁciently on hardware accelerators.\\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019; Li et al., 2019).\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 2}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 3}, page_content='Published as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\nz0 = [xclass; x1\\npE; x2\\npE; · · · ; xN\\np E] + Epos,\\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\\n(1)\\nz′\\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\\nℓ= 1 . . . L\\n(2)\\nzℓ= MLP(LN(z′\\nℓ)) + z′\\nℓ,\\nℓ= 1 . . . L\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\nHybrid Architecture.\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classiﬁcation input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n4.1\\nSETUP\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 4}, page_content='Published as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\\nimagery, and Structured – tasks that require geometric understanding like localization.\\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\nMetrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\\nwhere ﬁne-tuning would be too costly.\\n4.2\\nCOMPARISON TO STATE OF THE ART\\nWe ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 5}, page_content='Published as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfﬁcientNet-L2)\\nImageNet\\n88.55 ± 0.04\\n87.76 ± 0.03\\n85.30 ± 0.02\\n87.54 ± 0.02\\n88.4/88.5∗\\nImageNet ReaL\\n90.72 ± 0.05\\n90.54 ± 0.03\\n88.62 ± 0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50 ± 0.06\\n99.42 ± 0.03\\n99.15 ± 0.03\\n99.37 ± 0.06\\n−\\nCIFAR-100\\n94.55 ± 0.04\\n93.90 ± 0.05\\n93.25 ± 0.05\\n93.51 ± 0.08\\n−\\nOxford-IIIT Pets\\n97.56 ± 0.03\\n97.32 ± 0.11\\n94.67 ± 0.15\\n96.62 ± 0.23\\n−\\nOxford Flowers-102\\n99.68 ± 0.02\\n99.74 ± 0.00\\n99.61 ± 0.02\\n99.63 ± 0.03\\n−\\nVTAB (19 tasks)\\n77.63 ± 0.23\\n76.28 ± 0.46\\n72.72 ± 0.21\\n76.29 ± 1.70\\n−\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\\nin Touvron et al. (2020).\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy [%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\\nexperiments.\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\\nresolution increase during ﬁne-tuning improves the performance.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 6}, page_content='Published as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy [%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1 [%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size. ResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training. ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n102\\n103\\n90\\n95\\nTransfer accuracy [%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufﬁcient, even beneﬁcial.\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\\nis an exciting direction of future work.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 7}, page_content='Published as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Speciﬁcally, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model. Other attention heads\\nhave consistently small attention distances in the low layers. This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\\nregions that are semantically relevant for classiﬁcation (Figure 6).\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks. However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 8}, page_content='Published as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work.\\n5\\nCONCLUSION\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\nREFERENCES\\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views. In NeurIPS, 2019.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 9}, page_content='Published as a conference paper at ICLR 2021\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\\nICLR, 2019.\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\\nIn ICCV, 2019.\\nLucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\\nwe done with imagenet? arXiv, 2020.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv, 2020.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\\npixels. In ICML, 2020a.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\\nfor contrastive learning of visual representations. In ICML, 2020b.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. arXiv, 2019.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In ICLR, 2020.\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL, 2019.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. In CVPR, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\nMomentum contrast for\\nunsupervised visual representation learning. In CVPR, 2020.\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\\nmensional transformers. arXiv, 2019.\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\\nIn ICCV, 2019.\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\\nICML, 2020.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 10}, page_content='Published as a conference paper at ICLR 2021\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\\ntion. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten.\\nExploring the limits of weakly supervised\\npretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\\nICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\\n2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\\nJournal on Control and Optimization, 30(4):838–855, 1992.\\ndoi: 10.1137/0330046.\\nURL\\nhttps://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\\npreprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\\nStand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\\nfectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\\nmodel for video and language representation learning. In ICCV, 2019.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 11}, page_content='Published as a conference paper at ICLR 2021\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy. In NeurIPS. 2019.\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\\n2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\\narXiv preprint\\narXiv:2003.07853, 2020b.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. In ACL, 2019.\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\\nCVPR, 2018.\\nDirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\\nICLR, 2019.\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\\nfor computer vision. arxiv, 2020.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\\nimproves imagenet classiﬁcation. In CVPR, 2020.\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\\nSupervised Learning. In ICCV, 2019a.\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\\npreprint arXiv:1910.04867, 2019b.\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\\nCVPR, 2020.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 12}, page_content='Published as a conference paper at ICLR 2021\\nModels\\nDataset\\nEpochs\\nBase LR\\nLR decay\\nWeight decay\\nDropout\\nViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/32\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-H/14\\nJFT-300M\\n14\\n3 · 10−4\\nlinear\\n0.1\\n0.0\\nR50x{1,2}\\nJFT-300M\\n7\\n10−3\\nlinear\\n0.1\\n0.0\\nR101x1\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR152x{1,2}\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/32\\nJFT-300M\\n7\\n2 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-B/{16,32}\\nImageNet-21k\\n90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-L/{16,32}\\nImageNet-21k\\n30/90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-∗\\nImageNet\\n300\\n3 · 10−3\\ncosine\\n0.3\\n0.1\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\\nclipping at global norm 1. Training resolution is 224.\\nAPPENDIX\\nA\\nMULTIHEAD SELF-ATTENTION\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\\ntwo elements of the sequence and their respective query qi and key kj representations.\\n[q, k, v] = zUqkv\\nUqkv ∈RD×3Dh,\\n(5)\\nA = softmax\\n\\x10\\nqk⊤/\\np\\nDh\\n\\x11\\nA ∈RN×N,\\n(6)\\nSA(z) = Av .\\n(7)\\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\\nMSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\\nUmsa ∈Rk·Dh×D\\n(8)\\nB\\nEXPERIMENT DETAILS\\nB.1\\nTRAINING\\nTable 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\nB.1.1\\nFINE-TUNING\\nWe ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 13}, page_content='Published as a conference paper at ICLR 2021\\nDataset\\nSteps\\nBase LR\\nImageNet\\n20 000\\n{0.003, 0.01, 0.03, 0.06}\\nCIFAR100\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nCIFAR10\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nOxford-IIIT Pets\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nOxford Flowers-102\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nVTAB (19 tasks)\\n2 500\\n0.01\\nTable 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\\nﬁne-tuning resolution is 384.\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks.\\nB.1.2\\nSELF-SUPERVISION\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\\npatch representations.\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\\n(2019) but results were also slightly worse on our few-shot metrics.\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\\nImageNet.\\nC\\nADDITIONAL RESULTS\\nWe report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 14}, page_content='Published as a conference paper at ICLR 2021\\nViT-B/16\\nViT-B/32\\nViT-L/16\\nViT-L/32\\nViT-H/14\\nImageNet\\nCIFAR-10\\n98.13\\n97.77\\n97.86\\n97.94\\n-\\nCIFAR-100\\n87.13\\n86.31\\n86.35\\n87.07\\n-\\nImageNet\\n77.91\\n73.38\\n76.53\\n71.16\\n-\\nImageNet ReaL\\n83.57\\n79.56\\n82.19\\n77.83\\n-\\nOxford Flowers-102\\n89.49\\n85.43\\n89.66\\n86.36\\n-\\nOxford-IIIT-Pets\\n93.81\\n92.04\\n93.64\\n91.35\\n-\\nImageNet-21k\\nCIFAR-10\\n98.95\\n98.79\\n99.16\\n99.13\\n99.27\\nCIFAR-100\\n91.67\\n91.97\\n93.44\\n93.04\\n93.82\\nImageNet\\n83.97\\n81.28\\n85.15\\n80.99\\n85.13\\nImageNet ReaL\\n88.35\\n86.63\\n88.40\\n85.65\\n88.70\\nOxford Flowers-102\\n99.38\\n99.11\\n99.61\\n99.19\\n99.51\\nOxford-IIIT-Pets\\n94.43\\n93.02\\n94.73\\n93.09\\n94.82\\nJFT-300M\\nCIFAR-10\\n99.00\\n98.61\\n99.38\\n99.19\\n99.50\\nCIFAR-100\\n91.87\\n90.49\\n94.04\\n92.52\\n94.55\\nImageNet\\n84.15\\n80.73\\n87.12\\n84.37\\n88.04\\nImageNet ReaL\\n88.85\\n86.27\\n89.99\\n88.28\\n90.33\\nOxford Flowers-102\\n99.56\\n99.27\\n99.56\\n99.45\\n99.68\\nOxford-IIIT-Pets\\n95.80\\n93.40\\n97.11\\n95.83\\n97.56\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\nEpochs\\nImageNet\\nImageNet ReaL\\nCIFAR-10\\nCIFAR-100\\nPets\\nFlowers\\nexaFLOPs\\nname\\nViT-B/32\\n7\\n80.73\\n86.27\\n98.61\\n90.49\\n93.40\\n99.27\\n55\\nViT-B/16\\n7\\n84.15\\n88.85\\n99.00\\n91.87\\n95.80\\n99.56\\n224\\nViT-L/32\\n7\\n84.37\\n88.28\\n99.19\\n92.52\\n95.83\\n99.45\\n196\\nViT-L/16\\n7\\n86.30\\n89.43\\n99.38\\n93.46\\n96.81\\n99.66\\n783\\nViT-L/16\\n14\\n87.12\\n89.99\\n99.38\\n94.04\\n97.11\\n99.56\\n1567\\nViT-H/14\\n14\\n88.08\\n90.36\\n99.50\\n94.71\\n97.11\\n99.71\\n4262\\nResNet50x1\\n7\\n77.54\\n84.56\\n97.67\\n86.07\\n91.11\\n94.26\\n50\\nResNet50x2\\n7\\n82.12\\n87.94\\n98.29\\n89.20\\n93.43\\n97.02\\n199\\nResNet101x1\\n7\\n80.67\\n87.07\\n98.48\\n89.17\\n94.08\\n95.95\\n96\\nResNet152x1\\n7\\n81.88\\n87.96\\n98.82\\n90.22\\n94.17\\n96.94\\n141\\nResNet152x2\\n7\\n84.97\\n89.69\\n99.06\\n92.05\\n95.37\\n98.62\\n563\\nResNet152x2\\n14\\n85.56\\n89.89\\n99.24\\n91.92\\n95.75\\n98.75\\n1126\\nResNet200x3\\n14\\n87.22\\n90.15\\n99.34\\n93.53\\n96.32\\n99.04\\n3306\\nR50x1+ViT-B/32\\n7\\n84.90\\n89.15\\n99.01\\n92.24\\n95.75\\n99.46\\n106\\nR50x1+ViT-B/16\\n7\\n85.58\\n89.65\\n99.14\\n92.63\\n96.65\\n99.40\\n274\\nR50x1+ViT-L/32\\n7\\n85.68\\n89.04\\n99.24\\n92.93\\n96.97\\n99.43\\n246\\nR50x1+ViT-L/16\\n7\\n86.60\\n89.72\\n99.18\\n93.64\\n97.03\\n99.40\\n859\\nR50x1+ViT-L/16\\n14\\n87.12\\n89.76\\n99.31\\n93.89\\n97.36\\n99.11\\n1668\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\\naFLOPs).\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\\nvarying size, as well as the estimated computational cost of their pre-training.\\nD\\nADDITIONAL ANALYSES\\nD.1\\nSGD VS. ADAM FOR RESNETS\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 15}, page_content='Published as a conference paper at ICLR 2021\\nResNet50\\nResNet152x2\\nDataset\\nAdam\\nSGD\\nAdam\\nSGD\\nImageNet\\n77.54\\n78.24\\n84.97\\n84.37\\nCIFAR10\\n97.67\\n97.46\\n99.06\\n99.07\\nCIFAR100\\n86.07\\n85.17\\n92.05\\n91.06\\nOxford-IIIT Pets\\n91.11\\n91.00\\n95.37\\n94.79\\nOxford Flowers-102\\n94.26\\n92.06\\n98.62\\n99.32\\nAverage\\n89.33\\n88.79\\n94.01\\n93.72\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n100\\n101\\nRelative Compute\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nImageNet 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\n100\\n101\\nRelative Compute\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\nperformance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\\nfor 7 epochs, not 30.\\nD.2\\nTRANSFORMER SHAPE\\nWe ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\\nall dimensions proportionally results in robust improvements.\\nD.3\\nHEAD TYPE AND CLASS TOKEN\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\nThis design is inherited from the Transformer model for text, and we use it throughout the main\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 16}, page_content='Published as a conference paper at ICLR 2021\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nEpochs of training\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nImageNet linear 5-shot accuracy [%]\\nCLS-Token, lr=8e-4\\nGAP, lr=8e-4\\nGAP, lr=3e-4\\nFigure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\\nwell, but require different learning-rates.\\nPos. Emb.\\nDefault/Stem\\nEvery Layer\\nEvery Layer-Shared\\nNo Pos. Emb.\\n0.61382\\nN/A\\nN/A\\n1-D Pos. Emb.\\n0.64206\\n0.63964\\n0.64292\\n2-D Pos. Emb.\\n0.64001\\n0.64046\\n0.64022\\nRel. Pos. Emb.\\n0.64032\\nN/A\\nN/A\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\\nImageNet 5-shot linear.\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\\nFigure 9.\\nD.4\\nPOSITIONAL EMBEDDING\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\\ntried the following cases:\\n• Providing no positional information: Considering the inputs as a bag of patches.\\n• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\\nthe raster order (default across all other experiments in this paper).\\n• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\\nembedding for that patch.\\n• Relative positional embeddings: Considering the relative distance between patches to en-\\ncode the spatial information as instead of their absolute position. To do so, we use 1-\\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\\nembedding. Then, we simply run extra attention, where we use the original query (the\\ncontent of query), but use relative positional embeddings as keys. We then use the log-\\nits from the relative attention as a bias term and add it to the logits of the main attention\\n(content-based attention) before applying the softmax.\\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 17}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0002, WD=0.01\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0004, WD=0.1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n14 epochs, LR=0.0004, WD=0.1\\n1\\n1\\nCosine similarity\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\\nhyperparameters (Figure 10).\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nR50x1 + ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\\nheads at one layer. Image width is 224 pixels.\\nD.5\\nEMPIRICAL COMPUTATIONAL COSTS\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 18}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 19}, page_content='Published as a conference paper at ICLR 2021\\n102\\nTotal compute [exaFLOPs]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\n102\\n103\\nPeak inference speed [img/sec/core]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\\nattention and although the sequence length that self-attention operates on is smaller in axial case,\\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\\non TPUs (Figure 13, right).\\nD.7\\nATTENTION DISTANCE\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\nD.8\\nATTENTION MAPS\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\\nfor the mixing of attention across tokens through all layers.\\nD.9\\nOBJECTNET RESULTS\\nWe also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\nD.10\\nVTAB BREAKDOWN\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 20}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/vision_transformer.pdf', 'file_path': '../../docs/vision_transformer.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 21}, page_content='Published as a conference paper at ICLR 2021\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\nCaltech101\\nCIFAR-100\\nDTD\\nFlowers102\\nPets\\nSun397\\nSVHN\\nCamelyon\\nEuroSAT\\nResisc45\\nRetinopathy\\nClevr-Count\\nClevr-Dist\\nDMLab\\ndSpr-Loc\\ndSpr-Ori\\nKITTI-Dist\\nsNORB-Azim\\nsNORB-Elev\\nMean\\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 0}, page_content='Deep Residual Learning for Image Recognition\\nKaiming He\\nXiangyu Zhang\\nShaoqing Ren\\nJian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [41] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNet test set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n50, 40]. Deep networks naturally integrate low/mid/high-\\nlevel features [50] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[41, 44] reveals that network depth is of crucial importance,\\nand the leading results [41, 44, 13, 16] on the challenging\\nImageNet dataset [36] all exploit “very deep” [41] models,\\nwith a depth of sixteen [41] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/\\nand\\nhttp://mscoco.org/dataset/#detections-challenge2015.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0 \\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n \\n \\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n \\n \\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises: Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [1, 9], which\\nhamper convergence from the beginning.\\nThis problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 9, 37, 13] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly.\\nUnexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding\\nmore layers to a suitably deep model leads to higher train-\\ning error, as reported in [11, 42] and thoroughly veriﬁed by\\nour experiments. Fig. 1 shows a typical example.\\nThe degradation (of training accuracy) indicates that not\\nall systems are similarly easy to optimize. Let us consider a\\nshallower architecture and its deeper counterpart that adds\\nmore layers onto it. There exists a solution by construction\\nto the deeper model: the added layers are identity mapping,\\nand the other layers are copied from the learned shallower\\nmodel. The existence of this constructed solution indicates\\nthat a deeper model should produce no higher training error\\nthan its shallower counterpart. But experiments show that\\nour current solvers on hand are unable to ﬁnd solutions that\\n1\\narXiv:1512.03385v1  [cs.CV]  10 Dec 2015'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 1}, page_content='identity\\nweight layer\\nweight layer\\nrelu\\nrelu\\nF(x)\\x01+\\x01x\\nx\\nF(x)\\nx\\nFigure 2. Residual learning: a building block.\\nare comparably good or better than the constructed solution\\n(or unable to do so in feasible time).\\nIn this paper, we address the degradation problem by\\nintroducing a deep residual learning framework.\\nIn-\\nstead of hoping each few stacked layers directly ﬁt a\\ndesired underlying mapping, we explicitly let these lay-\\ners ﬁt a residual mapping. Formally, denoting the desired\\nunderlying mapping as H(x), we let the stacked nonlinear\\nlayers ﬁt another mapping of F(x) := H(x)−x. The orig-\\ninal mapping is recast into F(x)+x. We hypothesize that it\\nis easier to optimize the residual mapping than to optimize\\nthe original, unreferenced mapping. To the extreme, if an\\nidentity mapping were optimal, it would be easier to push\\nthe residual to zero than to ﬁt an identity mapping by a stack\\nof nonlinear layers.\\nThe formulation of F(x)+x can be realized by feedfor-\\nward neural networks with “shortcut connections” (Fig. 2).\\nShortcut connections [2, 34, 49] are those skipping one or\\nmore layers. In our case, the shortcut connections simply\\nperform identity mapping, and their outputs are added to\\nthe outputs of the stacked layers (Fig. 2). Identity short-\\ncut connections add neither extra parameter nor computa-\\ntional complexity. The entire network can still be trained\\nend-to-end by SGD with backpropagation, and can be eas-\\nily implemented using common libraries (e.g., Caffe [19])\\nwithout modifying the solvers.\\nWe present comprehensive experiments on ImageNet\\n[36] to show the degradation problem and evaluate our\\nmethod. We show that: 1) Our extremely deep residual nets\\nare easy to optimize, but the counterpart “plain” nets (that\\nsimply stack layers) exhibit higher training error when the\\ndepth increases; 2) Our deep residual nets can easily enjoy\\naccuracy gains from greatly increased depth, producing re-\\nsults substantially better than previous networks.\\nSimilar phenomena are also shown on the CIFAR-10 set\\n[20], suggesting that the optimization difﬁculties and the\\neffects of our method are not just akin to a particular dataset.\\nWe present successfully trained models on this dataset with\\nover 100 layers, and explore models with over 1000 layers.\\nOn the ImageNet classiﬁcation dataset [36], we obtain\\nexcellent results by extremely deep residual nets. Our 152-\\nlayer residual net is the deepest network ever presented on\\nImageNet, while still having lower complexity than VGG\\nnets [41].\\nOur ensemble has 3.57% top-5 error on the\\nImageNet test set, and won the 1st place in the ILSVRC\\n2015 classiﬁcation competition. The extremely deep rep-\\nresentations also have excellent generalization performance\\non other recognition tasks, and lead us to further win the\\n1st places on: ImageNet detection, ImageNet localization,\\nCOCO detection, and COCO segmentation in ILSVRC &\\nCOCO 2015 competitions. This strong evidence shows that\\nthe residual learning principle is generic, and we expect that\\nit is applicable in other vision and non-vision problems.\\n2. Related Work\\nResidual Representations. In image recognition, VLAD\\n[18] is a representation that encodes by the residual vectors\\nwith respect to a dictionary, and Fisher Vector [30] can be\\nformulated as a probabilistic version [18] of VLAD. Both\\nof them are powerful shallow representations for image re-\\ntrieval and classiﬁcation [4, 48]. For vector quantization,\\nencoding residual vectors [17] is shown to be more effec-\\ntive than encoding original vectors.\\nIn low-level vision and computer graphics, for solv-\\ning Partial Differential Equations (PDEs), the widely used\\nMultigrid method [3] reformulates the system as subprob-\\nlems at multiple scales, where each subproblem is respon-\\nsible for the residual solution between a coarser and a ﬁner\\nscale. An alternative to Multigrid is hierarchical basis pre-\\nconditioning [45, 46], which relies on variables that repre-\\nsent residual vectors between two scales. It has been shown\\n[3, 45, 46] that these solvers converge much faster than stan-\\ndard solvers that are unaware of the residual nature of the\\nsolutions. These methods suggest that a good reformulation\\nor preconditioning can simplify the optimization.\\nShortcut Connections. Practices and theories that lead to\\nshortcut connections [2, 34, 49] have been studied for a long\\ntime. An early practice of training multi-layer perceptrons\\n(MLPs) is to add a linear layer connected from the network\\ninput to the output [34, 49]. In [44, 24], a few interme-\\ndiate layers are directly connected to auxiliary classiﬁers\\nfor addressing vanishing/exploding gradients. The papers\\nof [39, 38, 31, 47] propose methods for centering layer re-\\nsponses, gradients, and propagated errors, implemented by\\nshortcut connections. In [44], an “inception” layer is com-\\nposed of a shortcut branch and a few deeper branches.\\nConcurrent with our work, “highway networks” [42, 43]\\npresent shortcut connections with gating functions [15].\\nThese gates are data-dependent and have parameters, in\\ncontrast to our identity shortcuts that are parameter-free.\\nWhen a gated shortcut is “closed” (approaching zero), the\\nlayers in highway networks represent non-residual func-\\ntions.\\nOn the contrary, our formulation always learns\\nresidual functions; our identity shortcuts are never closed,\\nand all information is always passed through, with addi-\\ntional residual functions to be learned. In addition, high-\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 2}, page_content='way networks have not demonstrated accuracy gains with\\nextremely increased depth (e.g., over 100 layers).\\n3. Deep Residual Learning\\n3.1. Residual Learning\\nLet us consider H(x) as an underlying mapping to be\\nﬁt by a few stacked layers (not necessarily the entire net),\\nwith x denoting the inputs to the ﬁrst of these layers. If one\\nhypothesizes that multiple nonlinear layers can asymptoti-\\ncally approximate complicated functions2, then it is equiv-\\nalent to hypothesize that they can asymptotically approxi-\\nmate the residual functions, i.e., H(x) −x (assuming that\\nthe input and output are of the same dimensions).\\nSo\\nrather than expect stacked layers to approximate H(x), we\\nexplicitly let these layers approximate a residual function\\nF(x) := H(x) −x. The original function thus becomes\\nF(x)+x. Although both forms should be able to asymptot-\\nically approximate the desired functions (as hypothesized),\\nthe ease of learning might be different.\\nThis reformulation is motivated by the counterintuitive\\nphenomena about the degradation problem (Fig. 1, left). As\\nwe discussed in the introduction, if the added layers can\\nbe constructed as identity mappings, a deeper model should\\nhave training error no greater than its shallower counter-\\npart.\\nThe degradation problem suggests that the solvers\\nmight have difﬁculties in approximating identity mappings\\nby multiple nonlinear layers. With the residual learning re-\\nformulation, if identity mappings are optimal, the solvers\\nmay simply drive the weights of the multiple nonlinear lay-\\ners toward zero to approach identity mappings.\\nIn real cases, it is unlikely that identity mappings are op-\\ntimal, but our reformulation may help to precondition the\\nproblem. If the optimal function is closer to an identity\\nmapping than to a zero mapping, it should be easier for the\\nsolver to ﬁnd the perturbations with reference to an identity\\nmapping, than to learn the function as a new one. We show\\nby experiments (Fig. 7) that the learned residual functions in\\ngeneral have small responses, suggesting that identity map-\\npings provide reasonable preconditioning.\\n3.2. Identity Mapping by Shortcuts\\nWe adopt residual learning to every few stacked layers.\\nA building block is shown in Fig. 2. Formally, in this paper\\nwe consider a building block deﬁned as:\\ny = F(x, {Wi}) + x.\\n(1)\\nHere x and y are the input and output vectors of the lay-\\ners considered.\\nThe function F(x, {Wi}) represents the\\nresidual mapping to be learned. For the example in Fig. 2\\nthat has two layers, F = W2σ(W1x) in which σ denotes\\n2This hypothesis, however, is still an open question. See [28].\\nReLU [29] and the biases are omitted for simplifying no-\\ntations. The operation F + x is performed by a shortcut\\nconnection and element-wise addition. We adopt the sec-\\nond nonlinearity after the addition (i.e., σ(y), see Fig. 2).\\nThe shortcut connections in Eqn.(1) introduce neither ex-\\ntra parameter nor computation complexity. This is not only\\nattractive in practice but also important in our comparisons\\nbetween plain and residual networks. We can fairly com-\\npare plain/residual networks that simultaneously have the\\nsame number of parameters, depth, width, and computa-\\ntional cost (except for the negligible element-wise addition).\\nThe dimensions of x and F must be equal in Eqn.(1).\\nIf this is not the case (e.g., when changing the input/output\\nchannels), we can perform a linear projection Ws by the\\nshortcut connections to match the dimensions:\\ny = F(x, {Wi}) + Wsx.\\n(2)\\nWe can also use a square matrix Ws in Eqn.(1). But we will\\nshow by experiments that the identity mapping is sufﬁcient\\nfor addressing the degradation problem and is economical,\\nand thus Ws is only used when matching dimensions.\\nThe form of the residual function F is ﬂexible. Exper-\\niments in this paper involve a function F that has two or\\nthree layers (Fig. 5), while more layers are possible. But if\\nF has only a single layer, Eqn.(1) is similar to a linear layer:\\ny = W1x + x, for which we have not observed advantages.\\nWe also note that although the above notations are about\\nfully-connected layers for simplicity, they are applicable to\\nconvolutional layers. The function F(x, {Wi}) can repre-\\nsent multiple convolutional layers. The element-wise addi-\\ntion is performed on two feature maps, channel by channel.\\n3.3. Network Architectures\\nWe have tested various plain/residual nets, and have ob-\\nserved consistent phenomena. To provide instances for dis-\\ncussion, we describe two models for ImageNet as follows.\\nPlain Network. Our plain baselines (Fig. 3, middle) are\\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\\nleft). The convolutional layers mostly have 3×3 ﬁlters and\\nfollow two simple design rules: (i) for the same output\\nfeature map size, the layers have the same number of ﬁl-\\nters; and (ii) if the feature map size is halved, the num-\\nber of ﬁlters is doubled so as to preserve the time com-\\nplexity per layer. We perform downsampling directly by\\nconvolutional layers that have a stride of 2. The network\\nends with a global average pooling layer and a 1000-way\\nfully-connected layer with softmax. The total number of\\nweighted layers is 34 in Fig. 3 (middle).\\nIt is worth noticing that our model has fewer ﬁlters and\\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\\nis only 18% of VGG-19 (19.6 billion FLOPs).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 3}, page_content='7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n3x3 conv, 512\\n3x3 conv, 64\\n3x3 conv, 64\\npool, /2\\n3x3 conv, 128\\n3x3 conv, 128\\npool, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\nfc 4096\\nfc 4096\\nfc 1000\\nimage\\noutput \\nsize: 112\\noutput \\nsize: 224\\noutput \\nsize: 56\\noutput \\nsize: 28\\noutput \\nsize: 14\\noutput \\nsize: 7\\noutput \\nsize: 1\\nVGG-19\\n34-layer plain\\n7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n34-layer residual\\nFigure 3. Example network architectures for ImageNet. Left: the\\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\\nRight: a residual network with 34 parameter layers (3.6 billion\\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\\nmore details and other variants.\\nResidual Network. Based on the above plain network, we\\ninsert shortcut connections (Fig. 3, right) which turn the\\nnetwork into its counterpart residual version. The identity\\nshortcuts (Eqn.(1)) can be directly used when the input and\\noutput are of the same dimensions (solid line shortcuts in\\nFig. 3). When the dimensions increase (dotted line shortcuts\\nin Fig. 3), we consider two options: (A) The shortcut still\\nperforms identity mapping, with extra zero entries padded\\nfor increasing dimensions. This option introduces no extra\\nparameter; (B) The projection shortcut in Eqn.(2) is used to\\nmatch dimensions (done by 1×1 convolutions). For both\\noptions, when the shortcuts go across feature maps of two\\nsizes, they are performed with a stride of 2.\\n3.4. Implementation\\nOur implementation for ImageNet follows the practice\\nin [21, 41]. The image is resized with its shorter side ran-\\ndomly sampled in [256, 480] for scale augmentation [41].\\nA 224×224 crop is randomly sampled from an image or its\\nhorizontal ﬂip, with the per-pixel mean subtracted [21]. The\\nstandard color augmentation in [21] is used. We adopt batch\\nnormalization (BN) [16] right after each convolution and\\nbefore activation, following [16]. We initialize the weights\\nas in [13] and train all plain/residual nets from scratch. We\\nuse SGD with a mini-batch size of 256. The learning rate\\nstarts from 0.1 and is divided by 10 when the error plateaus,\\nand the models are trained for up to 60 × 104 iterations. We\\nuse a weight decay of 0.0001 and a momentum of 0.9. We\\ndo not use dropout [14], following the practice in [16].\\nIn testing, for comparison studies we adopt the standard\\n10-crop testing [21]. For best results, we adopt the fully-\\nconvolutional form as in [41, 13], and average the scores\\nat multiple scales (images are resized such that the shorter\\nside is in {224, 256, 384, 480, 640}).\\n4. Experiments\\n4.1. ImageNet Classiﬁcation\\nWe evaluate our method on the ImageNet 2012 classiﬁ-\\ncation dataset [36] that consists of 1000 classes. The models\\nare trained on the 1.28 million training images, and evalu-\\nated on the 50k validation images. We also obtain a ﬁnal\\nresult on the 100k test images, reported by the test server.\\nWe evaluate both top-1 and top-5 error rates.\\nPlain Networks. We ﬁrst evaluate 18-layer and 34-layer\\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\\n18-layer plain net is of a similar form. See Table 1 for de-\\ntailed architectures.\\nThe results in Table 2 show that the deeper 34-layer plain\\nnet has higher validation error than the shallower 18-layer\\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\\npare their training/validation errors during the training pro-\\ncedure. We have observed the degradation problem - the\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 4}, page_content='layer name output size\\n18-layer\\n34-layer\\n50-layer\\n101-layer\\n152-layer\\nconv1\\n112×112\\n7×7, 64, stride 2\\nconv2 x\\n56×56\\n3×3 max pool, stride 2\\n\\x14\\n3×3, 64\\n3×3, 64\\n\\x15\\n×2\\n\\x14\\n3×3, 64\\n3×3, 64\\n\\x15\\n×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\nconv3 x\\n28×28\\n\\x14\\n3×3, 128\\n3×3, 128\\n\\x15\\n×2\\n\\x14\\n3×3, 128\\n3×3, 128\\n\\x15\\n×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×8\\nconv4 x\\n14×14\\n\\x14\\n3×3, 256\\n3×3, 256\\n\\x15\\n×2\\n\\x14\\n3×3, 256\\n3×3, 256\\n\\x15\\n×6\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×6\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×23\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×36\\nconv5 x\\n7×7\\n\\x14\\n3×3, 512\\n3×3, 512\\n\\x15\\n×2\\n\\x14\\n3×3, 512\\n3×3, 512\\n\\x15\\n×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n1×1\\naverage pool, 1000-d fc, softmax\\nFLOPs\\n1.8×109\\n3.6×109\\n3.8×109\\n7.6×109\\n11.3×109\\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\\n0\\n10\\n20\\n30\\n40\\n50\\n20\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\n \\n \\nplain-18\\nplain-34\\n0\\n10\\n20\\n30\\n40\\n50\\n20\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\n \\n \\nResNet-18\\nResNet-34\\n18-layer\\n34-layer\\n18-layer\\n34-layer\\nFigure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\\ntheir plain counterparts.\\nplain\\nResNet\\n18 layers\\n27.94\\n27.88\\n34 layers\\n28.54\\n25.03\\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\\nHere the ResNets have no extra parameter compared to their plain\\ncounterparts. Fig. 4 shows the training procedures.\\n34-layer plain net has higher training error throughout the\\nwhole training procedure, even though the solution space\\nof the 18-layer plain network is a subspace of that of the\\n34-layer one.\\nWe argue that this optimization difﬁculty is unlikely to\\nbe caused by vanishing gradients. These plain networks are\\ntrained with BN [16], which ensures forward propagated\\nsignals to have non-zero variances. We also verify that the\\nbackward propagated gradients exhibit healthy norms with\\nBN. So neither forward nor backward signals vanish. In\\nfact, the 34-layer plain net is still able to achieve compet-\\nitive accuracy (Table 3), suggesting that the solver works\\nto some extent. We conjecture that the deep plain nets may\\nhave exponentially low convergence rates, which impact the\\nreducing of the training error3. The reason for such opti-\\nmization difﬁculties will be studied in the future.\\nResidual Networks. Next we evaluate 18-layer and 34-\\nlayer residual nets (ResNets). The baseline architectures\\nare the same as the above plain nets, expect that a shortcut\\nconnection is added to each pair of 3×3 ﬁlters as in Fig. 3\\n(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),\\nwe use identity mapping for all shortcuts and zero-padding\\nfor increasing dimensions (option A). So they have no extra\\nparameter compared to the plain counterparts.\\nWe have three major observations from Table 2 and\\nFig. 4. First, the situation is reversed with residual learn-\\ning – the 34-layer ResNet is better than the 18-layer ResNet\\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\\nconsiderably lower training error and is generalizable to the\\nvalidation data. This indicates that the degradation problem\\nis well addressed in this setting and we manage to obtain\\naccuracy gains from increased depth.\\nSecond, compared to its plain counterpart, the 34-layer\\n3We have experimented with more training iterations (3×) and still ob-\\nserved the degradation problem, suggesting that this problem cannot be\\nfeasibly addressed by simply using more iterations.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 5}, page_content='model\\ntop-1 err.\\ntop-5 err.\\nVGG-16 [41]\\n28.07\\n9.33\\nGoogLeNet [44]\\n-\\n9.15\\nPReLU-net [13]\\n24.27\\n7.38\\nplain-34\\n28.54\\n10.02\\nResNet-34 A\\n25.03\\n7.76\\nResNet-34 B\\n24.52\\n7.46\\nResNet-34 C\\n24.19\\n7.40\\nResNet-50\\n22.85\\n6.71\\nResNet-101\\n21.75\\n6.05\\nResNet-152\\n21.43\\n5.71\\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\\nthat only uses projections for increasing dimensions.\\nmethod\\ntop-1 err.\\ntop-5 err.\\nVGG [41] (ILSVRC’14)\\n-\\n8.43†\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n7.89\\nVGG [41] (v5)\\n24.4\\n7.1\\nPReLU-net [13]\\n21.59\\n5.71\\nBN-inception [16]\\n21.99\\n5.81\\nResNet-34 B\\n21.84\\n5.71\\nResNet-34 C\\n21.53\\n5.60\\nResNet-50\\n20.74\\n5.25\\nResNet-101\\n19.87\\n4.60\\nResNet-152\\n19.38\\n4.49\\nTable 4. Error rates (%) of single-model results on the ImageNet\\nvalidation set (except † reported on the test set).\\nmethod\\ntop-5 err. (test)\\nVGG [41] (ILSVRC’14)\\n7.32\\nGoogLeNet [44] (ILSVRC’14)\\n6.66\\nVGG [41] (v5)\\n6.8\\nPReLU-net [13]\\n4.94\\nBN-inception [16]\\n4.82\\nResNet (ILSVRC’15)\\n3.57\\nTable 5. Error rates (%) of ensembles. The top-5 error is on the\\ntest set of ImageNet and reported by the test server.\\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\\nfrom the successfully reduced training error (Fig. 4 right vs.\\nleft). This comparison veriﬁes the effectiveness of residual\\nlearning on extremely deep systems.\\nLast, we also note that the 18-layer plain/residual nets\\nare comparably accurate (Table 2), but the 18-layer ResNet\\nconverges faster (Fig. 4 right vs. left). When the net is “not\\noverly deep” (18 layers here), the current SGD solver is still\\nable to ﬁnd good solutions to the plain net. In this case, the\\nResNet eases the optimization by providing faster conver-\\ngence at the early stage.\\nIdentity vs. Projection Shortcuts. We have shown that\\n3x3, 64\\n1x1, 64\\nrelu\\n1x1, 256\\nrelu\\nrelu\\n3x3, 64\\n3x3, 64\\nrelu\\nrelu\\n64-d\\n256-d\\nFigure 5. A deeper residual function F for ImageNet. Left: a\\nbuilding block (on 56×56 feature maps) as in Fig. 3 for ResNet-\\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\\nparameter-free, identity shortcuts help with training. Next\\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\\ncompare three options: (A) zero-padding shortcuts are used\\nfor increasing dimensions, and all shortcuts are parameter-\\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\\ntion shortcuts are used for increasing dimensions, and other\\nshortcuts are identity; and (C) all shortcuts are projections.\\nTable 3 shows that all three options are considerably bet-\\nter than the plain counterpart. B is slightly better than A. We\\nargue that this is because the zero-padded dimensions in A\\nindeed have no residual learning. C is marginally better than\\nB, and we attribute this to the extra parameters introduced\\nby many (thirteen) projection shortcuts. But the small dif-\\nferences among A/B/C indicate that projection shortcuts are\\nnot essential for addressing the degradation problem. So we\\ndo not use option C in the rest of this paper, to reduce mem-\\nory/time complexity and model sizes. Identity shortcuts are\\nparticularly important for not increasing the complexity of\\nthe bottleneck architectures that are introduced below.\\nDeeper Bottleneck Architectures. Next we describe our\\ndeeper nets for ImageNet. Because of concerns on the train-\\ning time that we can afford, we modify the building block\\nas a bottleneck design4. For each residual function F, we\\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\\nare 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers\\nare responsible for reducing and then increasing (restoring)\\ndimensions, leaving the 3×3 layer a bottleneck with smaller\\ninput/output dimensions. Fig. 5 shows an example, where\\nboth designs have similar time complexity.\\nThe parameter-free identity shortcuts are particularly im-\\nportant for the bottleneck architectures. If the identity short-\\ncut in Fig. 5 (right) is replaced with projection, one can\\nshow that the time complexity and model size are doubled,\\nas the shortcut is connected to the two high-dimensional\\nends. So identity shortcuts lead to more efﬁcient models\\nfor the bottleneck designs.\\n50-layer ResNet: We replace each 2-layer block in the\\n4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy\\nfrom increased depth (as shown on CIFAR-10), but are not as economical\\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\\nto practical considerations. We further note that the degradation problem\\nof plain nets is also witnessed for the bottleneck designs.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 6}, page_content='34-layer net with this 3-layer bottleneck block, resulting in\\na 50-layer ResNet (Table 1). We use option B for increasing\\ndimensions. This model has 3.8 billion FLOPs.\\n101-layer and 152-layer ResNets: We construct 101-\\nlayer and 152-layer ResNets by using more 3-layer blocks\\n(Table 1). Remarkably, although the depth is signiﬁcantly\\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil-\\nlion FLOPs).\\nThe 50/101/152-layer ResNets are more accurate than\\nthe 34-layer ones by considerable margins (Table 3 and 4).\\nWe do not observe the degradation problem and thus en-\\njoy signiﬁcant accuracy gains from considerably increased\\ndepth. The beneﬁts of depth are witnessed for all evaluation\\nmetrics (Table 3 and 4).\\nComparisons with State-of-the-art Methods. In Table 4\\nwe compare with the previous best single-model results.\\nOur baseline 34-layer ResNets have achieved very compet-\\nitive accuracy. Our 152-layer ResNet has a single-model\\ntop-5 validation error of 4.49%. This single-model result\\noutperforms all previous ensemble results (Table 5). We\\ncombine six models of different depth to form an ensemble\\n(only with two 152-layer ones at the time of submitting).\\nThis leads to 3.57% top-5 error on the test set (Table 5).\\nThis entry won the 1st place in ILSVRC 2015.\\n4.2. CIFAR-10 and Analysis\\nWe conducted more studies on the CIFAR-10 dataset\\n[20], which consists of 50k training images and 10k test-\\ning images in 10 classes. We present experiments trained\\non the training set and evaluated on the test set. Our focus\\nis on the behaviors of extremely deep networks, but not on\\npushing the state-of-the-art results, so we intentionally use\\nsimple architectures as follows.\\nThe plain/residual architectures follow the form in Fig. 3\\n(middle/right). The network inputs are 32×32 images, with\\nthe per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-\\nlutions. Then we use a stack of 6n layers with 3×3 convo-\\nlutions on the feature maps of sizes {32, 16, 8} respectively,\\nwith 2n layers for each feature map size. The numbers of\\nﬁlters are {16, 32, 64} respectively. The subsampling is per-\\nformed by convolutions with a stride of 2. The network ends\\nwith a global average pooling, a 10-way fully-connected\\nlayer, and softmax. There are totally 6n+2 stacked weighted\\nlayers. The following table summarizes the architecture:\\noutput map size\\n32×32\\n16×16\\n8×8\\n# layers\\n1+2n\\n2n\\n2n\\n# ﬁlters\\n16\\n32\\n64\\nWhen shortcut connections are used, they are connected\\nto the pairs of 3×3 layers (totally 3n shortcuts). On this\\ndataset we use identity shortcuts in all cases (i.e., option A),\\nmethod\\nerror (%)\\nMaxout [10]\\n9.38\\nNIN [25]\\n8.81\\nDSN [24]\\n8.22\\n# layers\\n# params\\nFitNet [35]\\n19\\n2.5M\\n8.39\\nHighway [42, 43]\\n19\\n2.3M\\n7.54 (7.72±0.16)\\nHighway [42, 43]\\n32\\n1.25M\\n8.80\\nResNet\\n20\\n0.27M\\n8.75\\nResNet\\n32\\n0.46M\\n7.51\\nResNet\\n44\\n0.66M\\n7.17\\nResNet\\n56\\n0.85M\\n6.97\\nResNet\\n110\\n1.7M\\n6.43 (6.61±0.16)\\nResNet\\n1202\\n19.4M\\n7.93\\nTable 6. Classiﬁcation error on the CIFAR-10 test set. All meth-\\nods are with data augmentation. For ResNet-110, we run it 5 times\\nand show “best (mean±std)” as in [43].\\nso our residual models have exactly the same depth, width,\\nand number of parameters as the plain counterparts.\\nWe use a weight decay of 0.0001 and momentum of 0.9,\\nand adopt the weight initialization in [13] and BN [16] but\\nwith no dropout. These models are trained with a mini-\\nbatch size of 128 on two GPUs. We start with a learning\\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\\nterminate training at 64k iterations, which is determined on\\na 45k/5k train/val split. We follow the simple data augmen-\\ntation in [24] for training: 4 pixels are padded on each side,\\nand a 32×32 crop is randomly sampled from the padded\\nimage or its horizontal ﬂip. For testing, we only evaluate\\nthe single view of the original 32×32 image.\\nWe compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and\\n56-layer networks. Fig. 6 (left) shows the behaviors of the\\nplain nets. The deep plain nets suffer from increased depth,\\nand exhibit higher training error when going deeper. This\\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\\non MNIST (see [42]), suggesting that such an optimization\\ndifﬁculty is a fundamental problem.\\nFig. 6 (middle) shows the behaviors of ResNets. Also\\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\\nmanage to overcome the optimization difﬁculty and demon-\\nstrate accuracy gains when the depth increases.\\nWe further explore n = 18 that leads to a 110-layer\\nResNet. In this case, we ﬁnd that the initial learning rate\\nof 0.1 is slightly too large to start converging5. So we use\\n0.01 to warm up the training until the training error is below\\n80% (about 400 iterations), and then go back to 0.1 and con-\\ntinue training. The rest of the learning schedule is as done\\npreviously. This 110-layer network converges well (Fig. 6,\\nmiddle). It has fewer parameters than other deep and thin\\n5With an initial learning rate of 0.1, it starts converging (<90% error)\\nafter several epochs, but still reaches similar accuracy.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 7}, page_content='0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nplain-20\\nplain-32\\nplain-44\\nplain-56\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nResNet-20\\nResNet-32\\nResNet-44\\nResNet-56\\nResNet-110\\n56-layer\\n20-layer\\n110-layer\\n20-layer\\n4\\n5\\n6\\n0\\n1\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nresidual-110\\nresidual-1202\\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\\nof plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.\\n0\\n20\\n40\\n60\\n80\\n100\\n1\\n2\\n3\\nlayer index (sorted by magnitude)\\nstd\\n \\n \\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\n0\\n20\\n40\\n60\\n80\\n100\\n1\\n2\\n3\\nlayer index (original)\\nstd\\n \\n \\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\\n10. The responses are the outputs of each 3×3 layer, after BN and\\nbefore nonlinearity. Top: the layers are shown in their original\\norder. Bottom: the responses are ranked in descending order.\\nnetworks such as FitNet [35] and Highway [42] (Table 6),\\nyet is among the state-of-the-art results (6.43%, Table 6).\\nAnalysis of Layer Responses. Fig. 7 shows the standard\\ndeviations (std) of the layer responses. The responses are\\nthe outputs of each 3×3 layer, after BN and before other\\nnonlinearity (ReLU/addition).\\nFor ResNets, this analy-\\nsis reveals the response strength of the residual functions.\\nFig. 7 shows that ResNets have generally smaller responses\\nthan their plain counterparts. These results support our ba-\\nsic motivation (Sec.3.1) that the residual functions might\\nbe generally closer to zero than the non-residual functions.\\nWe also notice that the deeper ResNet has smaller magni-\\ntudes of responses, as evidenced by the comparisons among\\nResNet-20, 56, and 110 in Fig. 7. When there are more\\nlayers, an individual layer of ResNets tends to modify the\\nsignal less.\\nExploring Over 1000 layers. We explore an aggressively\\ndeep model of over 1000 layers.\\nWe set n = 200 that\\nleads to a 1202-layer network, which is trained as described\\nabove. Our method shows no optimization difﬁculty, and\\nthis 103-layer network is able to achieve training error\\n<0.1% (Fig. 6, right).\\nIts test error is still fairly good\\n(7.93%, Table 6).\\nBut there are still open problems on such aggressively\\ndeep models. The testing result of this 1202-layer network\\nis worse than that of our 110-layer network, although both\\ntraining data\\n07+12\\n07++12\\ntest data\\nVOC 07 test\\nVOC 12 test\\nVGG-16\\n73.2\\n70.4\\nResNet-101\\n76.4\\n73.8\\nTable 7. Object detection mAP (%) on the PASCAL VOC\\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\\nble 10 and 11 for better results.\\nmetric\\nmAP@.5\\nmAP@[.5, .95]\\nVGG-16\\n41.5\\n21.2\\nResNet-101\\n48.4\\n27.2\\nTable 8. Object detection mAP (%) on the COCO validation set\\nusing baseline Faster R-CNN. See also Table 9 for better results.\\nhave similar training error. We argue that this is because of\\noverﬁtting. The 1202-layer network may be unnecessarily\\nlarge (19.4M) for this small dataset. Strong regularization\\nsuch as maxout [10] or dropout [14] is applied to obtain the\\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\\nwe use no maxout/dropout and just simply impose regular-\\nization via deep and thin architectures by design, without\\ndistracting from the focus on the difﬁculties of optimiza-\\ntion. But combining with stronger regularization may im-\\nprove results, which we will study in the future.\\n4.3. Object Detection on PASCAL and MS COCO\\nOur method has good generalization performance on\\nother recognition tasks. Table 7 and 8 show the object de-\\ntection baseline results on PASCAL VOC 2007 and 2012\\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\\ntection method. Here we are interested in the improvements\\nof replacing VGG-16 [41] with ResNet-101. The detection\\nimplementation (see appendix) of using both models is the\\nsame, so the gains can only be attributed to better networks.\\nMost remarkably, on the challenging COCO dataset we ob-\\ntain a 6.0% increase in COCO’s standard metric (mAP@[.5,\\n.95]), which is a 28% relative improvement. This gain is\\nsolely due to the learned representations.\\nBased on deep residual nets, we won the 1st places in\\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\\nageNet detection, ImageNet localization, COCO detection,\\nand COCO segmentation. The details are in the appendix.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 8}, page_content='References\\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\\ncies with gradient descent is difﬁcult. IEEE Transactions on Neural\\nNetworks, 5(2):157–166, 1994.\\n[2] C. M. Bishop.\\nNeural networks for pattern recognition.\\nOxford\\nuniversity press, 1995.\\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\\n2000.\\n[4] K. Chatﬁeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\\nis in the details: an evaluation of recent feature encoding methods.\\nIn BMVC, 2011.\\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\\npages 303–338, 2010.\\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\\nsemantic segmentation-aware cnn model. In ICCV, 2015.\\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\\narchies for accurate object detection and semantic segmentation. In\\nCVPR, 2014.\\n[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training\\ndeep feedforward neural networks. In AISTATS, 2010.\\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.\\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\\ncost. In CVPR, 2015.\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition. In ECCV, 2014.\\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation. In\\nICCV, 2015.\\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by preventing co-\\nadaptation of feature detectors. arXiv:1207.0580, 2012.\\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\\ncomputation, 9(8):1735–1780, 1997.\\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. In ICML, 2015.\\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\\nneighbor search. TPAMI, 33, 2011.\\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into compact codes.\\nTPAMI, 2012.\\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\\nfast feature embedding. arXiv:1408.5093, 2014.\\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\\nages. Tech Report, 2009.\\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. In NIPS, 2012.\\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\\nwritten zip code recognition. Neural computation, 1989.\\n[23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop.\\nIn Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.\\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.\\nDeeply-\\nsupervised nets. arXiv:1409.5185, 2014.\\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400,\\n2013.\\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll´ar, and C. L. Zitnick. Microsoft COCO: Common objects in\\ncontext. In ECCV. 2014.\\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\\nfor semantic segmentation. In CVPR, 2015.\\n[28] G. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\\nlinear regions of deep neural networks. In NIPS, 2014.\\n[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\\nboltzmann machines. In ICML, 2010.\\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\\nimage categorization. In CVPR, 2007.\\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\\nlinear transformations in perceptrons. In AISTATS, 2012.\\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\\nreal-time object detection with region proposal networks. In NIPS,\\n2015.\\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\\nnetworks on convolutional feature maps. arXiv:1504.06066, 2015.\\n[34] B. D. Ripley. Pattern recognition and neural networks. Cambridge\\nuniversity press, 1996.\\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.\\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.\\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\\nthe nonlinear dynamics of learning in deep linear neural networks.\\narXiv:1312.6120, 2013.\\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\\ndecomposition. Technical report, 1998.\\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\\nNeural Networks: Tricks of the Trade, pages 207–226. Springer,\\n1998.\\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\\nCun.\\nOverfeat: Integrated recognition, localization and detection\\nusing convolutional networks. In ICLR, 2014.\\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. In ICLR, 2015.\\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\\narXiv:1505.00387, 2015.\\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\\nnetworks. 1507.06228, 2015.\\n[44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\\nhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-\\ntions. In CVPR, 2015.\\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\\ntions. TPAMI, 1990.\\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\\nSIGGRAPH, 2006.\\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-\\ntic gradient towards second-order methods–backpropagation learn-\\ning with transformations in nonlinearities.\\nIn Neural Information\\nProcessing, 2013.\\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\\nof computer vision algorithms, 2008.\\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\\n1999.\\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\\ntional neural networks. In ECCV, 2014.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 9}, page_content='A. Object Detection Baselines\\nIn this section we introduce our detection method based\\non the baseline Faster R-CNN [32] system. The models are\\ninitialized by the ImageNet classiﬁcation models, and then\\nﬁne-tuned on the object detection data. We have experi-\\nmented with ResNet-50/101 at the time of the ILSVRC &\\nCOCO 2015 detection competitions.\\nUnlike VGG-16 used in [32], our ResNet has no hidden\\nfc layers. We adopt the idea of “Networks on Conv fea-\\nture maps” (NoC) [33] to address this issue. We compute\\nthe full-image shared conv feature maps using those lay-\\ners whose strides on the image are no greater than 16 pixels\\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\\nlayers in ResNet-101; Table 1). We consider these layers as\\nanalogous to the 13 conv layers in VGG-16, and by doing\\nso, both ResNet and VGG-16 have conv feature maps of the\\nsame total stride (16 pixels). These layers are shared by a\\nregion proposal network (RPN, generating 300 proposals)\\n[32] and a Fast R-CNN detection network [7]. RoI pool-\\ning [7] is performed before conv5 1. On this RoI-pooled\\nfeature, all layers of conv5 x and up are adopted for each\\nregion, playing the roles of VGG-16’s fc layers. The ﬁnal\\nclassiﬁcation layer is replaced by two sibling layers (classi-\\nﬁcation and box regression [7]).\\nFor the usage of BN layers, after pre-training, we com-\\npute the BN statistics (means and variances) for each layer\\non the ImageNet training set. Then the BN layers are ﬁxed\\nduring ﬁne-tuning for object detection. As such, the BN\\nlayers become linear activations with constant offsets and\\nscales, and BN statistics are not updated by ﬁne-tuning. We\\nﬁx the BN layers mainly for reducing memory consumption\\nin Faster R-CNN training.\\nPASCAL VOC\\nFollowing [7, 32], for the PASCAL VOC 2007 test set,\\nwe use the 5k trainval images in VOC 2007 and 16k train-\\nval images in VOC 2012 for training (“07+12”). For the\\nPASCAL VOC 2012 test set, we use the 10k trainval+test\\nimages in VOC 2007 and 16k trainval images in VOC 2012\\nfor training (“07++12”). The hyper-parameters for train-\\ning Faster R-CNN are the same as in [32]. Table 7 shows\\nthe results. ResNet-101 improves the mAP by >3% over\\nVGG-16. This gain is solely because of the improved fea-\\ntures learned by ResNet.\\nMS COCO\\nThe MS COCO dataset [26] involves 80 object cate-\\ngories. We evaluate the PASCAL VOC metric (mAP @\\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\\n.5:.05:.95). We use the 80k images on the train set for train-\\ning and the 40k images on the val set for evaluation. Our\\ndetection system for COCO is similar to that for PASCAL\\nVOC. We train the COCO models with an 8-GPU imple-\\nmentation, and thus the RPN step has a mini-batch size of\\n8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\\nmini-batch size of 16 images. The RPN step and Fast R-\\nCNN step are both trained for 240k iterations with a learn-\\ning rate of 0.001 and then for 80k iterations with 0.0001.\\nTable 8 shows the results on the MS COCO validation\\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\\nVGG-16, which is a 28% relative improvement, solely con-\\ntributed by the features learned by the better network. Re-\\nmarkably, the mAP@[.5, .95]’s absolute increase (6.0%) is\\nnearly as big as mAP@.5’s (6.9%). This suggests that a\\ndeeper network can improve both recognition and localiza-\\ntion.\\nB. Object Detection Improvements\\nFor completeness, we report the improvements made for\\nthe competitions. These improvements are based on deep\\nfeatures and thus should beneﬁt from residual learning.\\nMS COCO\\nBox reﬁnement. Our box reﬁnement partially follows the it-\\nerative localization in [6]. In Faster R-CNN, the ﬁnal output\\nis a regressed box that is different from its proposal box. So\\nfor inference, we pool a new feature from the regressed box\\nand obtain a new classiﬁcation score and a new regressed\\nbox. We combine these 300 new predictions with the orig-\\ninal 300 predictions. Non-maximum suppression (NMS) is\\napplied on the union set of predicted boxes using an IoU\\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\\nﬁnement improves mAP by about 2 points (Table 9).\\nGlobal context.\\nWe combine global context in the Fast\\nR-CNN step. Given the full-image conv feature map, we\\npool a feature by global Spatial Pyramid Pooling [12] (with\\na “single-level” pyramid) which can be implemented as\\n“RoI” pooling using the entire image’s bounding box as the\\nRoI. This pooled feature is fed into the post-RoI layers to\\nobtain a global context feature. This global feature is con-\\ncatenated with the original per-region feature, followed by\\nthe sibling classiﬁcation and box regression layers. This\\nnew structure is trained end-to-end.\\nGlobal context im-\\nproves mAP@.5 by about 1 point (Table 9).\\nMulti-scale testing. In the above, all results are obtained by\\nsingle-scale training/testing as in [32], where the image’s\\nshorter side is s = 600 pixels. Multi-scale training/testing\\nhas been developed in [12, 7] by selecting a scale from a\\nfeature pyramid, and in [33] by using maxout layers. In\\nour current implementation, we have performed multi-scale\\ntesting following [33]; we have not performed multi-scale\\ntraining because of limited time. In addition, we have per-\\nformed multi-scale testing only for the Fast R-CNN step\\n(but not yet for the RPN step). With a trained model, we\\ncompute conv feature maps on an image pyramid, where the\\nimage’s shorter sides are s ∈{200, 400, 600, 800, 1000}.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 10}, page_content='training data\\nCOCO train\\nCOCO trainval\\ntest data\\nCOCO val\\nCOCO test-dev\\nmAP\\n@.5\\n@[.5, .95]\\n@.5\\n@[.5, .95]\\nbaseline Faster R-CNN (VGG-16)\\n41.5\\n21.2\\nbaseline Faster R-CNN (ResNet-101)\\n48.4\\n27.2\\n+box reﬁnement\\n49.9\\n29.9\\n+context\\n51.1\\n30.0\\n53.3\\n32.2\\n+multi-scale testing\\n53.8\\n32.5\\n55.7\\n34.9\\nensemble\\n59.0\\n37.4\\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\\nsystem\\nnet\\ndata\\nmAP\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike person\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nbaseline\\nVGG-16\\n07+12\\n73.2\\n76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\\nbaseline\\nResNet-101\\n07+12\\n76.4\\n79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\\nbaseline+++ ResNet-101\\nCOCO+07+12\\n85.6\\n90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”\\ninclude box reﬁnement, context, and multi-scale testing in Table 9.\\nsystem\\nnet\\ndata\\nmAP\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike person\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nbaseline\\nVGG-16\\n07++12\\n70.4\\n84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nbaseline\\nResNet-101\\n07++12\\n73.8\\n86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\\nbaseline+++ ResNet-101 COCO+07++12\\n83.8\\n92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\\nTable 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/\\ndisplaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system “baseline+++” include\\nbox reﬁnement, context, and multi-scale testing in Table 9.\\nWe select two adjacent scales from the pyramid following\\n[33]. RoI pooling and subsequent layers are performed on\\nthe feature maps of these two scales [33], which are merged\\nby maxout as in [33]. Multi-scale testing improves the mAP\\nby over 2 points (Table 9).\\nUsing validation data. Next we use the 80k+40k trainval set\\nfor training and the 20k test-dev set for evaluation. The test-\\ndev set has no publicly available ground truth and the result\\nis reported by the evaluation server. Under this setting, the\\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\\n34.9% (Table 9). This is our single-model result.\\nEnsemble. In Faster R-CNN, the system is designed to learn\\nregion proposals and also object classiﬁers, so an ensemble\\ncan be used to boost both tasks. We use an ensemble for\\nproposing regions, and the union set of proposals are pro-\\ncessed by an ensemble of per-region classiﬁers. Table 9\\nshows our result based on an ensemble of 3 networks. The\\nmAP is 59.0% and 37.4% on the test-dev set. This result\\nwon the 1st place in the detection task in COCO 2015.\\nPASCAL VOC\\nWe revisit the PASCAL VOC dataset based on the above\\nmodel. With the single model on the COCO dataset (55.7%\\nmAP@.5 in Table 9), we ﬁne-tune this model on the PAS-\\nCAL VOC sets. The improvements of box reﬁnement, con-\\ntext, and multi-scale testing are also adopted. By doing so\\nval2\\ntest\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n43.9\\nour single model (ILSVRC’15)\\n60.5\\n58.8\\nour ensemble (ILSVRC’15)\\n63.6\\n62.1\\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\\nOur detection system is Faster R-CNN [32] with the improvements\\nin Table 9, using ResNet-101.\\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\\non PASCAL VOC 2012 is 10 points higher than the previ-\\nous state-of-the-art result [6].\\nImageNet Detection\\nThe ImageNet Detection (DET) task involves 200 object\\ncategories. The accuracy is evaluated by mAP@.5. Our\\nobject detection algorithm for ImageNet DET is the same\\nas that for MS COCO in Table 9. The networks are pre-\\ntrained on the 1000-class ImageNet classiﬁcation set, and\\nare ﬁne-tuned on the DET data. We split the validation set\\ninto two parts (val1/val2) following [8]. We ﬁne-tune the\\ndetection models using the DET training set and the val1\\nset. The val2 set is used for validation. We do not use other\\nILSVRC 2015 data. Our single model with ResNet-101 has\\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,\\nsubmitted on 2015-11-26.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-11T01:13:45+00:00', 'source': '../../docs/resnet_paper.pdf', 'file_path': '../../docs/resnet_paper.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-11T01:13:45+00:00', 'trapped': '', 'modDate': 'D:20151211011345Z', 'creationDate': 'D:20151211011345Z', 'page': 11}, page_content='LOC\\nmethod\\nLOC\\nnetwork\\ntesting LOC error\\non GT CLS\\nclassiﬁcation\\nnetwork\\ntop-5 LOC error\\non predicted CLS\\nVGG’s [41]\\nVGG-16\\n1-crop\\n33.1 [41]\\nRPN\\nResNet-101 1-crop\\n13.3\\nRPN\\nResNet-101 dense\\n11.7\\nRPN\\nResNet-101 dense\\nResNet-101\\n14.4\\nRPN+RCNN ResNet-101 dense\\nResNet-101\\n10.6\\nRPN+RCNN\\nensemble\\ndense\\nensemble\\n8.9\\nTable 13. Localization error (%) on the ImageNet validation. In\\nthe column of “LOC error on GT class” ([41]), the ground truth\\nclass is used. In the “testing” column, “1-crop” denotes testing\\non a center crop of 224×224 pixels, “dense” denotes dense (fully\\nconvolutional) and multi-scale testing.\\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\\non the DET test set (Table 12). This result won the 1st place\\nin the ImageNet detection task in ILSVRC 2015, surpassing\\nthe second place by 8.5 points (absolute).\\nC. ImageNet Localization\\nThe ImageNet Localization (LOC) task [36] requires to\\nclassify and localize the objects. Following [40, 41], we\\nassume that the image-level classiﬁers are ﬁrst adopted for\\npredicting the class labels of an image, and the localiza-\\ntion algorithm only accounts for predicting bounding boxes\\nbased on the predicted classes. We adopt the “per-class re-\\ngression” (PCR) strategy [40, 41], learning a bounding box\\nregressor for each class. We pre-train the networks for Im-\\nageNet classiﬁcation and then ﬁne-tune them for localiza-\\ntion. We train networks on the provided 1000-class Ima-\\ngeNet training set.\\nOur localization algorithm is based on the RPN frame-\\nwork of [32] with a few modiﬁcations. Unlike the way in\\n[32] that is category-agnostic, our RPN for localization is\\ndesigned in a per-class form. This RPN ends with two sib-\\nling 1×1 convolutional layers for binary classiﬁcation (cls)\\nand box regression (reg), as in [32]. The cls and reg layers\\nare both in a per-class from, in contrast to [32]. Speciﬁ-\\ncally, the cls layer has a 1000-d output, and each dimension\\nis binary logistic regression for predicting being or not be-\\ning an object class; the reg layer has a 1000×4-d output\\nconsisting of box regressors for 1000 classes. As in [32],\\nour bounding box regression is with reference to multiple\\ntranslation-invariant “anchor” boxes at each position.\\nAs in our ImageNet classiﬁcation training (Sec. 3.4), we\\nrandomly sample 224×224 crops for data augmentation.\\nWe use a mini-batch size of 256 images for ﬁne-tuning. To\\navoid negative samples being dominate, 8 anchors are ran-\\ndomly sampled for each image, where the sampled positive\\nand negative anchors have a ratio of 1:1 [32]. For testing,\\nthe network is applied on the image fully-convolutionally.\\nTable 13 compares the localization results. Following\\n[41], we ﬁrst perform “oracle” testing using the ground truth\\nclass as the classiﬁcation prediction. VGG’s paper [41] re-\\nmethod\\ntop-5 localization err\\nval\\ntest\\nOverFeat [40] (ILSVRC’13)\\n30.0\\n29.9\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n26.7\\nVGG [41] (ILSVRC’14)\\n26.9\\n25.3\\nours (ILSVRC’15)\\n8.9\\n9.0\\nTable 14. Comparisons of localization error (%) on the ImageNet\\ndataset with state-of-the-art methods.\\nports a center-crop error of 33.1% (Table 13) using ground\\ntruth classes. Under the same setting, our RPN method us-\\ning ResNet-101 net signiﬁcantly reduces the center-crop er-\\nror to 13.3%. This comparison demonstrates the excellent\\nperformance of our framework. With dense (fully convolu-\\ntional) and multi-scale testing, our ResNet-101 has an error\\nof 11.7% using ground truth classes. Using ResNet-101 for\\npredicting classes (4.6% top-5 classiﬁcation error, Table 4),\\nthe top-5 localization error is 14.4%.\\nThe above results are only based on the proposal network\\n(RPN) in Faster R-CNN [32]. One may use the detection\\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\\nresults. But we notice that on this dataset, one image usually\\ncontains a single dominate object, and the proposal regions\\nhighly overlap with each other and thus have very similar\\nRoI-pooled features. As a result, the image-centric training\\nof Fast R-CNN [7] generates samples of small variations,\\nwhich may not be desired for stochastic training. Motivated\\nby this, in our current experiment we use the original R-\\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\\nOur R-CNN implementation is as follows. We apply the\\nper-class RPN trained as above on the training images to\\npredict bounding boxes for the ground truth class. These\\npredicted boxes play a role of class-dependent proposals.\\nFor each training image, the highest scored 200 proposals\\nare extracted as training samples to train an R-CNN classi-\\nﬁer. The image region is cropped from a proposal, warped\\nto 224×224 pixels, and fed into the classiﬁcation network\\nas in R-CNN [8]. The outputs of this network consist of two\\nsibling fc layers for cls and reg, also in a per-class form.\\nThis R-CNN network is ﬁne-tuned on the training set us-\\ning a mini-batch size of 256 in the RoI-centric fashion. For\\ntesting, the RPN generates the highest scored 200 proposals\\nfor each predicted class, and the R-CNN network is used to\\nupdate these proposals’ scores and box positions.\\nThis method reduces the top-5 localization error to\\n10.6% (Table 13). This is our single-model result on the\\nvalidation set. Using an ensemble of networks for both clas-\\nsiﬁcation and localization, we achieve a top-5 localization\\nerror of 9.0% on the test set. This number signiﬁcantly out-\\nperforms the ILSVRC 14 results (Table 14), showing a 64%\\nrelative reduction of error. This result won the 1st place in\\nthe ImageNet localization task in ILSVRC 2015.\\n12'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 0}, page_content='How and Why to Use LLMs for\\nChunk-Based Information Retrieval\\nCarlo Peron\\nPublished in Towards Data Science · 9 min read · 1 day ago\\n24\\nRetrieve pipeline — Image by the author\\nIn this article, I aim to explain how and why it’s beneficial to use a Large\\nLanguage Model (LLM) for chunk-based information retrieval.\\nI use OpenAI’s GPT-4 model as an example, but this approach can be applied\\nwith any other LLM, such as those from Hugging Face, Claude, and others.\\nEveryone can access this article for free.\\nConsiderations on standard information retrieval\\nThe primary concept involves having a list of documents (chunks of text)\\nstored in a database, which could be retrieve based on some filter and\\nconditions.\\nSearch\\nWrite\\n60'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 1}, page_content='Typically, a tool is used to enable hybrid search (such as Azure AI Search,\\nLlamaIndex, etc.), which allows:\\nperforming a text-based search using term frequency algorithms like TF-\\nIDF (e.g., BM25);\\nconducting a vector-based search, which identifies similar concepts even\\nwhen different terms are used, by calculating vector distances (typically\\ncosine similarity);\\ncombining elements from steps 1 and 2, weighting them to highlight the\\nmost relevant results.\\nFigure 1- Default hybrid search pipeline — Image by the author\\nFigure 1 shows the classic retrieval pipeline:\\nthe user asks the system a question: “I would like to talk about Paris”;\\nthe system receives the question, converts it into an embedding vector\\n(using the same model applied in the ingestion phase), and finds the\\nchunks with the smallest distances;\\nthe system also performs a text-based search based on frequency;'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 2}, page_content='the chunks returned from both processes undergo further evaluation and\\nare reordered based on a ranking formula.\\nThis solution achieves good results but has some limitations:\\nnot all relevant chunks are always retrieved;\\nsometime some chunks contain anomalies that affect the final response.\\nAn example of a typical retrieval issue\\nLet’s consider the “documents” array, which represents an example of a\\nknowledge base that could lead to incorrect chunk selection.\\ndocuments = [\\n\"Chunk 1: This document contains information about topic A.\",\\n\"Chunk 2: Insights related to topic B can be found here.\",\\n\"Chunk 3: This chunk discusses topic C in detail.\",\\n\"Chunk 4: Further insights on topic D are covered here.\",\\n\"Chunk 5: Another chunk with more data on topic E.\",\\n\"Chunk 6: Extensive research on topic F is presented.\",\\n\"Chunk 7: Information on topic G is explained here.\",\\n\"Chunk 8: This document expands on topic H. It also talk about topic B\",\\n\"Chunk 9: Nothing about topic B are given.\",\\n\"Chunk 10: Finally, a discussion of topic J. This document doesn\\'t contain i\\n]\\nLet’s assume we have a RAG system, consisting of a vector database with\\nhybrid search capabilities and an LLM-based prompt, to which the user\\nposes the following question: “I need to know something about topic B.”\\nAs shown in Figure 2, the search also returns an incorrect chunk that, while\\nsemantically relevant, is not suitable for answering the question and, in\\nsome cases, could even confuse the LLM tasked with providing a response.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 3}, page_content='Figure 2 — Example of information retrieval that can lead to errors — Image by the author\\nIn this example, the user requests information about “topic B,” and the\\nsearch returns chunks that include “This document expands on topic H. It also\\ntalks about topic B” and “Insights related to topic B can be found here.” as well as\\nthe chunk stating, “Nothing about topic B are given”.\\nWhile this is the expected behavior of hybrid search (as chunks reference\\n“topic B”), it is not the desired outcome, as the third chunk is returned\\nwithout recognizing that it isn’t helpful for answering the question.\\nThe retrieval didn’t produce the intended result, not only because the BM25\\nsearch found the term “topic B” in the third Chunk but also because the\\nvector search yielded a high cosine similarity.\\nTo understand this, refer to Figure 3, which shows the cosine similarity\\nvalues of the chunks relative to the question, using OpenAI’s text-\\nembedding-ada-002 model for embeddings.\\nFigure 3 — Cosine similarity with text-embedding-ada-002- Image by the author\\nIt is evident that the cosine similarity value for “Chunk 9” is among the\\nhighest, and that between this chunk and chunk 10, which references “topic\\nB,” there is also chunk 1, which does not mention “topic B”.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 4}, page_content='This situation remains unchanged even when measuring distance using a\\ndifferent method, as seen in the case of Minkowski distance.\\nUtilizing LLMs for Information Retrieval: An Example\\nThe solution I will describe is inspired by what has been published in my\\nGitHub repository https://github.com/peronc/LLMRetriever/.\\nThe idea is to have the LLM analyze which chunks are useful for answering\\nthe user’s question, not by ranking the returned chunks (as in the case of\\nRankGPT) but by directly evaluating all the available chunks.\\nFigure 4- LLM Retrieve pipeline — Image by the author\\nIn summary, as shown in Figure 4, the system receives a list of documents to\\nanalyze, which can come from any data source, such as file storage,\\nrelational databases, or vector databases.\\nThe chunks are divided into groups and processed in parallel by a number of\\nthreads proportional to the total amount of chunks.\\nThe logic for each thread includes a loop that iterates through the input\\nchunks, calling an OpenAI prompt for each one to check its relevance to the\\nuser’s question.\\nThe prompt returns the chunk along with a boolean value: true if it is\\nrelevant and false if it is not.\\nLets’go coding'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 5}, page_content='To explain the code, I will simplify by using the chunks present in the\\ndocuments array (I will reference a real case in the conclusions).\\nFirst of all, I import the necessary standard libraries, including os,\\nlangchain, and dotenv.\\nimport os\\nfrom langchain_openai.chat_models.azure import AzureChatOpenAI\\nfrom dotenv import load_dotenv\\nNext, I import my LLMRetrieverLib/llm_retrieve.py class, which provides\\nseveral static methods essential for performing the analysis.\\nfrom LLMRetrieverLib.retriever import llm_retriever\\nFollowing that, I need to import the necessary variables required for\\nutilizing Azure OpenAI GPT-4o model.\\nload_dotenv()\\nazure_deployment = os.getenv(\"AZURE_DEPLOYMENT\")\\ntemperature = float(os.getenv(\"TEMPERATURE\"))\\napi_key\\n= os.getenv(\"AZURE_OPENAI_API_KEY\")\\nendpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\\napi_version = os.getenv(\"API_VERSION\")\\nNext, I proceed with the initialization of the LLM.\\n# Initialize the LLM\\nllm = AzureChatOpenAI(api_key=api_key, azure_endpoint=endpoint, azure_deployment\\nWe are ready to begin: the user asks a question to gather additional\\ninformation about Topic B.\\nquestion = \"I need to know something about topic B\"\\nAt this point, the search for relevant chunks begins, and to do this, I use the\\nfunction llm_retrieve.process_chunks_in_parallel from the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 6}, page_content='LLMRetrieverLib/retriever.py library, which is also found in the same\\nrepository.\\nrelevant_chunks = LLMRetrieverLib.retriever.llm_retriever.process_chunks_in_para\\nTo optimize performance, the function\\nllm_retrieve.process_chunks_in_parallel employs multi-threading to\\ndistribute chunk analysis across multiple threads.\\nThe main idea is to assign each thread a subset of chunks extracted from the\\ndatabase and have each thread analyze the relevance of those chunks based\\non the user’s question.\\nAt the end of the processing, the returned chunks are exactly as expected:\\n[\\'Chunk 2: Insights related to topic B can be found here.\\',\\n\\'Chunk 8: This document expands on topic H. It also talk about topic B\\']\\nFinally, I ask the LLM to provide an answer to the user’s question:\\nfinal_answer = LLMRetrieverLib.retriever.llm_retriever.generate_final_answer_wit\\nprint(\"Final answer:\")\\nprint(final_answer)\\nBelow is the LLM’s response, which is trivial since the content of the chunks,\\nwhile relevant, is not exhaustive on the topic of Topic B:\\nTopic B is covered in both Chunk 2 and Chunk 8.\\nChunk 2 provides insights specifically related to topic B, offering detailed inf\\nChunk 8 expands on topic H but also includes discussions on topic B, potentially\\nScoring Scenario\\nNow let’s try asking the same question but using an approach based on\\nscoring.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 7}, page_content=\"I ask the LLM to assign a score from 1 to 10 to evaluate the relevance\\nbetween each chunk and the question, considering only those with a\\nrelevance higher than 5.\\nTo do this, I call the function llm_retriever.process_chunks_in_parallel ,\\npassing three additional parameters that indicate, respectively, that scoring\\nwill be applied, that the threshold for being considered valid must be greater\\nthan or equal to 5, and that I want a printout of the chunks with their\\nrespective scores.\\nrelevant_chunks = llm_retriever.process_chunks_in_parallel(llm, question, docume\\nThe retrieval phase with scoring produces the following result:\\nscore: 1 - Chunk 1: This document contains information about topic A.\\nscore: 1 - Chunk 7: Information on topic G is explained here.\\nscore: 1 - Chunk 4: Further insights on topic D are covered here.\\nscore: 9 - Chunk 2: Insights related to topic B can be found here.\\nscore: 7 - Chunk 8: This document expands on topic H. It also talk about topic B\\nscore: 1 - Chunk 5: Another chunk with more data on topic E.\\nscore: 1 - Chunk 9: Nothing about topic B are given.\\nscore: 1 - Chunk 3: This chunk discusses topic C in detail.\\nscore: 1 - Chunk 6: Extensive research on topic F is presented.\\nscore: 1 - Chunk 10: Finally, a discussion of topic J. This document doesn't con\\nIt’s the same as before, but with an interesting score\\n.\\nFinally, I once again ask the LLM to provide an answer to the user’s question,\\nand the result is similar to the previous one:\\nChunk 2 provides insights related to topic B, offering foundational information\\nChunk 8 expands on topic B further, possibly providing additional context or det\\nTogether, these chunks should give you a well-rounded understanding of topic B.\\nConsiderations\\nThis retrieval approach has emerged as a necessity following some previous\\nexperiences.\\nI have noticed that pure vector-based searches produce useful results but are\\noften insufficient when the embedding is performed in a language other\\nthan English.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 8}, page_content=\"Using OpenAI with sentences in Italian makes it clear that the tokenization\\nof terms is often incorrect; for example, the term “canzone,” which means\\n“song” in Italian, gets tokenized into two distinct words: “can” and “zone”.\\nThis leads to the construction of an embedding array that is far from what\\nwas intended.\\nIn cases like this, hybrid search, which also incorporates term frequency\\ncounting, leads to improved results, but they are not always as expected.\\nSo, this retrieval methodology can be utilized in the following ways:\\nas the primary search method: where the database is queried for all\\nchunks or a subset based on a filter (e.g., a metadata filter);\\nas a refinement in the case of hybrid search: (this is the same approach\\nused by RankGPT) in this way, the hybrid search can extract a large\\nnumber of chunks, and the system can filter them so that only the\\nrelevant ones reach the LLM while also adhering to the input token limit;\\nas a fallback: in situations where a hybrid search does not yield the\\ndesired results, all chunks can be analyzed.\\nLet’s discuss costs and performance\\nOf course, all that glitters is not gold, as one must consider response times\\nand costs.\\nIn a real use case, I retrieved the chunks from a relational database\\nconsisting of 95 text segments semantically split using my\\nLLMChunkizerLib/chunkizer.py  library from two Microsoft Word documents,\\ntotaling 33 pages.\\nThe analysis of the relevance of the 95 chunks to the question was conducted\\nby calling OpenAI's APIs from a local PC with non-guaranteed bandwidth,\\naveraging around 10Mb, resulting in response times that varied from 7 to 20\\nseconds.\\nNaturally, on a cloud system or by using local LLMs on GPUs, these times can\\nbe significantly reduced.\\nI believe that considerations regarding response times are highly subjective:\\nin some cases, it is acceptable to take longer to provide a correct answer,\\nwhile in others, it is essential not to keep users waiting too long.\\nSimilarly, considerations about costs are also quite subjective, as one must\\ntake a broader perspective to evaluate whether it is more important to\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'PDF24 Creator', 'creationdate': '2024-10-30T09:17:28+01:00', 'source': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'moddate': '2024-10-30T09:17:28+01:00', 'trapped': '', 'modDate': \"D:20241030091728+01'00'\", 'creationDate': \"D:20241030091728+01'00'\", 'page': 9}, page_content='provide as accurate answers as possible or if some errors are acceptable.\\nIn certain fields, the damage to one’s reputation caused by incorrect or\\nmissing answers can outweigh the expense of tokens.\\nFurthermore, even though the costs of OpenAI and other providers have\\nbeen steadily decreasing in recent years, those who already have a GPU-\\nbased infrastructure, perhaps due to the need to handle sensitive or\\nconfidential data, will likely prefer to use a local LLM.\\nConclusions\\nIn conclusion, I hope to have provided my perspective on how retrieval can\\nbe approached.\\nIf nothing else, I aim to be helpful and perhaps inspire others to explore new\\nmethods in their own work.\\nRemember, the world of information retrieval is vast, and with a little\\ncreativity and the right tools, we can uncover knowledge in ways we never\\nimagined!\\nIf you’d like to discuss this further, feel free to connect with me on LinkedIn\\nGitHub repositories can be found here:\\n• https://github.com/peronc/LLMRetriever/\\n• https://github.com/peronc/LLMChunkizer/\\nWritten by Carlo Peron\\n58 Followers · Writer for Towards Data Science\\nYou can get more information about me on https://www.linkedin.com/in/carlo-peron\\nEdit profile\\nChunking\\nRetrieval\\nArtificial Intelligence\\nLlm\\nMachine Learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'source': '../../docs/attention_paper.pdf', 'file_path': '../../docs/attention_paper.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'trapped': '', 'modDate': 'D:20230803000729Z', 'creationDate': 'D:20230803000729Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 0}, page_content='Published as a conference paper at ICLR 2021\\nAN IMAGE IS WORTH 16X16 WORDS:\\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\\n∗equal technical contribution, †equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\nABSTRACT\\nWhile the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1\\nINTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiﬁcation in supervised fashion.\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 1}, page_content='Published as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufﬁcient amounts of data.\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefﬁciently on hardware accelerators.\\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019; Li et al., 2019).\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 2}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 3}, page_content='Published as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\nz0 = [xclass; x1\\npE; x2\\npE; · · · ; xN\\np E] + Epos,\\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\\n(1)\\nz′\\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\\nℓ= 1 . . . L\\n(2)\\nzℓ= MLP(LN(z′\\nℓ)) + z′\\nℓ,\\nℓ= 1 . . . L\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\nHybrid Architecture.\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classiﬁcation input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n4.1\\nSETUP\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 4}, page_content='Published as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\\nimagery, and Structured – tasks that require geometric understanding like localization.\\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\nMetrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\\nwhere ﬁne-tuning would be too costly.\\n4.2\\nCOMPARISON TO STATE OF THE ART\\nWe ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 5}, page_content='Published as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfﬁcientNet-L2)\\nImageNet\\n88.55 ± 0.04\\n87.76 ± 0.03\\n85.30 ± 0.02\\n87.54 ± 0.02\\n88.4/88.5∗\\nImageNet ReaL\\n90.72 ± 0.05\\n90.54 ± 0.03\\n88.62 ± 0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50 ± 0.06\\n99.42 ± 0.03\\n99.15 ± 0.03\\n99.37 ± 0.06\\n−\\nCIFAR-100\\n94.55 ± 0.04\\n93.90 ± 0.05\\n93.25 ± 0.05\\n93.51 ± 0.08\\n−\\nOxford-IIIT Pets\\n97.56 ± 0.03\\n97.32 ± 0.11\\n94.67 ± 0.15\\n96.62 ± 0.23\\n−\\nOxford Flowers-102\\n99.68 ± 0.02\\n99.74 ± 0.00\\n99.61 ± 0.02\\n99.63 ± 0.03\\n−\\nVTAB (19 tasks)\\n77.63 ± 0.23\\n76.28 ± 0.46\\n72.72 ± 0.21\\n76.29 ± 1.70\\n−\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\\nin Touvron et al. (2020).\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy [%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\\nexperiments.\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\\nresolution increase during ﬁne-tuning improves the performance.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 6}, page_content='Published as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy [%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1 [%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size. ResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training. ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n102\\n103\\n90\\n95\\nTransfer accuracy [%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufﬁcient, even beneﬁcial.\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\\nis an exciting direction of future work.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 7}, page_content='Published as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Speciﬁcally, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model. Other attention heads\\nhave consistently small attention distances in the low layers. This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\\nregions that are semantically relevant for classiﬁcation (Figure 6).\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks. However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 8}, page_content='Published as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work.\\n5\\nCONCLUSION\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\nREFERENCES\\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views. In NeurIPS, 2019.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 9}, page_content='Published as a conference paper at ICLR 2021\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\\nICLR, 2019.\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\\nIn ICCV, 2019.\\nLucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\\nwe done with imagenet? arXiv, 2020.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv, 2020.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\\npixels. In ICML, 2020a.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\\nfor contrastive learning of visual representations. In ICML, 2020b.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. arXiv, 2019.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In ICLR, 2020.\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL, 2019.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. In CVPR, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\nMomentum contrast for\\nunsupervised visual representation learning. In CVPR, 2020.\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\\nmensional transformers. arXiv, 2019.\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\\nIn ICCV, 2019.\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\\nICML, 2020.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 10}, page_content='Published as a conference paper at ICLR 2021\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\\ntion. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten.\\nExploring the limits of weakly supervised\\npretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\\nICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\\n2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\\nJournal on Control and Optimization, 30(4):838–855, 1992.\\ndoi: 10.1137/0330046.\\nURL\\nhttps://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\\npreprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\\nStand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\\nfectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\\nmodel for video and language representation learning. In ICCV, 2019.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 11}, page_content='Published as a conference paper at ICLR 2021\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy. In NeurIPS. 2019.\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\\n2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\\narXiv preprint\\narXiv:2003.07853, 2020b.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. In ACL, 2019.\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\\nCVPR, 2018.\\nDirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\\nICLR, 2019.\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\\nfor computer vision. arxiv, 2020.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\\nimproves imagenet classiﬁcation. In CVPR, 2020.\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\\nSupervised Learning. In ICCV, 2019a.\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\\npreprint arXiv:1910.04867, 2019b.\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\\nCVPR, 2020.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 12}, page_content='Published as a conference paper at ICLR 2021\\nModels\\nDataset\\nEpochs\\nBase LR\\nLR decay\\nWeight decay\\nDropout\\nViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/32\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-H/14\\nJFT-300M\\n14\\n3 · 10−4\\nlinear\\n0.1\\n0.0\\nR50x{1,2}\\nJFT-300M\\n7\\n10−3\\nlinear\\n0.1\\n0.0\\nR101x1\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR152x{1,2}\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/32\\nJFT-300M\\n7\\n2 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-B/{16,32}\\nImageNet-21k\\n90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-L/{16,32}\\nImageNet-21k\\n30/90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-∗\\nImageNet\\n300\\n3 · 10−3\\ncosine\\n0.3\\n0.1\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\\nclipping at global norm 1. Training resolution is 224.\\nAPPENDIX\\nA\\nMULTIHEAD SELF-ATTENTION\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\\ntwo elements of the sequence and their respective query qi and key kj representations.\\n[q, k, v] = zUqkv\\nUqkv ∈RD×3Dh,\\n(5)\\nA = softmax\\n\\x10\\nqk⊤/\\np\\nDh\\n\\x11\\nA ∈RN×N,\\n(6)\\nSA(z) = Av .\\n(7)\\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\\nMSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\\nUmsa ∈Rk·Dh×D\\n(8)\\nB\\nEXPERIMENT DETAILS\\nB.1\\nTRAINING\\nTable 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\nB.1.1\\nFINE-TUNING\\nWe ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 13}, page_content='Published as a conference paper at ICLR 2021\\nDataset\\nSteps\\nBase LR\\nImageNet\\n20 000\\n{0.003, 0.01, 0.03, 0.06}\\nCIFAR100\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nCIFAR10\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nOxford-IIIT Pets\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nOxford Flowers-102\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nVTAB (19 tasks)\\n2 500\\n0.01\\nTable 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\\nﬁne-tuning resolution is 384.\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks.\\nB.1.2\\nSELF-SUPERVISION\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\\npatch representations.\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\\n(2019) but results were also slightly worse on our few-shot metrics.\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\\nImageNet.\\nC\\nADDITIONAL RESULTS\\nWe report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 14}, page_content='Published as a conference paper at ICLR 2021\\nViT-B/16\\nViT-B/32\\nViT-L/16\\nViT-L/32\\nViT-H/14\\nImageNet\\nCIFAR-10\\n98.13\\n97.77\\n97.86\\n97.94\\n-\\nCIFAR-100\\n87.13\\n86.31\\n86.35\\n87.07\\n-\\nImageNet\\n77.91\\n73.38\\n76.53\\n71.16\\n-\\nImageNet ReaL\\n83.57\\n79.56\\n82.19\\n77.83\\n-\\nOxford Flowers-102\\n89.49\\n85.43\\n89.66\\n86.36\\n-\\nOxford-IIIT-Pets\\n93.81\\n92.04\\n93.64\\n91.35\\n-\\nImageNet-21k\\nCIFAR-10\\n98.95\\n98.79\\n99.16\\n99.13\\n99.27\\nCIFAR-100\\n91.67\\n91.97\\n93.44\\n93.04\\n93.82\\nImageNet\\n83.97\\n81.28\\n85.15\\n80.99\\n85.13\\nImageNet ReaL\\n88.35\\n86.63\\n88.40\\n85.65\\n88.70\\nOxford Flowers-102\\n99.38\\n99.11\\n99.61\\n99.19\\n99.51\\nOxford-IIIT-Pets\\n94.43\\n93.02\\n94.73\\n93.09\\n94.82\\nJFT-300M\\nCIFAR-10\\n99.00\\n98.61\\n99.38\\n99.19\\n99.50\\nCIFAR-100\\n91.87\\n90.49\\n94.04\\n92.52\\n94.55\\nImageNet\\n84.15\\n80.73\\n87.12\\n84.37\\n88.04\\nImageNet ReaL\\n88.85\\n86.27\\n89.99\\n88.28\\n90.33\\nOxford Flowers-102\\n99.56\\n99.27\\n99.56\\n99.45\\n99.68\\nOxford-IIIT-Pets\\n95.80\\n93.40\\n97.11\\n95.83\\n97.56\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\nEpochs\\nImageNet\\nImageNet ReaL\\nCIFAR-10\\nCIFAR-100\\nPets\\nFlowers\\nexaFLOPs\\nname\\nViT-B/32\\n7\\n80.73\\n86.27\\n98.61\\n90.49\\n93.40\\n99.27\\n55\\nViT-B/16\\n7\\n84.15\\n88.85\\n99.00\\n91.87\\n95.80\\n99.56\\n224\\nViT-L/32\\n7\\n84.37\\n88.28\\n99.19\\n92.52\\n95.83\\n99.45\\n196\\nViT-L/16\\n7\\n86.30\\n89.43\\n99.38\\n93.46\\n96.81\\n99.66\\n783\\nViT-L/16\\n14\\n87.12\\n89.99\\n99.38\\n94.04\\n97.11\\n99.56\\n1567\\nViT-H/14\\n14\\n88.08\\n90.36\\n99.50\\n94.71\\n97.11\\n99.71\\n4262\\nResNet50x1\\n7\\n77.54\\n84.56\\n97.67\\n86.07\\n91.11\\n94.26\\n50\\nResNet50x2\\n7\\n82.12\\n87.94\\n98.29\\n89.20\\n93.43\\n97.02\\n199\\nResNet101x1\\n7\\n80.67\\n87.07\\n98.48\\n89.17\\n94.08\\n95.95\\n96\\nResNet152x1\\n7\\n81.88\\n87.96\\n98.82\\n90.22\\n94.17\\n96.94\\n141\\nResNet152x2\\n7\\n84.97\\n89.69\\n99.06\\n92.05\\n95.37\\n98.62\\n563\\nResNet152x2\\n14\\n85.56\\n89.89\\n99.24\\n91.92\\n95.75\\n98.75\\n1126\\nResNet200x3\\n14\\n87.22\\n90.15\\n99.34\\n93.53\\n96.32\\n99.04\\n3306\\nR50x1+ViT-B/32\\n7\\n84.90\\n89.15\\n99.01\\n92.24\\n95.75\\n99.46\\n106\\nR50x1+ViT-B/16\\n7\\n85.58\\n89.65\\n99.14\\n92.63\\n96.65\\n99.40\\n274\\nR50x1+ViT-L/32\\n7\\n85.68\\n89.04\\n99.24\\n92.93\\n96.97\\n99.43\\n246\\nR50x1+ViT-L/16\\n7\\n86.60\\n89.72\\n99.18\\n93.64\\n97.03\\n99.40\\n859\\nR50x1+ViT-L/16\\n14\\n87.12\\n89.76\\n99.31\\n93.89\\n97.36\\n99.11\\n1668\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\\naFLOPs).\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\\nvarying size, as well as the estimated computational cost of their pre-training.\\nD\\nADDITIONAL ANALYSES\\nD.1\\nSGD VS. ADAM FOR RESNETS\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 15}, page_content='Published as a conference paper at ICLR 2021\\nResNet50\\nResNet152x2\\nDataset\\nAdam\\nSGD\\nAdam\\nSGD\\nImageNet\\n77.54\\n78.24\\n84.97\\n84.37\\nCIFAR10\\n97.67\\n97.46\\n99.06\\n99.07\\nCIFAR100\\n86.07\\n85.17\\n92.05\\n91.06\\nOxford-IIIT Pets\\n91.11\\n91.00\\n95.37\\n94.79\\nOxford Flowers-102\\n94.26\\n92.06\\n98.62\\n99.32\\nAverage\\n89.33\\n88.79\\n94.01\\n93.72\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n100\\n101\\nRelative Compute\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nImageNet 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\n100\\n101\\nRelative Compute\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\nperformance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\\nfor 7 epochs, not 30.\\nD.2\\nTRANSFORMER SHAPE\\nWe ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\\nall dimensions proportionally results in robust improvements.\\nD.3\\nHEAD TYPE AND CLASS TOKEN\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\nThis design is inherited from the Transformer model for text, and we use it throughout the main\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 16}, page_content='Published as a conference paper at ICLR 2021\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nEpochs of training\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nImageNet linear 5-shot accuracy [%]\\nCLS-Token, lr=8e-4\\nGAP, lr=8e-4\\nGAP, lr=3e-4\\nFigure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\\nwell, but require different learning-rates.\\nPos. Emb.\\nDefault/Stem\\nEvery Layer\\nEvery Layer-Shared\\nNo Pos. Emb.\\n0.61382\\nN/A\\nN/A\\n1-D Pos. Emb.\\n0.64206\\n0.63964\\n0.64292\\n2-D Pos. Emb.\\n0.64001\\n0.64046\\n0.64022\\nRel. Pos. Emb.\\n0.64032\\nN/A\\nN/A\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\\nImageNet 5-shot linear.\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\\nFigure 9.\\nD.4\\nPOSITIONAL EMBEDDING\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\\ntried the following cases:\\n• Providing no positional information: Considering the inputs as a bag of patches.\\n• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\\nthe raster order (default across all other experiments in this paper).\\n• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\\nembedding for that patch.\\n• Relative positional embeddings: Considering the relative distance between patches to en-\\ncode the spatial information as instead of their absolute position. To do so, we use 1-\\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\\nembedding. Then, we simply run extra attention, where we use the original query (the\\ncontent of query), but use relative positional embeddings as keys. We then use the log-\\nits from the relative attention as a bias term and add it to the logits of the main attention\\n(content-based attention) before applying the softmax.\\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 17}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0002, WD=0.01\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0004, WD=0.1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n14 epochs, LR=0.0004, WD=0.1\\n1\\n1\\nCosine similarity\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\\nhyperparameters (Figure 10).\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nR50x1 + ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\\nheads at one layer. Image width is 224 pixels.\\nD.5\\nEMPIRICAL COMPUTATIONAL COSTS\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 18}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 19}, page_content='Published as a conference paper at ICLR 2021\\n102\\nTotal compute [exaFLOPs]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\n102\\n103\\nPeak inference speed [img/sec/core]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\\nattention and although the sequence length that self-attention operates on is smaller in axial case,\\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\\non TPUs (Figure 13, right).\\nD.7\\nATTENTION DISTANCE\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\nD.8\\nATTENTION MAPS\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\\nfor the mixing of attention across tokens through all layers.\\nD.9\\nOBJECTNET RESULTS\\nWe also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\nD.10\\nVTAB BREAKDOWN\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 20}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-06-04T00:19:58+00:00', 'source': '../../docs/Vision Transformers.pdf', 'file_path': '../../docs/Vision Transformers.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-06-04T00:19:58+00:00', 'trapped': '', 'modDate': 'D:20210604001958Z', 'creationDate': 'D:20210604001958Z', 'page': 21}, page_content='Published as a conference paper at ICLR 2021\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\nCaltech101\\nCIFAR-100\\nDTD\\nFlowers102\\nPets\\nSun397\\nSVHN\\nCamelyon\\nEuroSAT\\nResisc45\\nRetinopathy\\nClevr-Count\\nClevr-Dist\\nDMLab\\ndSpr-Loc\\ndSpr-Ori\\nKITTI-Dist\\nsNORB-Azim\\nsNORB-Elev\\nMean\\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\\n22')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f03542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2015-12-03T01:48:07+00:00', 'source': '../../docs/cnn_paper.pdf', 'file_path': '../../docs/cnn_paper.pdf', 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2015-12-03T01:48:07+00:00', 'trapped': '', 'modDate': 'D:20151203014807Z', 'creationDate': 'D:20151203014807Z', 'page': 2}, page_content='Introduction to Convolutional Neural Networks\\n3\\nmore suited for image-focused tasks - whilst further reducing the parameters\\nrequired to set up the model.\\nOne of the largest limitations of traditional forms of ANN is that they tend to\\nstruggle with the computational complexity required to compute image data.\\nCommon machine learning benchmarking datasets such as the MNIST database\\nof handwritten digits are suitable for most forms of ANN, due to its relatively\\nsmall image dimensionality of just 28 × 28. With this dataset a single neuron in\\nthe ﬁrst hidden layer will contain 784 weights (28×28×1 where 1 bare in mind\\nthat MNIST is normalised to just black and white values), which is manageable\\nfor most forms of ANN.\\nIf you consider a more substantial coloured image input of 64 × 64, the number\\nof weights on just a single neuron of the ﬁrst layer increases substantially to\\n12, 288. Also take into account that to deal with this scale of input, the network\\nwill also need to be a lot larger than one used to classify colour-normalised\\nMNIST digits, then you will understand the drawbacks of using such models.\\n1.1\\nOverﬁtting\\nBut why does it matter? Surely we could just increase the number of hidden lay-\\ners in our network, and perhaps increase the number of neurons within them?\\nThe simple answer to this question is no. This is down to two reasons, one be-\\ning the simple problem of not having unlimited computational power and time\\nto train these huge ANNs.\\nThe second reason is stopping or reducing the effects of overﬁtting. Overﬁtting\\nis basically when a network is unable to learn effectively due to a number of\\nreasons. It is an important concept of most, if not all machine learning algo-\\nrithms and it is important that every precaution is taken as to reduce its effects.\\nIf our models were to exhibit signs of overﬁtting then we may see a reduced\\nability to pinpoint generalised features for not only our training dataset, but\\nalso our test and prediction sets.\\nThis is the main reason behind reducing the complexity of our ANNs. The less\\nparameters required to train, the less likely the network will overﬁt - and of\\ncourse, improve the predictive performance of the model.\\n2\\nCNN architecture\\nAs noted earlier, CNNs primarily focus on the basis that the input will be com-\\nprised of images. This focuses the architecture to be set up in way to best suit\\nthe need for dealing with the speciﬁc type of data.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6b2b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docx_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31dfcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../../docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': '../../docs', 'filename': 'Intel Strategy.docx', 'last_modified': '2025-05-30T10:16:46', 'orig_elements': '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', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}, page_content='Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\\n\\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\\n\\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.'),\n",
       " Document(metadata={'source': '../../docs/Intel Strategy.docx', 'emphasized_text_contents': ['AI and Cloud-to-Edge Infrastructure', 'IPU roadmap', 'Single GPU solution for media transcode, visual graphics and inference in the cloud:'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': '../../docs', 'filename': 'Intel Strategy.docx', 'last_modified': '2025-05-30T10:16:46', 'orig_elements': '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', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'dbd96567d34d7cad54c91a52fa8b663b'}, page_content='AI and Cloud-to-Edge Infrastructure: This allows for high performance computing to be immediately available, which is the backbone of ubiquitous compute. “With the unlimited reach of the intelligent edge, we can have low latency, high bandwidth, and real-time inference capabilities anywhere we want them,” he explained. Intelligent Edge could make visual experience of streaming services, cloud gaming, and visual workloads possible, however, there are hurdles to be overcome for that. Intel stands to overcome the challenges of deploying a complete cloud to edge infrastructure in today’s time with the launch of Habana Gaudi2 AI processor for training data centre workloads, and 12th Gen Intel Core HX processors for hybrid work.Habana Gaudi2 and Greco AI Accelerators are built on a single software stack, Synapse AI, that supports different architectures, enabling end-users to take advantage of the processors’ performance and efficiency. In addition, Gaudi2 delivers two times better AI training performance compared with current in-market A100-based offerings for key vision and NLP workloads, the company announced. The company also announced the shipment of the 4th Gen Intel Xeon Scalable processors, which will support DDR5, PCIe Gen5 and CXL 1.1, and are equipped with new integrated accelerators that deliver up to 30x performance versus the prior generation through software and hardware optimizations for AI workloads, along with new capabilities that deliver upto two times capacity gains for virtual radio access network (vRAN) deployments, for telco networks. Also, in partnership with Accenture, Intel has kickstarted Project Apollo, a program that will provide enterprises with more than 30 opensource AI solutions kits that are designed to make AI more accessible to customers in on-prem, cloud and edge environments. The company also unveiled its IPU roadmap, featuring new FPGA + Intel architecture platforms (code-named Hot Springs Canyon) and the Mount Morgan (MMG) ASIC, as well as next generation 800GB products. IPUs are dedicated products with hardened acceleration for infrastructure compute needs, allowing businesses to accomplish tasks quicker and solve problems faster.\\n\\nSingle GPU solution for media transcode, visual graphics and inference in the cloud: Intel’s data center GPU, code-named Arctic Sound-M (ATS-M), is the industry’s first discrete GPU with an AV1 hardware encoder. ATS-M is a versatile GPU with leadership transcode quality and performance targeting 150 trillion operations per second (TOPS). Developers will be able to easily design for ATS-M with an open software stack through oneAPI. ATS-M will be available in two form factors and in more than 15 system designs from partners including Dell Technologies, Supermicro, Cisco, HPE, Inspur and H3C. It will launch in 2022’s third quarter.'),\n",
       " Document(metadata={'source': '../../docs/Intel Strategy.docx', 'emphasized_text_contents': ['New 12th Gen Intel Core HX processors for hybrid work:', 'High performance computing to solve the world’s most complex challenges:', 'Confidence with confidential computing:', 'Agriculture autonomy with private wireless networks', 'Intel IDM 2.0', 'Intel’s global, internal factory network for at-scale manufacturing'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b', 'b'], 'file_directory': '../../docs', 'filename': 'Intel Strategy.docx', 'last_modified': '2025-05-30T10:16:46', 'orig_elements': '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', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '1165541ad1ff60df62a33fef71fa09bc'}, page_content='New 12th Gen Intel Core HX processors for hybrid work: The company completed the 12th Gen family with the launch of the new 12th Gen Intel Core HX processors. Created for professionals who need maximum performance and flexibility to navigate a hybrid environment, and with up to 16 cores and clock speeds up to 5 GHz, the Intel Core i9-12900HX processor is the world’s best mobile workstation platform.\\n\\nHigh performance computing to solve the world’s most complex challenges: Argonne National Laboratories is on track to deliver 2 exaflops of peak performance with the Aurora supercomputer running on the Intel Xeon processor, code-named Sapphire Rapids with High Bandwidth Memory (HBM), and the Intel data center GPU, code-named Ponte Vecchio, with Intel oneAPI providing developers seamless system integration.\\n\\nConfidence with confidential computing: Bosch and Intel collaborated on a research effort to develop a confidential AI solution that allows Bosch to train its neural networks confidentially in the public cloud. To help achieve this at scale, Bosch Corporate Research has built a confidential AI platform powered by Intel Software Guard Extensions available with 3rd Gen Intel Xeon Scalable platforms.\\n\\nAgriculture autonomy with private wireless networks: Intelligent edge solutions have the potential to transform food. Blue White Robotics developed a new type of autonomous agricultural solution that transforms a grower’s existing equipment into a fleet of autonomous tractors connected to an internet-based management platform. With help from Intel and Federated Wireless, Blue White Robotics made this a scalable solution that leverages Intel Smart Edge and Intel Xeon D processors and employs the power of edge computing and shared spectrum to create a private wireless network on any farm anywhere.\\n\\nIntel is moving at a “torrid pace,” Gelsinger said. “When you think about torrid, it’s a word about speed and energy and heat. But in the Intel context, we’re also applying a vector of that energy for setting a direction into the future.”\\n\\nIntel IDM 2.0\\n\\nThe Intel IDM 2.0 strategy revealed by new company CEO Pat Gelsinger is an ambitious plan to restore the company’s leadership in semiconductor production. Gelsinger described IDM 2.0 as the second generation of Intel’s integrated device manufacturing model.\\n\\nIntel’s global, internal factory network for at-scale manufacturing is a key competitive advantage that enables product optimization, improved economics and supply resilience. Gelsinger re-affirmed the company’s expectation to continue manufacturing most of its products internally. The company’s 7 Nanometer Processors development is driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow.'),\n",
       " Document(metadata={'source': '../../docs/Intel Strategy.docx', 'emphasized_text_contents': ['Expanded use of third-party foundry capacity.', 'Intel Foundry Services.', 'Expanding in the U.S. and Europe', 'Intel Sustainability'], 'emphasized_text_tags': ['b', 'b', 'b', 'b'], 'file_directory': '../../docs', 'filename': 'Intel Strategy.docx', 'last_modified': '2025-05-30T10:16:46', 'orig_elements': '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', 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'c04d633e4de3707bf9619657c386fd25'}, page_content=\"Expanded use of third-party foundry capacity. Gelsinger said he expects Intel’s engagement with third-party foundries to grow and to include manufacturing for a range of modular tiles on advanced process technologies, including products at the core of Intel’s computing offerings for both client and data center segments beginning in 2023. This will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule and supply, giving the company a unique competitive advantage.\\n\\nIntel Foundry Services. The launch of Intel Foundry Services means the company is not only going to manufacture its own chips, but it will also produce them for other semiconductor companies, including its competitors. Intel announced plans to become a major provider of U.S. and Europe-based foundry capacity to serve the global demand for semiconductor manufacturing. Hence, Intel is establishing a new standalone business unit, Intel Foundry Services (IFS), led by semiconductor industry veteran Dr. Randhir Thakur, who will report directly to Gelsinger. IFS will be differentiated from other foundry offerings with a combination of leading-edge process technology and packaging, committed capacity in the U.S. and Europe, and a world-class IP portfolio for customers, including x86 cores as well as ARM and RISC-V ecosystem IPs. Gelsinger noted that Intel’s foundry plans have received strong statements of support from across the industry. Intel conservatively sizes the foundry opportunity as a $100 billion addressable market by 2025.\\n\\nExpanding in the U.S. and Europe. Intel is expanding its manufacturing capacity in the U.S. and Europe to provide less dependence on any specific region. Noting that 80% of leading-edge foundry capacity is concentrated in Asia, Gelsinger believes “the industry needs more geographically balanced manufacturing capacity.”\\n\\nIntel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said.\\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611d912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
