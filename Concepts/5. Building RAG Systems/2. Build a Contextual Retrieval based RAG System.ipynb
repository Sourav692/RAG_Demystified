{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e88b0d",
   "metadata": {},
   "source": [
    "# Build a Contextual Retrieval based RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7659767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49a8ce",
   "metadata": {},
   "source": [
    "### Open AI Embedding Models\n",
    "\n",
    "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91adba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0ada3",
   "metadata": {},
   "source": [
    "## Loading and Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a807b22",
   "metadata": {},
   "source": [
    "### Load and Process JSON Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615f6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(file_path='../../docs/wikidata_rag_demo.jsonl',\n",
    "                    jq_schema='.',\n",
    "                    text_content=False,\n",
    "                    json_lines=True)\n",
    "wiki_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd5d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "wiki_docs_processed = []\n",
    "\n",
    "for doc in wiki_docs:\n",
    "    doc = json.loads(doc.page_content)\n",
    "    metadata = {\n",
    "        \"title\": doc['title'],\n",
    "        \"id\": doc['id'],\n",
    "        \"source\": \"Wikipedia\",\n",
    "        \"page\": 1\n",
    "    }\n",
    "    data = ' '.join(doc['paragraphs'])\n",
    "    wiki_docs_processed.append(Document(page_content=data, metadata=metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d827833",
   "metadata": {},
   "source": [
    "### Load and Process PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee39d3",
   "metadata": {},
   "source": [
    "#### Create Chunk Contexts for Contextual Retrieval\n",
    "\n",
    "![](https://i.imgur.com/LRhKHzk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca65590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e89f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chunk context generation chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "def generate_chunk_context(document, chunk):\n",
    "\n",
    "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
    "                            Your task is to provide brief, relevant context for a chunk of text\n",
    "                            based on the following research paper.\n",
    "\n",
    "                            Here is the research paper:\n",
    "                            <paper>\n",
    "                            {paper}\n",
    "                            </paper>\n",
    "\n",
    "                            Here is the chunk we want to situate within the whole document:\n",
    "                            <chunk>\n",
    "                            {chunk}\n",
    "                            </chunk>\n",
    "\n",
    "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
    "                            considering the following guidelines:\n",
    "\n",
    "                            - Give a short succinct context to situate this chunk within the overall document\n",
    "                            for the purposes of improving search retrieval of the chunk.\n",
    "                            - Answer only with the succinct context and nothing else.\n",
    "                            - Context should be mentioned like 'Focuses on ....'\n",
    "                            do not mention 'this chunk or section focuses on...'\n",
    "\n",
    "                            Context:\n",
    "                        \"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
    "\n",
    "    agentic_chunk_chain = (prompt_template\n",
    "                                |\n",
    "                            chatgpt\n",
    "                                |\n",
    "                            StrOutputParser())\n",
    "\n",
    "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c45bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "\n",
    "def create_contextual_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
    "\n",
    "    print('Loading pages:', file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print('Chunking pages:', file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                              chunk_overlap=chunk_overlap)\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "\n",
    "    print('Generating contextual chunks:', file_path)\n",
    "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
    "    contextual_chunks = []\n",
    "    for chunk in doc_chunks:\n",
    "        chunk_content = chunk.page_content\n",
    "        chunk_metadata = chunk.metadata\n",
    "        chunk_metadata_upd = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'page': chunk_metadata['page'],\n",
    "            'source': chunk_metadata['source'],\n",
    "            'title': chunk_metadata['source'].split('/')[-1]\n",
    "        }\n",
    "        context = generate_chunk_context(original_doc, chunk_content)\n",
    "        contextual_chunks.append(Document(page_content=context+'\\n'+chunk_content,\n",
    "                                          metadata=chunk_metadata_upd))\n",
    "    print('Finished processing:', file_path)\n",
    "    print()\n",
    "    return contextual_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059e50d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../docs/layoutparser_paper.pdf',\n",
       " '../../docs/cnn_paper.pdf',\n",
       " '../../docs/vision_transformer.pdf',\n",
       " '../../docs/resnet_paper.pdf',\n",
       " '../../docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf',\n",
       " '../../docs/attention_paper.pdf',\n",
       " '../../docs/Vision Transformers.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "pdf_files = glob('../../docs/*.pdf')\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70dc86",
   "metadata": {},
   "source": [
    "Only loading data from specific pdf's to handle GPT4 out of context error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7b54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages: ../../docs/vision_transformer.pdf\n",
      "Chunking pages: ../../docs/vision_transformer.pdf\n",
      "Generating contextual chunks: ../../docs/vision_transformer.pdf\n",
      "Finished processing: ../../docs/vision_transformer.pdf\n",
      "\n",
      "Loading pages: ../../docs/attention_paper.pdf\n",
      "Chunking pages: ../../docs/attention_paper.pdf\n",
      "Generating contextual chunks: ../../docs/attention_paper.pdf\n",
      "Finished processing: ../../docs/attention_paper.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_docs = []\n",
    "for fp in pdf_files:\n",
    "    if ('attention' in fp) or ('transformer' in fp) or ('vision' in fp):\n",
    "        paper_docs.extend(create_contextual_chunks(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c976b34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6cdcb",
   "metadata": {},
   "source": [
    "### Combine all document chunks in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "628387c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b712cf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1845"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_docs = wiki_docs_processed + paper_docs\n",
    "len(total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039b647",
   "metadata": {},
   "source": [
    "## Index Document Chunks and Embeddings in Vector DB\n",
    "\n",
    "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65915cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
    "chroma_db = Chroma.from_documents(documents=total_docs,\n",
    "                                  collection_name='my_context_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./my_context_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1221f9a",
   "metadata": {},
   "source": [
    "### Load Vector DB from disk\n",
    "\n",
    "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af12dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./my_context_db\",\n",
    "                   collection_name='my_context_db',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "233e8f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x11d3d32d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9fcd8",
   "metadata": {},
   "source": [
    "### Semantic Similarity based Retrieval\n",
    "\n",
    "We use simple cosine similarity here and retrieve the top 5 similar documents based on the user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fd07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61b8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_docs(docs):\n",
    "    for doc in docs:\n",
    "        print('Metadata:', doc.metadata)\n",
    "        print('Content Brief:')\n",
    "        display(Markdown(doc.page_content[:1000]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b350be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'id': '564928', 'page': 1, 'source': 'Wikipedia', 'title': 'Machine learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science. The idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs. Machine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '359370', 'page': 1, 'source': 'Wikipedia', 'title': 'Supervised learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a \"classifier\". Usually, the system uses inductive reasoning to generalize the training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '663523', 'page': 1, 'source': 'Wikipedia', 'title': 'Deep learning'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Deep learning (also called deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks. As with other kinds of machine-learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as as recognizing and understanding speech, images or handwriting, is easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, whic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '6360', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial intelligence'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation. An ideal (perfect) intelligent machine is a flexible agent which perceives its environment and takes actions to maximize its chance of success at some goal or objective. As machines become increasingly capable, mental facu"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '44742', 'page': 1, 'source': 'Wikipedia', 'title': 'Artificial neural network'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem. This is one method for creating artificially intelligent programs. Neural networks are an example of machine learning, where a program can change as it learns to solve a problem. A neural network can be trained and improved with each example, but the larger the neural network, the more examples it needs to perform well—often needing millions or billions of examples in the case of deep learning. There are two ways to think of a neural network. First is like a human brain. Second is like a mathematical equation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is machine learning?\"\n",
    "top_docs = similarity_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76720c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'id': '5b62f899-a41f-4452-8c51-82407866b9b5', 'page': 7, 'source': '../../docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on a controlled scaling study of various models, including Vision Transformers and ResNets, to evaluate their transfer performance from the JFT-300M dataset. It highlights the performance versus pre-training cost, revealing that Vision Transformers generally outperform ResNets in terms of efficiency and scalability. Additionally, it discusses the implications of hybrid models and the potential for further scaling of Vision Transformers.\n",
       "Published as a conference paper at ICLR 2021\n",
       "4.4\n",
       "SCALING STUDY\n",
       "We perform a controlled scaling study of different models by evaluating transfer performance from\n",
       "JFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\n",
       "performance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\n",
       "R50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\n",
       "for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\n",
       "L/16 and H/1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': 'f67497d2-411f-465b-84c4-c686183e95fd', 'page': 0, 'source': '../../docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the introduction of the Vision Transformer (ViT) model, which applies a standard Transformer architecture directly to image classification tasks by treating image patches as tokens. It highlights the limitations of traditional convolutional neural networks (CNNs) in computer vision and presents evidence that ViT can achieve competitive performance on various benchmarks with fewer computational resources when pre-trained on large datasets.\n",
       "Published as a conference paper at ICLR 2021\n",
       "AN IMAGE IS WORTH 16X16 WORDS:\n",
       "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n",
       "Alexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\n",
       "Xiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\n",
       "Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\n",
       "∗equal technical contribution, †equal advising\n",
       "Google Research, Brain Team\n",
       "{adosovitskiy, neilhoulsby}@google.com\n",
       "ABSTRACT\n",
       "While the Transformer architecture has become the de-facto standard for natural\n",
       "language "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '241f4ed4-9dd4-4cf1-b83f-bd8348556adf', 'page': 2, 'source': '../../docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the architecture and methodology of the Vision Transformer (ViT), detailing how images are processed by splitting them into patches, embedding them, and utilizing a standard Transformer encoder for image classification tasks. It describes the model's design principles, including the use of position embeddings and the integration of a classification token, while referencing foundational work in Transformer architecture.\n",
       "Published as a conference paper at ICLR 2021\n",
       "Transformer Encoder\n",
       "MLP \n",
       "Head\n",
       "Vision Transformer (ViT)\n",
       "*\n",
       "Linear Projection of Flattened Patches\n",
       "* Extra learnable\n",
       "     [ cl ass]  embedding\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "0\n",
       "Patch + Position \n",
       "Embedding\n",
       "Class\n",
       "Bird\n",
       "Ball\n",
       "Car\n",
       "...\n",
       "Embedded \n",
       "Patches\n",
       "Multi-Head \n",
       "Attention\n",
       "Norm\n",
       "MLP\n",
       "Norm\n",
       "+\n",
       "L x\n",
       "+\n",
       "Transformer Encoder\n",
       "Figure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\n",
       "add position embeddings, and feed the resulting sequence of vectors to a standard Transformer\n",
       "encoder. In order to perform classiﬁ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '817bc951-8051-48be-b7ff-fe40c55d58a3', 'page': 7, 'source': '../../docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the behavior of attention mechanisms in Vision Transformers, highlighting how attention distances vary across layers and the implications for model performance. It discusses the relationship between attention distance and network depth, as well as the role of hybrid models that incorporate convolutional layers. Additionally, it sets the stage for the subsequent exploration of self-supervised learning techniques in Transformers.\n",
       "have consistently small attention distances in the low layers. This highly localized attention is\n",
       "less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\n",
       "suggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\n",
       "attention distance increases with network depth. Globally, we ﬁnd that the model attends to image\n",
       "regions that are semantically relevant for classiﬁcation (Figure 6).\n",
       "4.6\n",
       "SELF-SUPERVISION\n",
       "Transformers show impressive performance on NLP tasks. However, much of thei"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': '778c3e13-86fe-4aaf-811d-ecb6fbf01a7a', 'page': 1, 'source': '../../docs/vision_transformer.pdf', 'title': 'vision_transformer.pdf'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Focuses on the performance of the Vision Transformer (ViT) when trained on large datasets, highlighting its ability to achieve state-of-the-art results in image recognition tasks despite lacking some inductive biases inherent to convolutional neural networks (CNNs). It also discusses related work in the field, particularly the application of self-attention mechanisms in image processing and comparisons with existing models.\n",
       "Published as a conference paper at ICLR 2021\n",
       "inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\n",
       "when trained on insufﬁcient amounts of data.\n",
       "However, the picture changes if the models are trained on larger datasets (14M-300M images). We\n",
       "ﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\n",
       "results when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\n",
       "pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT ap"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the difference between transformers and vision transformers?\"\n",
    "top_docs = similarity_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41f770",
   "metadata": {},
   "source": [
    "## Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10fa8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "                Answer the following question using only the following pieces of retrieved context.\n",
    "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "                Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d303ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (similarity_retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    rag_prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55054c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Machine learning is a subfield of computer science that provides computers with the ability to learn without being explicitly programmed. The concept was introduced by Arthur Samuel in 1959 and is rooted in artificial intelligence. Machine learning focuses on the study and construction of algorithms that can learn from data and make predictions or decisions based on that data. These algorithms follow programmed instructions but can also adapt and improve their performance by building models from sample inputs.\n",
       "\n",
       "Machine learning is particularly useful in scenarios where designing and programming explicit algorithms is impractical. Some common applications of machine learning include:\n",
       "\n",
       "- Spam filtering\n",
       "- Detection of network intruders or malicious insiders\n",
       "- Optical character recognition (OCR)\n",
       "- Search engines\n",
       "- Computer vision\n",
       "\n",
       "Within machine learning, there are different types of learning approaches, such as supervised learning, where a function is inferred from labeled training data. In supervised learning, the system learns to produce correct results based on known outcomes from the training data.\n",
       "\n",
       "Additionally, deep learning is a specialized area of machine learning that primarily utilizes neural networks. Deep learning involves multiple layers of processing, allowing the model to learn increasingly abstract representations of the data. This approach is particularly effective for complex tasks like speech recognition, image understanding, and handwriting recognition.\n",
       "\n",
       "Overall, machine learning enables computers to improve their performance on tasks through experience, making it a powerful tool in the field of artificial intelligence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88918232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A CNN, or Convolutional Neural Network, is a type of deep learning model primarily used for processing structured grid data, such as images. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images. They utilize convolutional layers that apply filters to the input data, allowing the model to capture local patterns and features effectively.\n",
       "\n",
       "In the context of the provided information, CNNs have been a standard approach in image recognition tasks. They have been shown to perform well, especially when trained on large datasets like ImageNet-21k and JFT-300M. The performance of CNNs can scale with the size of the dataset, meaning that as more data becomes available, CNNs can achieve better accuracy in classification tasks.\n",
       "\n",
       "Recent advancements in image recognition have also seen the emergence of models like the Vision Transformer (ViT), which apply self-attention mechanisms instead of traditional convolutional layers. These models have demonstrated competitive performance with CNNs, particularly when trained on large datasets.\n",
       "\n",
       "Overall, CNNs remain a foundational technology in the field of computer vision, but the landscape is evolving with the introduction of new architectures that challenge their dominance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is a CNN?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637f6fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided context does not contain specific information comparing ResNet to CNNs in terms of advantages or improvements. It primarily discusses the Vision Transformer (ViT) and its architecture, performance, and fine-tuning processes, without directly addressing how ResNet is better than CNNs. Therefore, I don't know how a ResNet is better than a CNN based on the given context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How is a resnet better than a CNN?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fd86c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How does a resnet work?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca15543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is LangGraph?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9c4f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The context provided does not contain specific information about an \"Agentic AI System.\" Therefore, I don't know what an Agentic AI System is."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is an Agentic AI System?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8bc74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is LangChain?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cad01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
